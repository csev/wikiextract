[[Category:Coursera]]
[[Category:Music Production]]
[[Category:Music]]

# Introduction to Music Production

#[[Musicproduction:daw | DAW specifics]]
All DAW information is collected and organized in the [[Musicproduction:daw |DAW wiki]]  

# Topics by week

## Week 1: Sound and Signal Flow

### Propagation of sound

Sound is a sequence of waves of pressure that propagates through compressible media such as air or water. (Sound can propagate through solids as well, but there are additional modes of propagation). Sound that is perceptible by humans has frequencies from about 20 Hz to 20,000 Hz. In air at standard temperature and pressure, the corresponding wavelengths of sound waves range from 17 m to 17 mm. During propagation, waves can be reflected, refracted, or attenuated by the medium.[2]

The behavior of sound propagation is generally affected by three things:

1. A relationship between density and pressure. This relationship, affected by temperature, determines the speed of sound within the medium.
2. The propagation is also affected by the motion of the medium itself. For example, sound moving through wind. Independent of the motion of sound through the medium, if the medium is moving, the sound is further transported.
3. The viscosity of the medium also affects the motion of sound waves. It determines the rate at which sound is attenuated. For many media, such as air or water, attenuation due to viscosity is negligible.

When sound is moving through a medium that does not have constant physical properties, it may be refracted (either dispersed or focused).[2]

### Perception of sound

#### Human ear

The perception of sound in any organism is limited to a certain range of frequencies. For humans, hearing is normally limited to frequencies between about 20 Hz and 20,000 Hz (20 kHz),[3] although these limits are not definite. The upper limit generally decreases with age. Other species have a different range of hearing. For example, dogs can perceive vibrations higher than 20 kHz, but are deaf to anything below 40 Hz. As a signal perceived by one of the major senses, sound is used by many species for detecting danger, navigation, predation, and communication. Earth's atmosphere, water, and virtually any physical phenomenon, such as fire, rain, wind, surf, or earthquake, produces (and is characterized by) its unique sounds. Many species, such as frogs, birds, marine and terrestrial mammals, have also developed special organs to produce sound. In some species, these produce song and speech. Furthermore, humans have developed culture and technology (such as music, telephone and radio) that allows them to generate, record, transmit, and broadcast sound. The scientific study of human sound perception is known as psychoacoustics.

### Physics of sound

#### Spherical compression waves

The mechanical vibrations that can be interpreted as sound are able to travel through all forms of matter: gases, liquids, solids, and plasmas. The matter that supports the sound is called the medium. Sound cannot travel through a vacuum.

#### Longitudinal and transverse waves

Sound is transmitted through gases, plasma, and liquids as longitudinal waves, also called compression waves. Through solids, however, it can be transmitted as both longitudinal waves and transverse waves. Longitudinal sound waves are waves of alternating pressure deviations from the equilibrium pressure, causing local regions of compression and rarefaction, while transverse waves (in solids) are waves of alternating shear stress at right angle to the direction of propagation.

Matter in the medium is periodically displaced by a sound wave, and thus oscillates. The energy carried by the sound wave converts back and forth between the potential energy of the extra compression (in case of longitudinal waves) or lateral displacement strain (in case of transverse waves) of the matter and the kinetic energy of the oscillations of the medium.

#### Sound wave properties and characteristics

Sinusoidal waves of various frequencies; the bottom waves have higher frequencies than those above. The horizontal axis represents time.

Sound waves are often simplified to a description in terms of sinusoidal plane waves, which are characterized by these generic properties:

* Frequency, or its inverse, the period
* Wavelength
* Wavenumber
* Amplitude
* Sound pressure
* Sound intensity
* Speed of sound
* Direction

Sometimes speed and direction are combined as a velocity vector; wavenumber and direction are combined as a wave vector.

Transverse waves, also known as shear waves, have the additional property, polarization, and are not a characteristic of sound waves.

#### Speed of sound

U.S. Navy F/A-18 approaching the sound barrier. The white halo is formed by condensed water droplets thought to result from a drop in air pressure around the aircraft (see Prandtl-Glauert Singularity).[4][5]

### Main article: Speed of sound

The speed of sound depends on the medium the waves pass through, and is a fundamental property of the material. In general, the speed of sound is proportional to the square root of the ratio of the elastic modulus (stiffness) of the medium to its density. Those physical properties and the speed of sound change with ambient conditions. For example, the speed of sound in gases depends on temperature. In 20 °C (68 °F) air at sea level, the speed of sound is approximately 343 m/s (1,230 km/h; 767 mph) using the formula "v = (331 + 0.6 T) m/s". In fresh water, also at 20 °C, the speed of sound is approximately 1,482 m/s (5,335 km/h; 3,315 mph). In steel, the speed of sound is about 5,960 m/s (21,460 km/h; 13,330 mph).[6] The speed of sound is also slightly sensitive (a second-order anharmonic effect) to the sound amplitude, which means that there are nonlinear propagation effects, such as the production of harmonics and mixed tones not present in the original sound (see parametric array).

## Week 2: DAW (Editing)
[[Musicproduction:daw | DAW specifics]]

### Reference Check List

#### Project start

As you start any new project in your DAW make sure you take the following in consideration.

* Proper project name and location
  * Preferentially create a folder on your computer that will have all different projects within it so it's easy to track where everything is.
* Set digital audio preferences
  * Sample Rate (recommended: 48kHz)
  * Bit depth (recommended: 24bit)
  * File type (recommended: bwf, wav or aiff)
* Set hardware preferences
  * Make sure your in/out devices are set to whichever audio interface you want to be using.
* Set buffer size
  * Record at 128 samples per second
  * Post-production at higher rates (1024 samples per second for example)

#### Recording
* Create a track
  * Normally you want to set it as mono, but if recording with two mics or a stereo synth set it to stereo.

* Name the track
  * This will help you recognize the files in your hard disk that are produced during recording

* Record enable the track (arm the track)

* Set the levels using the audio interface pre-amp
  * If using an instrument plugged to your interface, set its volume to maximum to make sure you have the best possible signal arriving at your interface.

* Enable the click (metronome) and countoff (how many clicks before starting to record)

* Record!


### Buffer Size
Is how much cache memory is dedicated to handle the tracks being processed in the DAW.

In practice this means:
* Bigger buffer --> more plugins can be used at the same time BUT more latency when converting from digital to analog
* Smaller buffer --> less plugins can be used BUT less latency

Therefore a low buffer size is good for recording and a high buffer size for post-production.

Rule of thumb:
* Recording: buffer size of 128 samples
** If sample rate is 48kHz (48000samples per second) and buffer size is 128 samples, then the delay will be 128/48000 = 2.6ms (hardly noticeable by our ear)
* Post-production: buffer size of 1024
** If sample rate is 48kHz and buffer size is 1024 then delay will be 1024/48000=21.3ms which is very noticeable.

Note: when many tracks with many plugins are in a project and, later, you want to record another instrument but can't lower the buffer size (or else the plugins will glitch for example), one solution is to convert the project to audio and then record with lower buffer size. Most DAW should have an option for doing this automatically (usually it's called freezing a track).

### File Types
* For recording use lossless filetypes
  * aiff
  * wav
  * bwf (broadcast wav files, which store metadata useful for recovering if there is a crash).

* Interleaved or non-interleaved: this means either that the left and right channels are in the same file or different files.
  * Recomended: interleaved, since it's easier to work with.

### Comping

Comping consists of taking parts of different tracks and putting them together in a single compiled track.

This is often used for putting together the best parts out of several recorded takes, thus getting the best-possible performance.

During comping you'll be putting clips from different tracks together and this often results in a click between clips. To resolve this use a cross fade between clips, usually a short cross-fade is best, so it's imperceptible to our ears.

Finally, when you finish compiling the new track it's good to merge the clips together to a single clip.

### MIDI

MIDI (Musical Instrument Digital Interface) contains information about sound, but is not sound in itself.

Each MIDI message contains:
* Channel number
** 16 channels available
* Type of message sent
** note on/note off
** control change (sustain pedals or knobs control this)
** pitch bend
** channel pressure or aftertouch (how hard you press the key)
* Data words (gives the parameters for the messages)
** Note
** Velocity (how hard you hit the key)

MIDI messages can be interpreted by samplers and synthesizers.
* Synthesizers create sound out of pre-determined wave-forms (e.g. a sawtooth wave form)
* Samplers playback pre-recorded sound (e.g. the recorded sound of a kick drum)

#### Quantization

Quantization is the process of adjusting the midi events to match the metric of the performance.

For quantization:
* Adjust the grid (e.g. 4th note, 8th note, etc...)
* Adjust the quantization strength (advice: adjust it to 20%, listen to performance and if still not happy repeat it, until you're happy with the result).

## Week 3: Mixer (Summing)

### Parts of a mixer

A typical mixer contains several channel strips, each of them similar to each other. Each channel strip contains, from top to bottom:
* Input section (to plug mics or instruments) including trimming knob
* Insert (to send signal to an external device, modify it in some way and bring it back into the mixer)
* Aux sends (auxiliary sends allow you to send signal to another output)
* EQ
* Pan knob
* Mute and Solo buttons
* Volume fader

The sound from all the channels in the mixer gets combined and can be monitored in another section of the mixer called the master bus. The master bus has its own volume fader.

### Effects Categories

* Dynamic effects (control amplitude)
  * Compressors
  * Expanders
  * Limiters
  * Noise Gates
* Time-based effects (control propagation)
  * Delay
  * Reverb
  * Chorus
  * Phasers
  * Flangers
* Filter effects (control timbre)
  * High Pass Filter (HPF)
  * Low Pass Filter (LPF)
  * Band Pass Filter (BPF)
  * EQ (parametric or graphic)

### Inserts and Sends

The signal going along the mixer can be routed to different buses/effects boxes.

An insert sends the signal to an external device (or plugin in the DAW), modifies it in some way and brings it back into the mixer.

A send sends the signal to another bus, where it can be further modified and mixed with the other tracks.

In practice, for mixing purposes:
* Inserts are useful to apply an effect to a particular track without affecting other tracks. For example, you may want to EQ each track separately.
* Sends are useful to apply an effect to several tracks at once. For example you may want to apply the same reverb to multiple tracks. Instead of applying an insert to each track individually, you can send their signal to an auxiliary bus and apply the effect there.

## Week 4: Dynamics Processing

### Dynamic Range

* Ratio between the loudest and quietest volume a sound signal can have.

* Volume (and therefore the dynamic range) of sound is measured in decibels (dB).
* Decibels are a measure of sound amplitude relative to a reference level. 
 * It should always be specified what the decibels are relative to. For example when referring to sound propagating in the air it is common to use db SPL (sound pressure level).

* The human ear's dynamic range is between 0-120 db SPL. In this context, 0db SPL is the quietest sound humans can hear and 120 db SPL the threshold of pain.
* The dynamic range of a microphone, for example, will be defined by the interval between noise and distortion.

For mixing purposes it is important to recognize that:
* Our brains adjust the perceived sensitivity to sound sources depending on the sounds in the surrounding environment.
** For example, you may not hear the ticking of a clock in a busy room, but clearly hear it when the room is quieter.
* Our sense of timbre changes with the amplitude of sound. This is because our brains' EQ changes with amplitude.
** For example, when things get quiet we hear the mid range more; when things get louder our response flattens out more. Because of this we should be careful when setting the gain during mixing.

### Dynamic Range Manipulation

Generally when manipulating the dynamics of a song stay true to the music and make sure the changes are smooth and non-perceptible, so they feel natural to the listener.

* Some dynamic range manipulation happens during the actual performance: musicians may play louder or quieter in different parts of the piece.
* Macro Scale: use volume fader to automate differences between different parts of the whole piece, e.g., make verse quieter and chorus louder.
* Smaller Scale: adjusting the dynamics of a single track. 
* Micro scale: control dynamics of individual events.
 * Transient: when amplitude changes very fast. For example drums, claps, etc... There are many automation tools in DAW to manipulate transients.

### Compression and Expansion

* Compression - reducing the dynamic range
 * Either by increasing the quietest or decreasing the loudest
* Expansion - increasing the dynamic range
 * Either by decreasing the quietest or increasing the loudest


A common case is compressing the vocals, i.e., adjusting the levels of the vocals to make sure they are not lost behind other instruments. 

There's a technique called riding the vocal: when voice gets quieter bring the volume up, when voice gets louder get the volume down. It is in fact a type of manual compression.

### Dynamic Processor Parameters

A dynamic processor device/plugin will analyze the sound signal and, following certain rules, changes the input level.

Some dynamic processors are:
* Compressor - reduces input level going above a certain threshold.
** Limiter - is a type of compressor where the ratio (amount of gain reduction) is high, usually above 10:1
* Expander - reduces input level going bellow a certain threshold
** Noise Gate - is a type of expander

Parameters of dynamic processors:
* Threshold - the value of the input signal at which the dynamic processor will be triggered.
* Attack - how fast the change in volume is when the device is triggered.
* Decay - how fast the volume goes back to input level after the device is released.
* Ratio - how much the volume change should be in relation to the input level.
** For example a 2:1 ratio in a compressor means that for every 2db of input only 1db will go through the output.
** Ratios greater than 10:1 in a compressor are said to be limiting
* Knees - how abrupt is the compression ratio applied. A soft knee will progressively increase the ratio of compression as the gain input increases, so the compression is smoother.


See [this article](https://en.wikipedia.org/wiki/Dynamic_range_compression) for a more detailed description.

## Week 5: Delay and Filters 

### Delays
#### Basic components of delay

* Delay time - amount of time delay between the input and output
* Dry/Wet - dry is signal without delay; wet is signal with the delay.
* Feedback - how much of the wet signal goes back into the effect.

#### Some types of delay

* Comb filter: adding a short delay of a signal to itself. [More on comb filters](https://en.wikipedia.org/wiki/Comb_filter)
* Flanger: adding a changing short delay of a signal to itself. Really, it's a comb filter moving under the control of a LFO. [More on flanging](https://en.wikipedia.org/wiki/Flanger)
* Phaser: splits the signal in two and one of them goes through an all-pass filter which alters the phase of the signal. When the two signals become mixed a series of notches are formed. This effect is also modulated by a LFO giving an effect similar to flanging. [More on phasers](https://en.wikipedia.org/wiki/Phaser_%28effect%29)
* Chorus: multiple signals are delayed and their pitch modulated by an LFO. The signals are then mixed creating a chorus effect. [More on chorus](https://en.wikipedia.org/wiki/Chorus_effect#Electronic_effect)

#### Slap Back

* Medium delay with no feedback and single repetition.
* Gives an effect similar to reverb, but less pronounced. It's similar to what happens in a room when the sound hits a wall and bounces back.
* Works well on guitars and vocals for example.
Example of a slap back delay:
* Delay time: 80ms
* Wet signal: 20%
Tip: set delay time differently in left and right channels (e.g. 80ms in one and 90ms in the other) to get a wider sound.

#### Long Delays

Long delays are quite evident in a mix. They can result well but careful because they can get in the way of the harmony and clash with other instruments in the mix.

Some characteristics/tips of long delays:

* Delay time is often set not in ms but according to the tempo of the song. This helps synchronize the delay with the progression of the song.
* Panning the wet signal often helps keeping the dry signal in focus and make the effect less overwhelming.
* Setting a different delay time on left and right channels can result in a "ping-pong" effect.
* Filtering the wet signal helps the listener distinguish between the dry and wet signals (often HPF and LPF options are available in delay plugins).

#### Reverb

Generally there are two ways of mixing with reverb:
* Giving a sense that all instruments were recorded in the same space - use an aux send to route signal through a track where reverb is applied. In this case the aux track will only carry wet signal, since the dry signal is comming from each instrument's track.
* Using reverb as a more evident effect - use inserts on each track, and adjust the parameters individually for each one.

There are two types of reverb plugins:
* Algorithmic reverb - uses an algorithm to change the signal. Not as realistic as a convoluted reverb, but much more customizable.
* Convoluted reverb - uses recordings of actual spaces which are then "mixed" (more precisely convoluted) with the signal. It's more realistic, but one is limited to the spaces sampled.

Tip: usually it's a good idea to be subtle on using reverb. Often it's good to set the reverb level up to a noticeable point and then bring it back down so it's hardly noticeable.


### Filters
#### Overview

Filters let certain frequencies of the signal pass through while attenuating other frequencies. There are several types, including:
* High pass filter - frequencies above threshold pass through; frequencies bellow the threshold are attenuated.
* Low pass filter - frequencies bellow threshold pass through; frequencies above the threshold are attenuated.
* Band pass filter - frequencies within a range pass through; frequencies outside that range are attenuated.

All these three are quite extreme filters, in the sense that the attenuation of the frequencies is quite high. There are less extreme types of filter, namely:
* Shelving filter - similar to a high-pass or low-pass but the attenuation of frequencies is not complete, instead it's set at a certain level.
* Parametric EQ - allows to attenuate or boost frequencies, with high flexibility across the frequency spectrum.

####EQ tips

High end and low end:
* High pass filter - most frequencies bellow the fundamental (lowest) frequency of the instrument will be noise so removing those is always good practice.
* Low shelving filter - useful to emphasise the low end (after applying the HPF). Useful for bass tracks or give more body to a piano track for example. 
* High shelving filter - we are naturally attracted to brighter sounds, so slightly boosting the high end of the instrument you want to focus on (e.g. vocals) while slightly cutting the high end of all other instruments is quite effective.

Mid range: mixing in the mid range is the most challenging. Usually boosting in the mid range can be quite obvious and sound unnatural. Cutting can be quite useful though, for example to remove unwanted resonances.

Suggestion of a preset for the equalizer in a DAW (all parameters can be changed depending on instrument to be mixed!):
* High pass filter - threshold at 75Hz; high Q (e.g. 24)
* Low Shelving filter - threshold at 80Hz; Q adjustable.
* Low mid-range EQ - sweepable between 100-2000Hz; Q adjustable
* High mid-range EQ - sweep between 400-8000Hz; Q adjustable
* High Shelving filter - threshold at 12kHz; Q adjustable

When mixing a piece with multiple instruments it's sometimes necessary to do the EQ not on tracks individually but together. For example, a piano and guitar occupy the same frequency space, so mixing them together is a good idea.

####Stereo Width and the perception of Space in a mix

Usually, when mixing, the idea is to create a sense that the instruments are being played in a space. So sound should be perceived as coming from the right, left, from far away or close by. Creating this sense of space is challenging, but here are some tips. Don't forget: these are general rule-of-thumb tips, NOT RULES.

<b>Spread your instruments around the stereo space</b>

Generally what's in the middle of a mix is the point of focus of the listener.

In a typical mix, the middle would contain the following, in order from the most perceptible (or loud) to least perceptible:
* Hi-hat
* Vocals
* Snare Drum
* Bass
* Kick Drum

Instruments occupying the same frequency space as the vocals should be panned or have some effect that spreads them around the stereo field (e.g. chorus, flanger, etc...). Typical instruments where this happens are guitars and pianos.

* Balancing the left and right signals is always important
* It may be a good idea to pan different instruments to different sides of the stereo field so they don't clash into each other.
 * For example it's a good idea to pan the piano track to one channel and the guitar to the other.

<b>Create a Sense of Space</b>

To create a sense of space one has to think about where each instrument should be, as if it where really on a space: what's left, what's right, what's near or far from the listener?

To create a sense of direction, using delays can be quite effective. For example, if a track is panned to the right channel, applying a slight delay going into the left channel can be quite effective at creating the sense of spatial location of that instrument.

To create a sense of depth (how far each instrument is):
* Use volume faders - the further an instrument is, the quieter it should be relative to other instruments.
* Use reverb - the further an instrument is, the more reverb it will have. Adjust the dry/wet proportion or the send knobs for those instruments that are further away in the mix.
* Use a high shelving filter - instruments further away tend to have less high end than instruments near by, which sound brighter. Playing around with a high shelving filter of different tracks can help.
* Play with stereo width - playing with how much an instrument is spread over the stereo field can give a sense of where it is relative to the listener.


## Week 6: Synthesizer

###Signal Flow

Subtractive synthesis:
Oscillator --> Filter --> Amplifier

Each of these are typically modulated by:
* Keyboard input (which note is being played)
* Envelopes (dynamics and timbre variations as the note is played)
* Low-frequency oscillators (LFO) (tremolo, vibrato)

See [Loudon's schematic](https://d396qusza40orc.cloudfront.net/musicproduction/L06_a_overview-synthblock-01.png)

Other modulations are possible for effects. E.g. Noise/random, siren or sweeping effects using the LFO, inter-modulation between oscillators.

###Wave Forms

####Sawtooth wave

A [sawtooth wave's](http://en.wikipedia.org/wiki/Sawtooth_wave) sound is harsh and clear and its spectrum contains both even and odd harmonics of the fundamental frequency.

####Sine Wave

[Sine waves](http://en.wikipedia.org/wiki/Sine_wave) are representations of a single frequency with no harmonics

####Square Wave

A [Square Wave](http://en.wikipedia.org/wiki/Square_wave) will have only odd-integer harmonic frequencies. In musical terms, they are often described as sounding hollow, and are therefore used as the basis for wind instrument sounds created using subtractive synthesis.Additionally, the distortion effect used on electric guitars clips the outermost regions of the waveform, causing it to increasingly resemble a square wave as more distortion is applied.

####Triangle Wave

Like Square wave, the [triangle wave](http://en.wikipedia.org/wiki/Triangle_wave) contains only odd harmonics. However, the higher harmonics roll off much faster than in a square wave (proportional to the inverse square of the harmonic number as opposed to just the inverse).

#Categories
