[[ML3:Main|Â« Back to the main page]]

## Notation
Week 1 lesson 3 video "Linear classifier model" reviews some of the notation used in the course: https://www.coursera.org/learn/ml-classification/lecture/XBc9n/linear-classifier-model

Mostly, the notation is the same as what we saw in the Regression course.

* $$N$$ - the number of examples in the training data set
* $$d$$ - the number of features (other than the constant feature)
* $$\mathbf{x}_i$$ - the vector of features for the $$i^{\mathrm{th}}$$ training example.
* $$\mathbf{x}_{ij}$$ - the value of the $$j^{\mathrm{th}}$$ feature for the $$i^{\mathrm{th}}$$ training example.
* $$y_i$$ - the true label for the $$i^{\mathrm{th}}$$ training example. For example, when classifying examples as positive or negative, the true label would be +1 for a positive example and -1 for a negative example.
* $$\mathbf{w}$$ - a vector of weights (also called coefficients) that comprise a machine learning model.
* $$\eta$$ and $$\lambda$$ - tunable parameters. $$\eta$$ typically refers to the chosen step size for gradient descent/ascent. $$\lambda$$ typically refers to a parameter for some sort of regularization (e.g. L2 regularization).
* $$\hat{y}_i$$ - the model's prediction (predicted label) for the $$i^{\mathrm{th}}$$ example, typically of the test or validation data sets. When $$y_i = \hat{y}_i$$, then the model predicted the label correctly; otherwise, the model made a mistake.

## Mathematics
* "Sigma" notation (so-named because it uses the Greek capital Sigma letter) $$\sum_{i = 1}^N f(i)$$ is shorthand for the sum $$f(1) + f(2) + \ldots + f(N)$$.
* Product notation is analogous to Sigma notation, but instead of summing the terms, we take a product; $$\prod_{i = 1}^N f(i)$$ is shorthand for the product $$f(1) \cdot f(2) \cdot \ldots \cdot f(N)$$.
* The indicator function, $$\mathbf{1}[\mathrm{some\;condition}]$$ is 1 if $$\mathrm{some\;condition}$$ is true; otherwise it is 0.
* Probability notation, $$P(Y \,|\, w)$$, which is read "probability of Y given w", is the probability of event Y given that w occurred. For example, if you consider a fair six-sided die, the probability that you roll an odd number is 1/3, but the probability that you roll an odd number given that whatever you roll is greater than 2 is 1/2. This is because knowing that the roll is greater than 2 means that you must roll a 3, 4, 5, or 6, and of those four outcomes, two are odd numbers.
* A partial derivative $$\frac{\partial f(x, y)}{\partial x}$$ means the derivative of $$f$$ with respect to $$x$$ treating $$y$$ as a constant. For example, if $$f(x, y) = x\cdot y$$, $$\frac{\partial f(x, y)}{\partial x} = y$$.
