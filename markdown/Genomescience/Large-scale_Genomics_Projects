

<!-- START: ID Link References -->

[Annealing]: http://en.wikipedia.org/wiki/Nucleic_acid_thermodynamics#Annealing
[Bartter_Syndrome]: http://en.wikipedia.org/wiki/Bartter_syndrome
[Biotinylation]: http://en.wikipedia.org/wiki/Biotinylation
[Cancer_Genome_Project]: http://www.sanger.ac.uk/genetics/CGP/
[Cancer_Genome_Hub]: https://cghub.ucsc.edu/
[ChIP-Seq]:http://en.wikipedia.org/wiki/ChIP-sequencing
[Consanguineous_Marriage]: http://en.wikipedia.org/wiki/Consanguineous_marriage
[Cosmic]: http://www.sanger.ac.uk/genetics/CGP/cosmic/
[DNA_Copy_Number_Variation]: http://en.wikipedia.org/wiki/DNA_copy_number_variation
[DNase-Seq]: http://en.wikipedia.org/wiki/DNase-Seq
[Elute]: http://en.wikipedia.org/wiki/Elute
[ENCODE]: http://www.genome.gov/10005107
[Exome]: http://en.wikipedia.org/wiki/Exome
[F8-Gene]: http://en.wikipedia.org/wiki/F8_(gene)
[Haplotype]: http://en.wikipedia.org/wiki/Haplotype
[HapMap]: http://en.wikipedia.org/wiki/HapMap
[Histone_Modification]:http://en.wikipedia.org/wiki/Histone_modification#Functions_of_histone_modifications
[Indel]: http://en.wikipedia.org/wiki/Indel 
[Jun_Kinase]: http://en.wikipedia.org/wiki/C-Jun_N-terminal_kinases
[Microbiome]: http://en.wikipedia.org/wiki/Microbiome
[Missense_Mutation]: http://en.wikipedia.org/wiki/Missense_mutations
[Moiety]: http://en.wiktionary.org/wiki/moiety
[Oligonucleotide]: http://en.wikipedia.org/wiki/Oligonucleotide
[RNA-Seq]: http://en.wikipedia.org/wiki/RNA-Seq
[SNPs]: http://en.wikipedia.org/wiki/Single-nucleotide_polymorphism
[UCSC_Bioinformatics]:http://genome.ucsc.edu/index.html

<!-- END: ID Link References -->

###Large-Scale Genomic Projects
#### Introduction
The previous section covered how the finished human genome sequence was prepared beginning with the initial draft sequences and all the work and effort that went into getting very-large accurate sequence segments. These prove to be extremely valuable for the next phases of the project which were really revolutionized by  the development of next-generation sequencing applications.

The critical complement of the finished genome sequence and the next-gen applications is perhaps under-appreciated because the next-gen sequencing applications generated by-in-large short sequence fragments and did not provide a simple way to get to whole genome assemblies.

Almost all of the next-gen sequencing applications are directly dependent on the mapping of next-gen reads to the human reference sequence. So the high-quality reference sequence of the human genome was critically important for these next phases of applying genomics to human biology.

The previous section also cover the initiation of the next-gen sequencing methods and a little bit about the methodology. This section will cover a few specific applications and also a couple of the large-scale genomics projects launched that have leveraged next-gen sequencing applications.

These are meant to be a sampling, they are certainly not comprehensive. Later lectures in this course will cover more specifically some of the additional next-gen sequencing applications, particularly [ChiP-Seq][ChIP-Seq] and [RNA-Seq][RNA-Seq].

Next-gen sequencing methods really drove down the cost of sequencing to the extent you can now get a whole genome sequence for less than $10,000. Some people say seven, some people say five, with the ultimate goal being able to get a full-human genome re-sequencing done for $1,000 or less. Once you reach that level it becomes a tool that is very affordable and can be incorporated into routine medical applications.

The initial impact of cheap DNA sequencing is that a plethora of applications were developed to probe biology. Whole genome sequencing we've talked about. [Exome][Exome] sequencing is very similar to that. But also sequencing all the RNA in a genome, sequencing all the micro-RNAs, doing experiments where precipitate proteins that are bound to DNA and finding out where all the localization sites are in the genome, ChIP-seq and [DNase I hypersensitive][DNase-Seq], doing experiments where you probe the binding of proteins to RNA sequences. All kinds of different innovative and imaginative applications were developed based on the ability to sequence lots of DNA very cheaply and generate sequence tags that can be mapped back to the human reference sequence.

The large scale genomics projects, of course, the original large scale genomics project was the human genome project. A whole series of additional projects have been developed based on the human reference sequence including the encode project, the 1000 genome project, and the cancer genome project.

Those first three projects I'll talk about today and later the [microbiome][Microbiome] will have it's own lecture in this series, as well as a whole host of other projects focusing on creating large data sets that can be shared by a particular biological community whose interested in the data for developing experimental systems, different kinds of approaches to studying problems, and so forth.

What we'll talk about today, as far as next-gen sequencing applications or the whole genome sequencing route, I have shown example of a paper that uses this exome sequencing,

Similarly I will briefly introduce the method and then talk about, go through a paper that uses this, as well as the personalized copy number segmental duplication maps that can be constructed from a whole genome
re-sequencing data.

#### Whole Genome Re-Sequencing
So whole genome re-sequencing... one paper I selected is an interesting case where this was applied is where ten genomes from hemophilia patients and ten controlled genomes were all sequenced to high depth of coverage choosing paradigm luminous sequencing. So you had 75 base pairs paradigm sequencing done on these genomes.

And I already mentioned that like all the next gen methods, the data requires, as an initial step, an alignment to a reference sequence. Through those alignments in comparison with the reference sequence you can detect all the single nucleotide variation, all the point mutations, all the single base changes between
the genomes. And very similarly, small [indels][Indel] can be detected and characterized using the reads themselves.

You need to use the paired-end infiltration to get information on structural variation in larger deletions, insertions, inversions, trans-locations, and so on and so forth. So paradigm reads are quite important for the success of this approach.

A reminder once again of the pipeline for the luminous sequencing. You go from the massively parallel sequencing to base calling to the alignment of high-depth of coverage reads to the reference genome. By comparing the aligned reads to the reference genome you can detect the variations -- the small variations  by the read alignments and the large variations (the structural variations and so forth) by the paired-end organization of the reads from the fragments in the library.


This table (insert table) summarizes the data and the depth of coverage in this particular data set. You can see that there is between twenty and typically 50X coverage of these genomes and very, very good coverage of all the exomes.

This was a whole genome data set. The entire sample was converted into paired-end format and sequenced and then mapped back to the reference genome. One thing to remember is that not all of the data is mappable back to the reference genome, particularly in cases where there is a lot of repeat structure and segmental duplication content the reads cannot necessarily be uniquely mapped back to their source to the genome. So there is always going to be holes of the analysis of these whole genome and exome sequencing experiments.

An interesting characteristic of the data seen early on is that you see all of the variation that has been seen before and there is always a set of new information with each genome. These were unrelated individuals so they should not necessarily share large [haplotypes][Haplotype] or anything between them. Here about twelve to fifteen percent of the [SNPs][SNPs] and the small variations are always novel. This suggests that there is a significant fraction of human variation that's either private or very rare in the population. This is certainly interesting and perhaps quite important in thinking about human variation and the kinds of variation that can cause [phenotypes][Phenotype].

A lot of the initial studies using [HapMap][HapMap] were based on common variants and trying to do association studies
of those haplotypes that contain the common variates with different human phenotypes. It turns out that a significant, a very significant fraction of the variation in the human genome is caused by very rare variates or even private variates. So the ability to completely sequence the genome is going to be really critical in trying to understand human variation and human disease.

This slide shows the concordance between the genotyped SNPs. For all these genomes they carried out conventional a genotyping analysis for the common SNPs. When they compared that with the SNPs that were seen using a whole-genome sequencing analysis, they found a very high concordance but not a perfect concordance, and this is typical of all of these experiments.

Many of the non-concordance reads turn out to be sequences where the SNP that is being interrogated in the genotyping experiment has a second SNP very close to it and that second SNP prevents the proper [annealing][Annealing] of the anglenoid (?) nuceotides that are required for conventional genotyping and so you can only detect these kinds of variance using the sequenced based approaches.

A large and perhaps surprising number of coding region mutations were found in all these genomes.  Interestingly, when there were indels in the coding region they were enriched in every third nucleotide. This is consistent with preferential insertion or deletion of nucleotides that correspond to a complete amino acid so that you do not frame shift your protein with these mutations.

Nonetheless, there are also a fair number of mutations that disrupted the coding sequence, so in every individual there are actually a surprising number of protein encoding genes that are non-functional and not simply in the heterozygous state but also the homozygous state. Everyone of us is walking around with basically no alleles of some protein encoding genes.

I mentioned earlier the patients whose DNA were sequenced were selected, ten of them had hemophilia A. The
known major mutation for hemophilia A is in the in the F8 gene. This was confirmed by the whole genome sequencing experiment for five of the ten individuals.

One thing to note is that other genes that were found to be mutated and deleted homozygously in individuals were also found to be homozygously deleted in the controls. That is not the case for the [F8 gene][F8-Gene]. So it is important to always have the ability to compare the genomes of the affected individuals with controlled engomes so that you can get a sense for which genes are preferentially affected in the diseased individuals because there are always going to be a fraction of genes which appear to be mutated which in fact, are mutated, but may not have any functional consequence.

In looking at all of the data from these genomes, I already mentioned there is a certain level of private or rare variance in all of the individuals, that number, at least in this data set, comes to about 140,000 or so single nucleotide variations that are very rare or even private. And that seems to be a level that is likely to be present throughout most of the population.

In addition the way that they determined the homozygous null mutations in these genomes, which was a little bit  more loose than other studies, they found an average of an 165 homozygous null mutations which means that genes that are predicted to encode proteins are deleted or non-functional in both of alleles in the particular genome. So out of the 20,000 to 25,000 protein encoding genes, there is a small but significant number in any given genome that appear to be non-functional.

So, whole genome sequencing is obviously an extremely important and powerful technique but a more efficient way to get at the protein encoding fraction of the human genome is to use a targeted sequencing approach.

#### Targeted Sequencing
Other applications of targeted sequencing include selecting particular chromosome regions that may have been
identified through genome-wide association studies or other kinds of studies and targeting those chromosome regions for re-sequencing in a large number of individuals -- perhaps a number of disease cases compared to a number of controls to try to determine which actual sequence variants maybe causing the phenotype or the disease that you are interested in.

Targeted sequence capture followed by massively parallel sequences has evolved as a very powerful technique and a way to very cheaply, relatively speaking, get at the DNA sequence you are the most interested in from a very large number of samples.
(?)(?)The way this method works. And I'm showing, showing it in this case for a region that I'm interested in,
the human sub-telomere region.

You start with genomic DNA and create a library of fragments and put on the illumina adapters, as one typically would. In my case, I'm actually using a sequence tag to physically tag the tips of the chromosomes.
Once you have the fragment library, you go through a targeted sequence capture step. I'll show you the two main ways that's done in a moment. But once you've selected your DNA from the whole genome using this targeted
selection method, you amplify that part of DNA and go through that high-throughput paradigm sequencing.

There are two main ways to do the targeted sequence capture. First through a custom micro-ray. In my case of subtle agilent (?) type micro array that is capable of annealing to the complimentary DNA that is in the luminal library that has been prepared, and sticks to the array, and then you wash away the non-specific DNA and [elute][Elute] the specifically binding DNA, and you have your selected target region.

A similar approach but using RNA, instead of DNA, is to start out once again with a large set of long DNA molecules, but create RNA replicates of those that are [biotinylated][Biotinylation]. The RNA that is biotinylated is then used to do a solution hybridisation reaction with the illumino library and, once again, the complimentary DNA sticks to the RNA. You can use the biotin [moiety][Moiety] to effectively pull out affinity purified the anneal parts of the genomic library (?) and then you can digest away the RNA and you are left with the DNA that has been selected.

So either of these two methods actually works extremely well. If you make your targeted microarray or your long RNA pool specific for [oligonucleotides][Oligonucleotide] that cover only coding regions of the human genome or the exons, it is called an exome microarray. There are now commercial exome microarray approaches available that very efficiently enrich the illumina library that was prepared from whole genomes for only those fractions of the genome that contained the exons. These provide very powerful approaches for very efficiently getting at the protein coding regions of the human
genome.

This paper out of out of Liftons lab shows an extremely powerful approach of exome capture. Here they are  looking at individuals. In particular, one individual who has a phenotype that looks like something called [Bartter syndrome][Bartter_Syndrome], which is a renal disease, a kidney disease.

It turns out that after doing exome sequencing on this individual they actually identified the mutation as being in a chloride channel, and so the individual actually did not have classical Bartter syndrome but instead a condition called congenital chloride diarrhea. And this is just one example out of hundreds of examples that are now available where, where exome sequencing has been done on individuals with particular diseases, and the causative mutations have been found.

So, to take a quick look at the kind of data that you get from, from these exome capture experiments. In this case there were five individuals who were sequenced, one of whom had a phenotype that was of interest and that individual is the G-I-T 284 individual. And you see you get very similar kinds of statistics from normal individuals and diseased individuals in terms of the numbers of [missense mutations][Missense_Mutation] that are found and the number of premature termination mutations that are found splice variants and so forth.

So there are always more of these kinds of mutations that are expected to lead to nonfunctional proteins that can actually cause the disease, so a critical step is always that last step to sift through these candidates and try to find the one mutation that is actually believed to cause the disease. The way that that is typically done is to take advantage of other kinds of biological information that you might have.

For example, in this case they looked at all of those mutations that caused what appeared to be non-functional proteins, which of those proteins were actually highly conserved evolutionarily. Where did the mutations occur? Did they occur in an evolutionarily conserved part of the protein where the change in amino acid might cause a mutation that would cause a nonfunctional protein?

They narrow their list of candidates using that kind of an approach and by doing so they come up with a
single or just a few that they can then test individually using PCR-based approaches on additional patients that have similar kinds of conditions.

So this describes the sensitivity in the coverage of the exome sequencing itself. What, one can see from panel A is that while you get a high depth of coverage for most of the basis representing the exons, there is always a small subset of  coverage that is rather poor and this corresponds to what I mentioned earlier in terms of very GC rich and very AT rich segments of the genome. Some of these occur in the exonic regions and coverage is not complete so that is one of the drawbacks of the approach, but itis a fairly small fraction, and you can capture greater than 93, 95, 97 percent of the exome sequence, with ten or more reads covering each base, which gives you almost complete coverage but not total coverage.

B shows the error rate of the short reads. As you get longer and longer reads you increase the percentage of error. Even as you approach 75, it is still less than three percent error per base. But clearly, one needs to take that into account when when looking at potential, what, what you identify as potential mutations. You wanna make sure that you have sufficient read coverage and that different parts of the reads cover the candidate mutation so that you can be sure that the difference that you are seeing is real and not an error.

Coverage in both cases, the depth of coverage reaches a plateau at about between 20 and 50 x typically. So by the time you get to 30 or 40 x coverage, you are getting all the information that you are likely to get from these experiments.

Now, in this particular paper the affected individual's actually part of a large family that included some [consanguineous marriages]{Consanguineous_Marriage], Which means first or second degree relatives were marrying each other, and in those cases you would expect much of the genome to be homozygous since the related individuals were mating with each other.
Because this is family structure, one would expect to see a recessive gene that would be homozygously deleted in the case of the mutation. And that is in fact what they found.

In this particular study they found a single gene that had a homozygous change that was predicted to create a nonfunctional protein, in this particular chloride channel. Now importantly, this was still a candidate mutation, in that it was only
found in this one individual, among other possible mutations that might have caused the disease.

When they went into a pool of additional patients with a very similar kind of phenotype, they found that in fact, in this very same gene, they found many additional mutations that were capable of causing the same phenotype, which is basically this chloride deficiency. And in each case it is the mutation that causes a nonfunctional protein.

#### DNA Copy Number and Variation
So a third application I wanted to talk about very briefly is how you can actually use whole genome data sets to look at a
DNA copy number and [DNA copy number variation][DNA_Copy_Number_Variation] in individual g-nodes.

Now we talked a lot about SNP and SNP variation and small nucleotides, and we talked to some extent about structural variation, including insertions, deletions, and inversions. 

Copy number variation is a kind of structural variation where you have more, you know greater or fewer numbers of copies of a given segment of DNA. Genes embedded in these copy number variance can obviously be present in more or fewer copies depending on what the level of how many copies of that particular variant an individual has So the way that the whole genome data sets are used for this, is to simply count the number of reads all the way across the genome. In this case across 100 base windows.

As you slide those across the chromosome you do a count of how many mapped reads there are. In cases where you have fewer numbers of copies in a source genome relative to the reference genome, you get what appears to be a deletion. Where you have more copies, you get what appears to be an insertion. This simple principle was applied genome wide. Algorithms were developed to very efficiently enable detection of these copy number variants.

(Insert Image) This shows a particular region of chromosome one where the data indicates a fewer number of copies relative to the reference sequence. 

(Insert Image) And this slides shows several different regions of chromosome one in different individuals. You see that different individuals have different numbers of copies of those particular segments in each of the different cases. These  are from different segments of chromosome one.

Clearly DNA within these segments are in different copies in different individuals. Any genetic content, any genes present, would be present in more numbers, in higher numbers in people with duplications and additional copies, and in fewer numbers in people with deletions.

(Insert Image) Statistics from this particular study along chromosome one are shown here. You can take a look at that. Now in a more thorough and broader study that was done by Alkan et al in 2009, a genome wide assessment was done of this
copy number variation. A list of the 30 genes in the human genome with the most variable copy number are shown here.

This is quite interesting because you can see the very broad differences, the very dramatic differences in many of these genes. All of which have functions in the human genome.

One of the challenges to is to detect these variance and ask the question what does it mean to have a much fewer or a much greater number of some of these genes in an individual? That is still an open question. There are a number of studies that suggest that copy number variant regions perhaps play a role in certain diseases. There are followup studies underway to determine whether, in fact, that's true.

So the large-scale genomics projects are once again based on the human reference sequence and they have a number of
properties in common with each other. They are almost always interdisciplinary and involve large consortia, which are large, large, sets with large numbers of individual laboratories contributing to them.

Computational bound informatics resources are critical for all of these projects to be able to handle the huge amounts of information that are being produced, to organize the information and to figure out ways to display the information that
scientists can understand and people can use to gain access.

To this information almost always there are very rapid release policies in public data access to these data sets. There are typically large scientific communities that benefit from each of these large scale genomic projects, so there are centralized websites, data centers that analyze the data and distribute it to the whole community.

As I mentioned, the human reference sequence is the foundation for many of these large-scale genomics projects. But, I should mention as well that model organisms, where the reference sequences are finished, also form the foundation for a number of large scale genomics projects that are similar in principle but focus more on understanding the function of the model organisms and processes that can be studied in the model organisms.

All these projects are heavily dependent on next-gen sequencing to very rapidly and cheaply generate information on all
kinds of different aspects of the reference sequence.

#### 1000 Genomes Project
The 1000 genomes project is one such large scale genomics project. It grew out of the hapmap project, which was a project to understand the common variation in the human genome. The 1000 genomes was an effort to use next-gen sequencing to probe beyond just the common variation to identify all the sequence variants that are present in at least one percent or at a frequency of one percent in the population, and also to get information on a lot of the rarer variants, and to create a database of this information and make it available in forms that people can use for their genetic studies.

The initial plan for the full project is to sequence about 2,500 genomes at 4x coverage. To use that information to get
SNPs, primarily SNIP information because any particular region of a genome contains a limited number of haplotypes, and this was know from the Hapmap Project data that could actually be combined across lots of samples to allow the efficient detection of most of the variants in a region.

If you had all of this information from all these different genomes, you'd have multiple copies of each of the major  haplotypes and you could leverage that information. So combining the data from all these samples would allow highly accurate estimation of variance in genotypes for samples that were not seen directly perhaps by the light sequencing of
individual samples.

The way the data is being used is they are combining this 1000 genomes data with genotyping data from, GY studies that use primarily tagging kinds of SNPs to identify additional variance beyond the SNPs that are genotyped directly. Once again this is based on the understanding that there are a limited number of haplotypes in the population and so you can predict from the 1000 genome data where some of these additional SNPs are likely to map, even if you have not genotyped them directly. That is a process called amputation.

The data is available in almost all of the variance with the frequency of at least one percent, plus a large number of
additional rare variances. These are used in population-based studies of recombination selection and looking at different kinds of population structures.

To give you a sense from the HapMaps, since we really have not talked about that very much yet, this is a screen shot from the HapMap (Insert Image) website of a particular region of the genome and the DNA is organized into regions of linkage 
disequilibrium that vary from population to population, and all of this information was gained initially through genotyping
experiments using common SNPs.

With the 1000 Genome data you can overlay this rich data set of additional inputted (?) genotypes on top of the common variance to more precisely define intervals where associations between the variance and disease phenotypes occur in the genome. The 1000 Genomes Project leads directly to the concept that personal genomes may in fact be very valuable tools in
understanding an individual's susceptibility to disease.

In personal genomes, as the term implies, is the complete sequence of an individual's genome. So in addition to the common variants that one would be able to find using genotyping, you would also get access to all the rare variants and private
variants. The idea is to use this information to help define disease susceptibilities. If you know you're susceptible, for
example, to heart disease, you may be able to modify your behavior to, to live a healthier life.

In addition, if you have susceptibilities to certain drugs, that could be identified by sequence variance in your genome, then you'd know which drugs to avoid, which drugs are likely to be effective for particular diseases. 

Importantly for specific diseases like cancer, where actual genome mutations occur that lead directly to the disease itself, you may be able to use the particular changes that occur in the cancer genome to figure out which particular drugs, in theory, might be the most effective in treating that particular cancer.

Most of the promise of personal genomes has not yet been realized. There have been individual cases, in a few
cases where it's actually been used very effectively. But while it's very straight forward now, to get the information. It is still very difficult to try to associate that information with particular diseases and susceptibilities. There just isn't enough information yet, enough biological information available to understand what the variance mean.

#### ENCODE Project
So a second important large-scale project is the [ENCODE Project][ENCODE]. The ENCODE is basically an attempt to
assign function to DNA sequence and that is done at many levels. At the level of annotating all of the genes in the genome. Looking at all of the regulatory sequences in the genome. And looking at more recently the epigenome and how the locations and organization and patterns of [histone modifications][Histone_Modification].

In particular cell types under particular conditions may be affecting the function of the, of the genome, and of the cell.
And so, all of the, the next-gen sequencing methods that I talked about earlier are applied to gathering information that can then be mapped back to the reference sequence to give clues as to what kinds of functions are happening in the DNA.

All this information is collected on a a single website hosted by UCSC, the [UCSC Genome Bioinformatics site][UCSC_Bioinformatics]. It is a very data rich and powerful tool for comprehensively organizing all the information that we have that is related to that particular segment of DNA. An example that is shown here where we are looking at a 40 or 50 KB region of the genome and the RNA-Seq that are identified in the gene models at the top of the figure and below you have tracks you can actually select among all the data available from the ENCODE Project.

To look at things like the expression of RNA sequences in different cell types, patterns of histone modifications, patterns of hypersensitivity sites, DNase 1 hypersensitivity sites, methylation patterns under different cell types and different conditions, and in the organization of DNA sequence repeats and duplications, and copy number variants at a given region of
the genome. And all this information can be organized and laid out in tracks underneath the genome sequence itself to try to get as comprehensive an understanding as one can of the of the functional elements that are present in that segment of DNA.

#### Cancer Genome Project
The final large-scale project to talk about is the [Cancer Genome Project][Cancer_Genome_Project]. This is an international effort to get at the genetic cause of cancer. Cancer is not a single disease, it is hundreds, thousands of individual diseases and each cancer has its own characteristics, biological properties. In fact, each cancer can be divided
into sub-types that have characteristic features that they share. Ultimately, each cancer is essentially an individual disease. 

The way cancer is dealt with by an individual is going to be different depending on the person. Thus the ultimate power of the Cancer Genome Project is to gain enough understanding of the different types and subtypes of cancer and the different
stages of cancer so that one can identify targets for intervention. The project will also help develop ideas for predicting which cancers are likely to be aggressive, which are likely to be susceptible to intervention by which therapy -- radiation or chemotherapeutics or, more recently, molecularly targeted therapies designed to disrupt particular pathways that are known to be mutated in the particular cancer type.

So once again, next-gen sequencing has taken a lead role in this large-scale genomics project in terms of understanding the mutations that occur during tumor progression as well as the gene expression changes that occur and in developing an understanding about which pathways and what genes are affected.

In this particular paper they looked at breast cancer and the copy number changes that occurred in a large number of
individual breast cancers as well as the mutations that occurred by doing a complete exome sequencing of 100 different breast cancers.

[[File:GenSci\_L04\_LandscapeDriverMutationsInBreastCancer.png|200px|thumb|left|Landscape of Breast Cancer Mutations]] There The two major categories that the cancer is clustered in using estrogen receptor positive or estrogen receptor negative typing are shown on the left and the right respectively. The individual cancers are represented by each  column. In this figure the individual genes that were found to be mutated are shown in each row in these different cancers.

One can see that there are different patterns of mutations that can be identified in the cancerous, and different mutations in copy number changes. And the take-home message form this particular paper was that they found a number of additional genes that had not been discovered previously in breast cancer. Those are listed, Map 3K1, Map 3K13, Akt2, ecetera.

They also found several non-overlapping driver mutation patterns. Driver mutations in cancer are those mutations that actually lead to tumor evolution and cancer progression. There are lots of mutations in cancer genomes, many of which have no functional role in the cancer process itself.

One of the major challenges is and remains understanding which of the mutations are actually driving the cancer and which ones are just occurred during the process and are taken along for a ride, the so-called passenger mutations.

They found several non-overlapping driver mutation patterns suggesting that those groups of cancer with the different patterns are likely to have different biological properties. Perhaps be more-or-less susceptible to different kinds of therapies, and so-on-and-so-forth.

In this study they also found several new genes present in the same signaling pathway, the [jun kinase][Jun_Kinase] signaling pathway. This suggests that this particular pathway may be quite important for breast cancer tumor progression and highlights a particular pathway that might be targeted in developing drugs or trying to figure out how to intervene
and cure the cancer, or at least inhibit the cancer.

[[File:GenSci\_L04\_CosmicCancerGeneData.png|200px|thumb|left|Cosmic Cancer Gene Data]] 

So this was just one study and one cancer genome project. All of the information from all of the mutations from all of the studies that are being done are being gathered in a centralized database called Catalogue of Somatic Mutations in Cancer or [COSMIC][Cosmic]. This is a way to collate all of the information in a central location. Not just the mutation information
but a lot more information -- what publications are associated with the studies, what samples types were used to do the sequencing, what kinds of cancers were involved, what were the cancer characteristics, ws the cancer in situ or invasive
tumors, was the cancer a recurrence, was it a metastases of cell lines, ecetera.

All this information is gathered into this single database that can be queried from a number of different perspectives -- by tissue type, by cancer type, or by gene -- to look at all the different kinds of changes that happened in particular genes. 

(Insert Image) This is a an example of a gene-centric query. This is the BRAF gene. The purple areas show where mutations have been found in the gene in particular cancers. You can see where the different breakpoints have been found to occur in
this particular gene in different cancers, as well as the copy number changes, loss of heterozygosity, zygosity and amplifications. You can drill down into each to these categories to get additional information on, to help understand the role of B-RAF in tumor progression in these different kinds of cancers. 

In addition to the consolidated data, you can also access the raw data itself by accessing the [Cancer Genomics Hub][Cancer_Genome_Hub], which is a repository for all of these data sets. In this case you need to be registered for this because there is obviously patient information. Once you validate that you are a researcher and agree to certain protections of the data, then you can actually dig into this data and do your own analysis of the data to try to understand it from whatever hypothesis or perspective you're interested in.

This highlights the major value of these large-scale projects -- they leverage all the data from all of the groups that are participating in the analysis. So, if you come up with a new analysis technique you can immediately go to the rich data collection and analyze all of these different kinds of data sets using your technique. In doing so any individual laboratory has access to the data from all this large group of consortia that generated the data initially.
