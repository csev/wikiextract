Data Analysis, Week 2, Lecture 8 Data munging basics  

* This video is about the basics of data munging or processing. Data munging or processing is required for basically every data set that you will have access to. Even when the data are nice and neatly formatted like you get from open data sources like Data.gov, you'll frequently need to do things that make it slightly easier to analyze what make character name slightly better for use in modeling.  

**Recall Tidy Data**  

* So the goal is to get to tidy data, where each variable forms a column, and each observation forms a row, and each table or file stores data about one kind of observation.  So for example, the data about people are stored in one table and the data about hospitals are stored in another table.  

**Where we would like to be**  

* So the other components, in addition to the three components of tidy data, are that we might want to have column names that are easy to use and informative. Row names that are also easy to use and informative, and then we've removed obvious mistakes in the data and we have variable values that are internally consistent.  

* So, a very common occurrence is if you have a character vector representing the sex of patients in a clinical study, you might have men labeled as both *men* and *male* and we might want to consolidate that so that only *male* is used as the indicator of male sex.  

* The appropriate transformation should be applied to variables as well, so that they can be used in downstream analyses, without having to transform the data over and over. This is typically done when you're analyzing a type of data set that you know that there's a particular transform that will be useful for you later on either because you've seen it exploratory analysis or it has been in the past.  

**A partial list of munging operations**  

* So this is a very partial list of data munging operations. So you might fix variable names, create variables, merge datasets, reshape datasets, deal with missing data, take transformations of variables, check on and remove inconsistent values, and many more things. Honestly, this is just the very smallest part of data munging or processing. I'm going to try to illustrate the functions that are most useful for the greatest variety of data munging operations, so that you'll be equipped to be able to process whatever data comes your way.  

* Remember that the steps of data munging or processing should be recorded in their own script, R script that's labeled, so that you can tell where the process data come from. 

* Just to tell you the truth, 90% of the effort of data analysis is often spent on data munging. This is not where the glory is, but it is where the work is. And so, to be able to analyze the data very quickly and easily usually requires a ton of processing effort. But it's usually worth it because then you end up with data sets that you could easily share, distribute and communicate to other people.  

* So this lecture is going to focus on these first four components, fixing variable names, creating new variables, merging data sets and reshaping data sets. The last three components will be covered in the future lectures.  

**Fixing character vectors - tolower(), toupper()**  

* So first, we're going to talk about fixing character vectors. So, in general, these, commands that I'm going to be talking to you about can be used just as easily to deal with character vectors that are included as part of the data set. But I'll primarily be applying them to deal with the names of the data frame. And, the reason why is because, when you’re doing your analysis, it's often easy, easy to get confused if the names of the data frame are either inconsistent or have strange names that you’re not used to or have weird capitalization and so forth. In that case you might accidentally call the wrong variables or not be able to figure out what the right variable to include in your model.  

* So this is the cameras data that we downloaded previously from the fixed camera locations in Baltimore. And so I'm going to read that data in with read.csv() and look at the names of that data set. 

* So the names of the data set is just a character vector and so each character string, is listed here. There are actually six columns of the camera data. And so you could see that for example for the first three elements of this vector are all entirely in lower case while the fourth vector, or element of the vector has a capitalized letter. Similarly, the sixth element of the vector has a .1 at the end which might be confusing later when you are looking for the location variable.  

* So, the first thing you can do is use the tolower() function applied to a character vector to make all of the letters lowercase for all of the elements of that vector.  

* So for example you see if we apply tolower() to the *names* vector, from the *cameraData*, we see that say the *crossstreet* variable, or element, now no longer has a capitalized S in it. Similarly the *location* element now has a lower case l to lead with.  

**Fixing character vectors - strsplit()**  

* You could also use string split {strspilt()} to separate out components of elements of a character vector. So, it's very good for automatically splitting up variable names that are confusing. The most common occurrence is they might have a dot followed by a number. And you might want to remove that dot number to make them a little bit easier to work with.  

* So, what you can do is use the string split command. And so, what that is, is you apply string split to a vector of characters. And then, what it returns is a list where, in each element of the list, the character, at that position in the vector, has been split at the argument that you define by split.  

* So it's best to see this with an example. So we have this names vector for the camera data, so this is the names of the data frame, and you remember the last element was *Location.1* and the second to last line, the fifth element was *intersection* without any dot. So if I apply a string split to this character vector and I split it at the value of a period, then I will see what results {*splitNames*}.  

* So, when I want to split on a period, this is actually a special character used in R and so we need to actually escape this special character. So we have to use this \\ before the period. But this is just saying we would like to split every element of this character vector up whenever we see a period.  

* So, if we look at the fifth element, that was intersection and there were no periods in that element and so what we see is that *intersection* gets returned at the fifth element of this list just as it was before with no splitting.  

* However, the last element of the character vector was *Location .1*. So if we look at the last element of the list that's returned and we do this by accessing an element of the list, we do this by using double brackets [[ ]] here. We see that the sixth element and the last element of that list actually returns a character vector itself where this new character vector has *Location* and *1* and the dot has been removed. So basically what it does is, it goes through and any time it sees a dot it splits that character vector, that character element up into a character vector where everything that came before the dot is the first element. Everything that came after that dot is the second element and the dot has been removed. So, why is this useful?  

**Fixing character vectors - sapply()**  

* Well, we can use the sapply() function to leap over all the names of that data frame, and remove everything after the trailing dot for every element of that character vector. So recall that *splitNames* was the list that we got from the string split function and so the sixth element of that list was a character vector that had *Location* as the first element and *1* as the second element and the dot had been removed.  

* So if we select just the first element of that character vector, we can see that it's the *Location*. And so what we would like to be able to do is loop over each of the names in the character vector and just select the first component of that list that results.  

* So what we do is, we define a little function called firstElement(). And so, all this function does is it takes a character or a numeric vector in and returns just the first element of that vector. So then what we can do is apply that function to every element of the list and so what we do is we apply the sapply() function to the splitNames (that was the list that came out of strsplit) and what we do is for each element of that list we apply the first element function.  

* And so what it does basically is, it applies to that list this first element function and selects out only the first element of the character vector. So, what you get as a result are a set of names that no longer includes the “.1” at the end of *Location*. This seems like sort of a convoluted way to remove just the “.1” at the end of *Location*; and in this data set, it may not be necessary to go through such hoops. But in general, you might have lots of character names that you need to change all automatically. And so, the sapply() function allows you to do that very quickly.  

**Peer review experiment data**  

* So now, we're going to use the peer review data set again to explain a couple of other data munging operation that you might perform. So this is data on submissions and reviews of SAT questions. So each question gets answered by a particular person; and then that question gets randomly assigned to another person to review their answer and determine if it's correct or not.  

**Peer review data**  

* So again to load in the peer review data, we actually have URLs of the review solution datasets that are on DropBox, and we can download those files, using the download.file() commands that we've learned about earlier in getting data. And then what we can do is load in the reviews and solutions datasets separately with the read.csv() command. So at the end of the day, we have two data sets, the reviews data sets and the solutions data sets, and what we would like to be able to do is look at those and see if there's anything that we need to do to process them and maybe if we can put them together into one data set.  

* So, the first thing we do is we look at the top part of the reviews data set. We can use the head command and look at only the first two rows and so we see that there are elements for *id*, *solution_id*, *reviewer_id*, and so forth. And they all appear to be quantitative.  

* Similarly if we look at the top part of the solutions data set we see similar sorts of variables. But, now we see a *problem_id*, and an *id*, and a *subject_id*, and so forth. And here we think that maybe the *answer* variable is qualitative because it's reported here as a character.  

**Fixing character vectors - sub(),gsub()**  

* So one thing that we might want to do again is dealing with the names of the data frames. So if you look at the names of the reviews data frames. We see that, there is actually this underscore character in several of the names. So that might be useful or it might be an inconvenience, in particular, if you're dealing with data sets where you have to match names across data sets. Sometimes by having this underscore in one set of names and not in another you'll cause names to not be able to match. So what we might want to be able to do is substitute out this underscore character and replace it with something else.  

* And so you can do that with this sub() command. And so what you do is if you apply sub() to the vector of names of the reviews, but what you can do is you can say take the character underscore and replace it with nothing in each element of these character vector of names of reviews.  

* And so what you get instead of what you saw originally where you have underscores in several of the characters, you actually end up where that character has been deleted because we replaced it with no space at all. You could also replace it with a comma or a period or whatever else you want, and then you would end up between *solution* and *id* with whatever character you had placed in here in the replacement value.  

**Fixing character vectors - sub(),gsub() continued**  

* So an important thing to keep in mind when you're using sub() is it only replaces the first instance of a particular character string that you want to replace. So, this is a test, it has nothing to do with the data set. So if we create a test name that has several underscores in it (this_is_a_test_here) and we apply the sub function to substitute the underscore with nothing in this variable, testName, the first time that an underscore appears, it gets replaced, but everywhere else it does not get replaced.  

* So instead of using the sub command, if you want to replace all underscores you can use the gsub() command. So the gsub() command will again replace underscore with nothing in testing but now it goes through and every time it sees an underscore it gets replaced. So what you end up with is now is a character vector that has no underscores in it.  

**Quantitative variables in ranges - - cut()**  

* So another thing that you might want to do when data munging is actually take some quantitative variables and turn them into ranges. This is because sometimes you what to be able to look at variables at a glance or sometimes you think that you don't necessarily believe the precision of the quantitative variables you want to look at, just ranges of those quantitative variables and to do that you can use the cut() function.  

* So if we look at the, reviews data set and look at the *time_left* variable, it's a quantitative variable. And if we look at the first 10 elements, you actually see there's one missing value but then these are the numbers of the time_left for each of those reviews. And so, what you can do is, actually break that up into different ranges. And so, what you do is, you apply cut() to the variable reviews *time_left*. And what you can do is, you tell it where you want it to cut that variable up.  

* So here, what we've done is we've passed it a sequence, so it's a sequence from zero to 3600 by 600. So this sequence occur, happens at zero, 600, 1200. 1800, 2400, 3000, and 3600. And, so what you do is, apply this cut function, and what you end up with is each element appears in between the two values that occurred in this list of cuts.  

* So if we, again, look at the first ten elements of this new cut up vector. What we see are the first element, 1754, is between 1,200 and 1,800. So it gets labeled as 1,200 to 1800. The second element 2306 appears between 1800 and 2400 so it gets labeled in the range 1800 to 2400. So basically we would to replace these quantitative variable with qualitative variables that are labels of the ranges where they appear.  

**Quantitative variables in ranges - - cut()** continued  

* You can then look at the class of that ranges variable that we have created. It's now a factor variable instead of a quantitative variable. And we can actually make a table of those ranges. And so what this will do is: now instead of, if you made a table of the actual quantitative variables, since they are quantitative each value would essentially get whatever has been observed once. So, you'd see a very long table with each element observed once.  

* Instead, what we see is now, the different ranges, zero to 600, 600 to 1200, and so forth. And you get to see the number of times that there was a value that was between zero and 600: there were 30 of those times. 32 times the value was between 600 and 1200 and so forth. So it gives you a little bit of information if there are specific cutoffs that you want to look and see where the quantitative variables break down you can actually do that with this cut ()function.  

**Quantitative variables in ranges - cut2() {Hmisc}**  

* Alternatively, you can use cut2(), if you want to break the values up by quantile. So, remember when we used the cut() function, we actually had to tell it where the breaks were going to occur, so at zero, 600 and so forth.  

* But if you use the cut2() function, you can actually pass it this parameter, g, and say equal to six. And so what it's going to do then is break the dataset up into six ranges where in each range there's an approximately equal number of points. So what it basically does is break it up by the quantiles of this quantitative variable.  So if we make a table of this new cut up variable we see that there's about twenty instances in each of the different categories of this new cut up variable. 

**Adding an extra variable**  

* So you might want to add this extra variable to your data set, so when you're doing data munging, sometimes you'll create a new data variable, either by taking a transformation, or in this case by breaking a quantitative variable up into a qualitative variable that is defined by ranges, and sometimes you want to add that to your data frame.  

* So what you can do is, you define this *timeRanges* variable by applying the cut2() function. And then what you can do is, to add it to the *reviews* dataframe you just access the particular name that you are trying to load it into, in this case, *timeRanges*. And say, and, assign the value of the *timeRanges* variable that we've created with cut2(). So then, if we look at the *reviews* data frame, we see that it now has one additional column, which is the *timeRanges*. So basically, what you can do is, by using this $ accessory, you can actually add variables to your data frame.  

**Merging data - merge()**  

* Another thing that you might want to do is, you might want to merge data sets together. So, in the case of these data sets that we're looking at here, we have a bunch of solutions that were submitted by people. And we have a bunch of reviews that are the reviews of those solutions and we might want to merge those two data sets together, so that we can look at each particular solution, and see who reviewed it, and see what their response was, and so forth.  

* So, the way that you merge data sets together in R is with the merge() function. There are actually other functions that can be used to do this, and I'll explain a few of those, or refer to a few of those, later. But this is the most common approach. And so what you can do is; you can actually merge data sets together that have common variable names. And so what it's going to try to do is merge variables together based on their common *id* and then reorder the variables so that their ordered by the common variable names.  

* So the important parameters here are *x* and *y* which are two data frames that you want merged together; *by* which are the variable names that you want to use to merger the data sets. You could also break it up so that, *by.x* and *by.y*. In other words, you can match the variable names even if they're different across data sets, and have them merge together.  

* And, then the *all* value, which says that you would actually like even if there are certain values that don't appear in one data frame, but they appear in the other; you would like an NA to appear in the merge data set.  

**Merging data - merge()** {example}  

* So just to show you an example of how this works so we have the *reviews* data frame and the *solutions* data frame and we're going to merge them together with the merge() command and we're not going to tell at this point which *id* is to merge on.  

* So it’s going to do is its going to go through and look at the *reviews* data frame and the *solutions* data frame and look at the names of those data frame. And if they have any of those names in common, it's going to try to merge those variables together and we say that all equals TRUE because there may be values in *solutions*  for a particular emerging variable that don't exists for in the *reviews* variable.  

* So if we look at the merged data, what happens is we can see that now, the *solution_id* and *reviewer_id* variables both exist in the same data set now because it's been merged. We see that now there's only one *id* variable, this was a variable that was common to the two data sets. So this is the variable that they got merged on. And what you'll notice is that sometimes there are two values that cannot be reconciled. That it's, they both have *id* 1, but they have different values of *start* and *stop*. So, they end up creating two different records in this merge data set. And when the *solution_id* and the *reviewer_id* don't exist for one of those records, they just get replaced with missing values.  

* So it turns out when we wanted to merge the *reviews* and *solutions* data sets together, we didn't want to merge them on the basis of the *id*. Because if you go back and look, *reviews* actually had a *solution_id* variable. And so what we'd like to merge is the *id* variable for the solutions, to the *solution_id* variable for *reviews*. 

**Merging data - merge()** {example continued}  

* So what we do here then is use the by.x and by.y commands. So basically what we do is, we say okay we're going to pass the data frame *reviews* and the data frame *solutions* to the merge() command. And what we would like to do is merge them on the basis of, in the *reviews* data set, the *solution_id* and in the *solutions* data set, the *id*variable.  

* So basically, what it's going to do, is it's going to say: “Okay, I'm going to look for every time I see a particular *solution_id* in the *reviews* data set and see if I can find that same value of *id* in the *solutions* data set and then merge those two values together.” Again I'm going to do all =TRUE. So if it can't find say a particular value of *solution_id* in the *id* variable, it will just create a new row with NAs for the data set where you can't find those values.  

* So here now I see that when I do this merging, I see that I get a *solution_id* and an *id* variable and a *reviewer_id*. So what it's done now is this *solution_id* has replaced the *id* variable for solutions and it has merged these two data sets together.  

* And so what you can see, for example, is the *solution_id* is 3 and the *id* is 1 for this observation in the new data frame. And so if we go back to the just the *reviews* data set, we can see that for the *reviews* data set its id=1 and its *solution_id*=3. And so this is that same record in the merge data set. And in the merge data set we know that from the *solutions* data frame, this will correspond to the value of *id* equal to three. So it's actually merging two different variables together into one variable and then combining the two data sets on the basis of that variable. So merging is a little bit hard to explain you with these very simple examples, the best way to understand it is to practice and so there will be this week several practice examples where you can try out merge() and see how It works.  

**Sorting values - sort()**  

* So another thing that you would like to maybe be able to do is to sort values. So, in particularly you might want to see if, once you put the values in order, how many of a particular value there are and so forth.  

* And so the way that you do that, is with the sort() command. It has two parameters *x*, which is the vector of values that you're going to sort and*decreasing* which is a parameter that tells you whether to sort in a decreasing or increasing order. By default they sort them in increasing order.  

* So if we look at the *mergedData2$reviewer_id* in the first ten elements you can see that they're not in numerical order. So there is 26, then 29 which is larger, and then back to 27, which is a smaller value. 
* If instead we sort the *reviewer_id* from this merged data set and look at the first ten values it's all 22, and that's because 22 is the smallest value of the reviewer variable and it's in order from smallest to largest. So, if you kept going and you look at more elements of this it would go all the 22 values then all the 23 values and so forth.  Importantly you notice that sometimes you have 22s as the same value for, for this variable. And so what it does is, all the values that are 22 get sorted into a fixed order.  

**Ordering values - order()**  

* When you want to sort a data frame by a particular variable, then you can use the order() function. So, the order() function takes a list of variables to order; the *na.last* parameter which tells you where to put the NAs and whether to sort values into decreasing or increasing order just like with the sort function.  

* So, let's see a little bit about how this works. So, suppose we have this same *reviewer_id* vector and we look at the first ten elements. And we see that they're 26, 29, 27 and so forth. If we apply order() to this same set of elements, the *reviewer_id* vector, and we look at the first ten elements there are 4, 6, 14, 22, 23, and so forth. So what is this order function doing?  

* It's saying, if I want to put these values in order, the first element has to be fourth element of this vector. So I have to take 22, because it's the fourth element of this vector and make it the first value. Then it says the second value should be the sixth value of the vector. So it should go six, so you see it's also a 22. So it needs to get moved into the second value, and so forth. So basically what it does is it says how you need to rearrange these values in order to get them into order.  

* And this is really useful, because first of all you can re-order that single variable with this order() command by saying okay, I've got this vector of values, the *reviewer_id* values and I want to sort them into order. So what I can do is take the indices that I get from the order command and just subset this *reviewer_id* variable vector by that order. And so what it does is, it will say, okay I'm going to take the fourth value of this vector and put it first. Then I'm going to take the sixth value of this vector and put it second, and so forth. And so what it'll do is order the values for that particular vector.  

**Reordering a data frame**  

* But what's even more useful is you might be able to reorder a data frame by a particular variable if you want to. So, remember we're looking at this merged data set and if we look at just the first six columns in the first three rows. So this is selecting the first six columns here and this is selecting the first three rows. What we can see that is this data set is not ordered by say the *reviewer_id*, and maybe we'd like to reorder the data frame so the reviewer data is in order.  

* So what we can do is actually take the merged data, and then calculate that order() function, which is remember, the indices that are used to reorder the values so that they're in increasing order. And we can apply it now, we can we can pass those indices to the selection of the data frame and select the rows corresponding to ordering those *reviewer_id* vectors.  

* And so what we get is if we look at the sorted data that we generate by assigning that to assorted data, we actually see now that the *reviewer_id* is in order. So it's reordered all the other variables so that the *reviewer_id* is in increasing order. And so it goes 22's followed by 23's and so forth. And it's reordered all the other variables so that the value corresponding to this *reviewer_id*  is still the right value; they've just been reordered.  

**Reordering by multiple variables**  

* The other thing that you can do is actually reorder by multiple variables, so in this case each reviewer might have reviewed multiple solutions and they might have a different *id* for each of those reviews. And so suppose we want to sort both by the *reviewer_id* and put those in the increasing order, but we also want to sort by the *id* of the individual so that the *id* is also in increasing order. So within *reviewer_id* we'd like the *id*  to be increasing since each reviewer gets a certain number of reviews and they have a unique *id*.  

* So what we can do is then use this order() function but instead of passing order just the *reviewer_id*, we first pass it the *reviewer_id* and then we pass it the *id* variable. So what this does is it says, okay. Give me back a set of indices that puts the *reviewer_id*s in order. Because that's the first variable I passed to order. And then it says: “okay, within each *reviewer_id*, let's go through and put the *id* variable order in order within that *reviewer_id*.” And the reason why it does it within *reviewer_id* is because it's the second element that we pass is the *id* variable whereas the first element we pass is the *reviewer_id*.  

* So then if we reorder our dataset or the rows of our dataset according to those indices, we actually can look at this dataset and we end up with a dataset that now has the *reviewer_id* in order. And then within these *reviewer_id*, we have the *id* variable in order. This can be useful both for just looking at your data set and for apply, applying particular kinds of functions that require data to be in ordered values.  

**Reshaping data – example**  

* So, the only thing that we need to do and was one of the components of tidy data is that we need to maybe sometimes change the format of data sets. So, basically this is changing the shape so that we end up with observations in rows and variables in columns.  

* So this is a misshaped data set, this is actually an example from this paper. So what it has is treatment A, treatment B and people are the different elements in this data frame. And so then there are values that correspond to different treatments and different people.  

**Reshaping data - melt()**  

* So what we want to do is reconstruct this data set into such a way that we have in each row, we have a particular observation, and in each column, we have a variable. And so, one way that you can do that is with the melt function(), which is in the reshape2 package.  

* And so, what you need to do is load the reshape2 package in and then the, use these parameters to able to reorganize your data set. So, you need to have ID variables { *id.vars* }, measurement variables {*measure.vars* ], and *variable.name*s. And so, the ID variables tell you which of the variables is just an ID for a particular observation and then which of the variables are actually measurements that we've taken for those ID's and so we take this misshaped data set and we say: okay the ID variables { *id.vars* } for this data set are the *people* and the *variable.name* that we're going to collect is *treatment*. And then, the *value.name* is going to be *value*. 

* And so, what it does is, it melts this data set. It basically takes the data set that was organized in such a way, so that there wasn't necessarily one observation per row. You can actually see that. Here the *treatmentA* and *treatmentB* observations for John are both on the same row here. In the new data set, we have John for *treatmentA* and we get his value and then we also have John for *treatmentB* in a separate row and their value. So what we've done is basically put individual observations in individual rows and variables in columns so that the data are tidy. The reason why is that it's easier then to do downstream modeling with data that are in this format.  

* There is obviously a lot more ways in which data could be formatted and a lot of ways in which it could be reformatted.  

**More resources**  

* So, you can look at the [tidy data and the tidy tools]( http://vita.had.co.nz/papers/tidy-data-pres.pdf) presentation that I have linked to here gives you a lot of information about that.  

* You could also look into Andrew Jaffe's [Data Cleaning Lecture](https://dl.dropbox.com/u/7710864/courseraPublic/otherResources/lecture3/index.html#1) which I've linked to here on Drop Box. 

* Hadley [Wickham talks about regular expressions](https://dl.dropbox.com/u/7710864/courseraPublic/otherResources/14-reg-exp.pdf) which can be used for cleaning up variable names. 

* And then in addition to the [reshape2](http://cran.r-project.org/web/packages/reshape2/index.html) package and others that I've mentioned, a key component of doing data munging is **just long painful experience**.  

* I wish I could say that there was an easy way to take any data set, process it and have it be clean automatically. But that's definitely not true. Each data set is messy in its own way and so a lot of this is just practice and looking at different data sets and looking at the ways in which they're messed up and trying to figure out how to fix them.  

* So, I'll try to give you some examples on how to do that, but really the key to becoming a good data processor is practice, practice, practice.
