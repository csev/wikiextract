[[ML:Linear Regression with Multiple Variables]]

# Errors in the video lectures

- Error in 2-1, Multiple Features, at 7:25 to 7:30

    It is recorded that $$\theta^T$$ is an $$(n+1) \times 1$$ matrix; it should be $$1 \times (n+1)$$ matrix since it has 1 row with $$n+1$$ columns.
- Error in 2-3, Gradient Descent in Practice I - Feature Scaling, at 6:20 to 6:24

    It is recorded that the average price of the house is 1,000 but he writes 100 on the slide.  The slide should show "Average size = 1000" instead of "Average size = 100".

- Error in 2-3, Gradient Descent in Pratice I - Feature Scaling, at 7:45
    The formula for $$S_1$$ is (max - min) but if our number of bedrooms is 1-5 then the number should be 4 not 5.  He mentions this shortly after and says these are only approximations but you must use this formula and not substitute the max value to get the quiz answer correct.

- Error in 2-4, Gradient Descent in Practice II - Learning Rate, at 5:00

    The plot on the right-hand side is not of $$J(\theta)$$ against the number of iterations, but rather against the parameters; so the x axis should say "$$\theta$$", not "No. of iterations".


- Error in 3_1, Normal Equation, at 2:19 (Not an error, someone please verify)

    The function $$J$$ is a function of $$\theta$$ from $$0$$ to $$n$$, not $$m$$ i.e. $$J(\theta_0, \theta_1, ... , \theta_n)$$

    The range shown is $$J(\theta_0, \theta_1, ... , \theta_m)$$.  $$m$$ is number of test cases, as stated above it should read $$J(\theta_0, \theta_1, ... , \theta_n)$$

- Error in 3_1, Normal Equation, from 8:00 to 8:44

    The design matrix X (in the bottom right side of the slide) given in the example should have elements x with subscript 1 and superscripts varying from $$1$$ to $$m$$ because for all $$m$$ training set there are only 2 features $$x_0$$ and $$x_1$$.
- Error in 4_6, Normal Equation, at 12:56

    $$(X^TX)^{-1}$$ is described as an $$n \times n$$ matrix, but it should be $$n+1 \times n+1$$
- Error in "Normal Equation Noninvertibility" at 3:20

    Prof Ng states that X is non-invertible if m <= n. The correct statement should be that X is non-invertible if m < n, and may be non-invertible if m = n.

- Exercise sheet, page 9; code segment

    Prior to defining J\_vals, we must initialize theta0\_vals and theta1\_vals. Add "theta0\_vals = -10:0.01:10; theta1\_vals = -1:0.01:4;" to the beginning of the code segment.

- Exercise sheet, page 9; code segment

    computeCost(x, y, t) - should be capital X (as it is correctly in ex1.m)
- Exercise sheet, page 14; normal equations in closed-form solution to linear regression:

    Inconsistency in notation: Whereas the  column vector $$\vec{y}$$ of dimension $$m \times 1$$ is denoted by an overline vector, the $$(n+1) \times 1$$ vector $$\theta$$ lacks the overline vector.

    Same applies to definition of vector $$\theta$$ in lecture notes - I would suggest to always denote column and row vectors by overline vectors, increases readability enormously to distinguish vectors from matrices. Matlab itself of course does not distinguish between vectors and matrices, vectors are just special matrices with only one row resp. column.
 
- Exercise sheet, page 14:

    In section 3.3, there is the following sentence: "The code in ex1.m will add the column of 1â€™s to X for you." But in this section, it is talking about ex1_multi.m. Of course, it is also true that ex1.m adds the bias units for you, but that is not the script being referred to here.

- Octave Tutorial:

    When Prof Ng enters this line: "w=-6+sqrt(10) * randn(1,10000))", add a semicolon at the end to prevent its display.

    If Octave crashes when you type "hist(w)", try the command "graphics\_toolkit('gnu_plot')".

- Vectorization video at 2:19

    The comma at the end of the line "for j=1:n+1," is a typo. The comma is not needed.

# Errors in the programming exercise scripts
- In plotData.m, the statement "figure" at line 17 should be above the "===== YOUR CODE HERE ===" section. The code you add must be below the "figure" statement.
[[Category:ML:Errata]]
