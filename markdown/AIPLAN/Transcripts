[[Category:AI]]
[[Category:Artificial Intelligence Planning]]
[[Category:Coursera]]

* [[AIPLAN:Main | AI Planning: Main WIKI page]]

Artificial Intelligence Planning - Transcripts of the Video Lectures - English (En)

#Week 1

**Course Welcome**

Welcome to the course on AI planning.

Planning is one of the most important aspects of intelligent behaviour. The ability to identify and select appropriate activity, and to project forward the consequences of executing that activity, is fundamental to humans and intelligent robots alike.

The planning course lasts five weeks. In the first week of this course, we will formally define the problem solved by planning algorithms and talk about the context in which planning is used. After that in week two, we will describe some basic AI techniques used for problem solving in general, and planning problems in particular. This will leave you with a good understanding of the STRIPS planning system.

In week three, we will look at different, a different approach to the same problem, namely, plan space search. In addition, we will cover hierarchical planning, which addresses a similar problem and has been used in many real world applications. Week four gets us closer to the state-of-the-art in planning research, introducing some of the techniques used in the fastest planners available today. Finally, in week five, we will talk more about application areas in which they AI planning has been used, and some more advanced topics.

The course is going to make use of lectures on video, supplementary material on a wiki, and we'll give you some links to some AI planners to try, along with relevant online resources, and some recommended quite short readings. There'll be a number of quizzes and online assessments to monitor your progress.

We're going to encourage you to use the course social platform, which includes the discussion forums in the wiki. There's going to be a course Twitter tag and there's also a virtual world meeting space for you to try. Full details of all aspects of the social platform are on the course website.

We hope you will enjoy the course and will learn something useful.

##What is planning?

This is the first segment of the artificial intelligence planning course. In this segment I will give you an introduction and overview to the problem we are addressing in the field of A.I. planning. This will include some examples and introduction to the basic techniques we will be using to solve planning problems.

So the first question I have to answer is, what is planning? And more specifically, what do we mean by planning in the context of artificial intelligence? I will answer this question by informally describing the planning problem that is the problem we are trying to solve in this field. I will then argue why this problem is important for artificial intelligence as a whole, and then continue to describe some techniques that will be used to solve this problem.

So let us start by looking at human planning and acting. Humans rarely plan before acting in everyday situations. Ask yourself, when was the last time I sat down and made a plan before acting? Chances are this will have been some time ago. This is because humans act without prior explicit planning quite often.

There's a number of situations where this is the case and here are some examples. When the purpose of my action is immediate, I don't need to make an explicit plan. For example, to record this lecture, I needed to switch on this computer. I know how to do this, so I just did it. I didn't need to make an explicit plan. The purpose of the action was immediate.

When performing well-trained behaviours, I also don't need to do explicit planning. For me, this would be driving a car. I know how to drive a car. I've done this many times, so I don't need to make a plan before I switch gears or before I turn the steering wheel. It's a well-trained behaviour, I don't need to plan.

When the course of action can be freely adapted, I also don't need to plan. This would be when I go shopping in the supermarket. I don't need to plan in which order I go through the different aisles, because I can always adapt my acting to what I've missed in previous aisles and just go there again. So the, the course of action can be freely adapted, means I don't need to plan.

A number of situations make it possible to plan, though, and here are some examples where planning is necessary, that is explicit planning. So, when I'm addressing a new situation, something that I haven't done before or haven't done often, then I need to do explicit planning.

An example of this would be moving a house. Everybody who has done a big move with furniture will know what this means. You need to organize a van. You need to organize people. You need to have an explicit plan in place before you can successfully move from one place to another.

Another situation is when the task you're trying to achieve is very complex. So, for example, when I was planning this course, I was doing explicit planning. This is quite a complex task, it involve ten hours of lecturing and many other things, so explicit planning was necessary. 

Another type of situation where acting happens only after planning is when the environment imposes a high risk or a high cost. So if I'm the manager of a nuclear power station, I will do a lot of planning before I act, because it's very important what I do and the potential damage I can do with wrong action is high. So, I will do explicit planning to counteract that.

Also, when I'm collaborating with others, explicit planning can be extremely helpful. So, think of people who are trying to build a house. That's the people who are trying to put up the walls, trying to put in the plumbing, and of, the electricians. They all need to coordinate their activity and that means they all need to have an explicit plan for when they do what and in which order.

So the main lesson here is that people only plan when it's strictly necessary. We don't do planning when we don't have to. We only plan when we feel there's a benefit to it. And this is because planning is a complicated and time-consuming process.

There is a basic trade-off here. If we plan, we normally come up with a course of action that leads to better results, but there is a cost. So, if there is no benefit to be had from planning, we're often better off not planning. That is, often we seek only solutions or plans that are good enough for what we are trying to achieve, not optimal plans. So people only plan when it's strictly necessary.

Here is the definition for what we mean by artificial intelligence planning. Let me read this out for you first.

Planning is an explicit deliberation process that chooses and organizes actions by anticipating their outcomes and that aims at achieving some pre-stated objectives.

So I will try to take this apart for you now.

What this says is, planning is an explicit deliberation process. What this means is, to plan, we need to think. It's a mental process where we think about the actions that we are trying to do. 

It also needs to be explicit thinking, which means, it's conscious. It's not a subconscious process that's going on, we are aware that we are doing this planning so we are thinking about planning. 

In this thought process, we choose and organize actions. So, choosing means, we have some options available, things that we may be able to do. And we choose some of these actions, and we discard others, as part of the planning process.

We also organize these actions into a structure. That is, we could choose which actions to do before which other action, which actions to do in parallel, what the outcomes of each action will be, etcetera. So we organize them into some structure.

And, the way we do this is by anticipating the outcomes of the different actions that we have available as options. So we think about, what will the world be like if we do this action? And the result is either what we want or don't want and that's what the next point is.

The process aims at achieving some pre-stated objectives. So there are things that we want to have true in the world, these are our objectives, and by anticipating the outcomes, we can compare the world states as they will be when we execute an action to the ones in which the objectives we try to achieve are satisfied.

So that is what we mean by planning. Planning is an explicit deliberation process that chooses and organizes actions by anticipating their outcomes and that aims at achieving some pre-stated objectives.

Artificial intelligence planning now is the computational study of this deliberation process. So what we're interested in is the thinking about plans, the reasoning about actions that takes place when we are planning and we are trying to build a computational model of this process.

Now that I've defined what we mean by planning, I want to explain to you why it is so important to study planning in artificial intelligence.

The goal of artificial intelligence is really twofold, there's a scientific goal and an engineering goal. The scientific goal of A.I. is to understand intelligence, and the key observation here is, that planning is an important aspect of intelligent behaviour. So, if we observe some intelligent behaviour, we assume that there is an underlying plan and we assume that this plan is the result of some planning. So, to understand intelligence, we need to understand planning, which is part of intelligence. In that sense, understanding planning directly contributes to the scientific goal of A.I..

The other goal of A.I. is the engineering goal, which is to build intelligent entities, that is we want to build robots or other entities that exhibit intelligent behaviour. And if this is to be intelligent to us, this needs to involve actions that are carefully chosen and organized as we do in planning. So what we do in planning is we build models of how this planning works and these models are software models, so we can build them into our intelligent entities as components. So planning directly also contributes to the engineering goal of A.I. And just as a side remark, the robot you see here is the Shakey robot that was built in the late 60s and that was one of the first robots that used an actual planner to come up with its actions. 

There are really two different types of planning, domain-specific and domain-independent planning. In domain-specific planning, we use specific representations and techniques that are adapted to each problem we are trying to solve. There are a number of important examples for this type of planning, domain-specific planning, for example, path and motion planning. If we are trying to navigate a robot through a two-dimensional or three-dimensional space, we need to come up with a path through that space, that gets the robot from one location to another. And to do so, a number of algorithms have been developed to, to make sure that the robot doesn't bump into other objects or will fit through narrow passages. All these algorithms are highly specific and very efficient.

Another example is perception planning. If we try to understand a given situation a robot may have to wander around in a scene and observe different aspects of different angles to understand what is going on. And again, there are highly specific algorithms that have been developed for this type of problem.

Manipulation planning is another such problem where we are trying to, for example, assemble an object from different parts and that needs to happen in a specific order for it to work.

Also, natural language generation uses highly specific algorithms for planning, namely the planning of utterances that lead to communicating, as given subject.

The point is in all these domains, we have specific algorithms that we use to efficiently solve a specific problem.

On the other hand, there's domain-independent planning. And there, we use generic representations and techniques to solve the generic planning problem. The advantage of this is that it saves effort, so we don't need to reinvent the same techniques for different problems all the time. We can always reuse the same planning algorithms.

The disadvantage is that, this means planning from first principles and is often relatively slow, but it also leads to a general understanding of planning and as I've just explained, that's the scientific goal of artificial intelligence.

The important lesson here is that domain-independent planning complements domain-specific planning. Domain-specific planning is good for specific problems where highly efficient solutions are required. Domain-independent planning is good if we need to plan from first principles for the type of situation I've explained earlier, situations we have never seen before for example. So the two types of planning complement each other.

But in this course, we will focus on techniques for domain-independent planning. So here's a little quiz to test your understanding so far. The following five statements are either true or false. Please tick the box for the statements that are true.

[inline quiz]

The first statement, people only plan when they have to because the benefit of an optimal plan does not justify the effort of planning is true.

The second statement for humans planning as a subconscious process, which is why computational planning so hard is false. The reason is that planning is not a subconscious process. We have defined planning as the explicit deliberation process, so it needs to be conscious.

Third statement, planning involves a mental simulation of actions to foresee future world states and compare them to goals, that statement is true.

The fourth statement, in artificial intelligence, planning is concerned with the search for computationally optimal plans, that statement is false. We're not only after optimal plans, we also want to sometimes find out whether a plan exists at all, whether it's optimal or not.

Finally, domain-specific planning is used when efficiency is vital, whereas domain-independent planning is good for planning from first principles. That statement is true again.

##Conceptual Model for Planning

You should now understand what we mean by planning in AI.

Next, we will formalise this understanding of planning by means of a conceptual model for planning.

This conceptual model will be a state-transition system.

Before I introduce the conceptual model that underlies planning, I want to talk to you about conceptual models in general and why they are a good idea.

So, what is a conceptual model?

A conceptual model is a theoretical device for describing the elements of a problem.

What this means is it helps us to formalize the problem we are trying to solve.

This is good for a number of things.

For example, we can explain the basic concepts with this model so it helps us to define what the objects are that we are manipulating during problem solving.

It also helps us to clarify some assumptions.

What constraints are imposed by this model is clarified by writing down such a model.

We can also use it to analyze requirements, so we can look at the representations we need to develop to represent the objects that we're manipulating during problem solving.

Also, we can prove semantic properties with a theoretical device like this.

The most important properties for algortithms we're interested in our soundness and completeness and they require a semantic foundation which is given by a conceptual model.

What a conceptional model isn't good for, is developing efficient algorithms and other computational concerns.

So, we cannot immediately derive planners from the conceptual model.

But the conceptual model we are using and planning is called a state-transition system.

Formally, a state-transition system is defined as a 4-tuple consisting of four components, S, A, E, and gamma.

I will now explain these components in turn.

The first component, S, is a finite or recursively innumerable set of states.

So, these are all the possible states the world can be in.

The set can be finite or recursively innumerable, which means infinite.

But in most of the examples we'll be looking at, we have only finite sets of states so don't worry about the second part for now.

The second component is a set of actions.

Actions are the things an agent can do to change the state of the world.

The third component is a set of events.

Events can happen in the world and are not under the control of an agent, but events, too, can change the state of the world.

The fourth and most complex component of a state-transition system is the state-transition function, gamma.

Gamma takes two things.

It takes a state, a state of the world as input, and it takes an action or event.

So, this second component is the union of all the actions and events and one of those is the second argument to the state-transition function.

The result of applying the state-transition functions then, is another set of states.

So, this notation here, 2 to the s, just denotes the power set of all possible states.

Which means an element of the set is, itself, a set.

A set of world states.

So, the state-transition function takes a state, an action or event and gives us all the possible states that may be the result of applying this action, or this event happening.

We can now use this definition to define some other concepts formally.

For example, applicability, we can say that an action A is applicable in a state S, if gamma of S and A is not empty so if there is at least one state that is the result of applying this action in the given state.

And when we apply an action A in a state S, this will take our state-transition system to a new state S prime.

And S prime must be an element of gamma of S and A.

Another way to look at a state-transition system is to view it as a graph.

Suppose we are given a state-transition system S, A, E, and gamma, then we can define a directed labeled graph G, that consists of nodes NG and edges EG.

The nodes of this graph are simply the world states that are possible in this state-transition system.

NG is equal to S.

And the edges in this graph correspond directly to state transitions defined by the state-transition function.

So, we have an arc from a node, s, to another node, s prime.

So, this is and edge in this graph and that is labeled with label u, which is either an action or an event, if and only if.

The state s prime is the result of applying u in s.

u can be action or event so we have a transition from here to here with label u.

So, a state-transition graph consists of nodes that correspond to world states and edges that correspond to state transitions.

Let me illustrate a state-transition system with a very old problem that has been used many times in AI, the missionaries and cannibals problem.

In this problem we have a river.

And on one side of the river we have three missionaries and three cannibals initially.

The missionaries are black triangles and the cannibals are red circles here.

There is also a boat available and in this boat, can be up to two people.

And they can use this boat to cross the river.

Now, the problem is, if the cannibals ever outnumber the missionaries on either of the banks of the river, then the missionaries will get eaten by the cannibals.

And we don't want that.

So, you can see in the initial state, there's an equal number of missionaries and cannibals on one side and no missionaries or cannibals on the other side, so there's no problem.

The planning problem now is to come up with a sequence of actions that carries all the missionaries and cannibals safely across the river, to the other side.

This system can be described by a state-transition system.

And if you're not familiar with this type of system, I would advise you to now try to define this as a state-transition system.

Specifically, you are trying to see what are the world states that are possible here, what are the actions and what are the events that can happen in this problem.

The state-transition function is best defined as a graph.

And, if you've sit down for about half an hour, I'm pretty sure you can come up, come up with a graph that describes the whole state-transition graph for this problem.

So, if you want to do this little exercise, you need to pause the video now.

So, here is my version of the state-transition graph for the missionaries and cannibals problem.

To define this as a state-transition system, we have to define the four components.

The first component is the set of states S.

And that can be defined as the different world states we see here.

And these are all the squares, rectangle that are drawn here.

there are sixteen different world states and they are denoted by these rectangles.

So, this is the initial state, as we've seen in the previous slide, where all the missionaries and cannibals are on the left-hand side of the river.

And over here on the right, we have the gold state.

And in this gold state, all the missionaries and cannibals are on the right-hand side of the river.

And the second component of a state-transition systems are the actions that are possible.

In this case, there are five different actions.

And I've denoted them here with the labels that occur on the different state transitions.

So, there's two types of actions, namely, actions with one person in the boat, and actions with two people in the boat.

The actions with one person in the boat are the ones where we have one missionary or one cannibal in the boat, or we can have two people in the boat.

This can be two missionaries, two cannibals or one missionary and one cannibal.

It's denoted here by 1m1c.

So, these are the five possible actions that we have to to do something in this system.

I don't need to denote where the boat is.

Because the boat can only cross from one side of the river to the other.

The set of events is empty for the state-transition system.

And finally, the state-transition function is defined by all the arcs that make up the, the lines between the different state here.

Note in this specific problem, all the arcs are bidirectional.

Which means, with the same action, we can go to one state and then back to the original state.

So, this is one arc here, and this is one, this is one.

And all these arcs together make define the state-transition function.

And that concludes the definition of the state-transition system.

A state-transition system is useful because it describes all the possible ways in which our system may evolve as a result of applying actions or events happening.

But what we want to do is solve planning problems and the solution to a planning problem is a plan.

And by a plan, we mean a structure that gives appropriate actions that we can apply in the initial state of our problem such that it gets us to a different state in which our objective that we're trying to achieve, as part of the planning problem, will be achieved.

A simple example of such a structure would be to have a sequential list of actions that we need to perform in order.

A more complex structure could be a function that maps states to actions so that when we are in a given state, we can use that function to decide what action to apply.

A plan implicitly describes a path through, through our state-transition graph.

So, when we execute a plan, we expect to end up in a state in which our objective is satisfied.

There are different types of objectives that can be defined for planning, and I will give you some examples now.

The simplest way to define an objective is simply to have a gold state.

This can be an individual gold state that is named, we've seen this in the missionaries and cannibals problem, or it can be a set of gold states that means one of those states is one that we want to reach.

An objective can also include some constrains on itermediate state through which we're passing on the way to the goal, for example, we can have states that we don't want to go through that we need to avoid as part of the objective.

A more complex objective could also come with a utility function for each state and tells us that we have to maximize the utility on our way to the goal.

As you can see, an objective can be quite complex.

A completely different view of an objective would be to not try achieve something but to perform a given task.

So, a good example of this is when you are going on a holiday.

You're not really trying to change the state of the world.

You want to end up back in the same state where you started.

But you want to do something in the time where you go on holiday.

And that's a task that needs to be performed.

Probably, the most common reason for solving planning problems is that we want to execute the resulting plans.

And here is the model for how planned execution might actually work.

So, we have a planner that is given a description of the state-transition system that tells the planner how the world may evolve.

We're also giving this planner the initial state.

That is, the state in which the world is in and some objectives that tell the planner where we want to be.

The planner then solves this planning problem and generates a plan which is passed to the controller for execution.

The controller takes this plan and executes the actions in this plan.

So, it has to extract the next action to be executed and passes this to the system.

The execution of the action then changes the state of the actual system that we're trying to manipulate.

For example, the real world.

And hopefully, our system is consistent with the description of the system that was given to the planner to generate the plan that we're now executing.

But the system is not only changed by the actions we are taking that are controlled by the controller, it is also changing because of events that are happening.

For the controller to take appropriate actions, it usually needs to know what state the system is actually in and to do so it has observations which are going from the system to the controller.

We model observations through the observation function eta which maps a state to set up observations that can be made in the state.

Quite often, the world is not fully observable, and in this case, the set of observation does not allow us to immediately infer which state we are in.

So, a given set of observation makes it possible that we are in a number of states, and this is what is called the belief state of the controller.

Now, the model we've just seen is not very realistic, because the real world on which we are executing our plans is often different from the description of the state-transition system that we are giving to our planner.

So, those two are not identical.

The reason is that this description we're giving to the planner is an abstraction.

It leaves out many details about the real world which make planning possible.

And then, when we execute the plan, things may go wrong because the two models are not the same.

A more realistic model is called dynamic planning, in which planning and execution are actually interleaved.

What is different in this model is that the controller has to do something called plan supervision and that means, it has to detect when observations differ from expected results.

So, it expects the world to be in a certain state as a result of an action, but it can observe that it isn't.

What it can do, in this case, is plan revision.

That is, we take the existing plan and try to change it in some way to take into account the new state.

This can be done by the controller for very simple cases or it has to be done by the planner for more complex cases.

In this case, the controller has to pass an execution status back to the planner, so that the planner can generate a new plan that is passed to the controller.

And that takes into account the change that has happened.

In the worst case, the planner will have to re-plan that is, it will have to create a completely new plan from scratch for the given problem.

Dynamic planning then, closes the loop between the planner and execution by passing back the execution status to the planner for replanning or plan repair.

## Planning and Search

###a Planning and Search

We have now seen a conceptual model for planning, namely, the state transition system.

This helped us formulize the problem we are trying to solve in planning.

A technique that is used almost everywhere in planning, and in many other areas of AI is called search.

And this is what we will be looking at next.

Before we go in to the details of search algorithms, I briefly want to talk about the types of problems we will see on this course.

Namely, toy problems versus real-world problems.

Toy problems are characterized by a description that is concise and exact.

The description being concise allows me to describe the problem in one slide quickly.

The description being exact means there shouldn't be too much ambiguity about what the problem is trying to do.

Toy problems are often used for illustration purposes, for example, on this course.

They are also used to compare the performance of different search algorithms.

Toy problems have two interesting properties.

Namely they are rich and simple.

By rich, we mean they're anything but trivial to solve.

And by simple, I mean, they can be described easily and precisely.

Real-world problems tend to be very different.

To begin with, there is no single agreed-upon description for most real-world problems.

That means if we were to use real-world problems in this course, I would spend a lot of time describing what the actual problems is and describing the details that need to be addressed in each problem.

However, toy problems are good as mental exercises.

But real-world problems, people care about the solutions of those.

The missionaries and cannibals problem we have seen earlier clearly falls into the category of toy problems.

Now, before we go into search algorithms, we need to characterize what constitutes a search problem, and here are the four components that characterize a search problem.

The first component is the initial state.

This is simply the state of the world from which we start our search.

In the missionaries and cannibals problem, this was the state where all the missionaries and all the cannibals and the boat were on the left-hand side of the river.

Next, we need a set of possible actions.

Not every action will be applicable in every state, so we will also need to define the applicability conditions for actions in states.

We can define the actions through a successor function.

A successor function maps a state into a set of pairs of actions and state.

So, for each state of the world, the successor function maps it to all those actions that are applicable, together with the states that result from the action being applied in the original state, and leading us to this new state.

Together, the successor function and the initial state span a state space which corresponds roughly to the states transition graph we have seen earlier.

The state space is a directed graph with states as nodes and actions as labels on arcs.

The third component of a search problem is the goal.

This can be either an individual state, in which case, we have just one unique goal state, or in general, we can have a function that tests whether a given state is a goal state or not, which allows us to have many different goal states.

A solution to a search problem is simply a path in the state space from the initial state to a gold state.

The final component of a search problem is the path cost function.

This simply assigns a cost value to each possible path in the state space.

we use this when we're doing optimal search.

When we're looking for an optimal path, we're looking for the path with the lowest cost.

A simplification that is often used in planning is that each action has a fixed cost and that the cost of a path is simply, the sum of the step cost, the cost of each action.

Now, you will have noticed that there are some similarities between what we've just defined and stay transition systems, and the next quiz will give you little time to think about those similarities and differences.

Going back to the missionaries and cannibals example, let's try to define this as a search problem.

So, we need to define the four components, which were the initial state which was given to us as part of the problem.

And the third component was the goal state which was also given to us as part of the problem.

Then, the fourth component is the path cost function.

And there, we simply assume that every step has the same cost.

So, the only thing that is slightly more complex is the successor function.

And we can define this as a table as shown here.

So, what this table does is simply enumerate all the mappings of states to set of action state pairs.

What we have on the left-hand side is a state.

So, this is the initial state as defined in the problem.

And I'll decipher this for you quickly.

This has two components.

It says, what's on the left-hand side and what's on the right-hand side.

On the left-hand side, we have the three missionaries, we have the three cannibals, and the boat.

And on the right-hand side, we have no missionaries and no cannibals initially.

So, that is the initial state.

We describe everything that's on each side of the river.

This is mapped to three pairs.

One, two, three.

each of which consists of an action and another state.

So, let's look at the first one.

In the first case, we ship two cannibals across the river.

And this gives us, on the left-hand side, three missionaries, one cannibal, because we ship two cannibals across.

On the right-hand side, zero missionaries, and two cannibals, plus the boat, which is also now on the right-hand side.

Altogether, we see there are three different pairs here because, there are three applicable actions in the initial state.

In the next row, we have a different state.

And we, again, define what can be done in this state, these are the two actions, and the resulting states that we get if we apply these actions.

This is actually, the same state that we've looked at just now.

This state here became the state there.

So, we need to do this for the whole set of states that are available for the whole states base.

Which means we need to go through a whole list of states and define where we can go from these states.

And if we had completed this table, we had defined the whole state-transition function and this concludes the definition of the surge problem.

In contrast to this toy problem, we will now look at a real-world problem, namely, touring in Romania.

What you see here is a rough map of the country with some of its major cities.

To define touring Romania as a search problem, we again have to define the four components of a search problem.

So, let's start with the initial state.

Suppose we are in the city of Arad, that's where we start our tour of Romania.

The next thing we need to define are the possible actions that we can take in this problem.

Since we are looking at a map, it suggests that we can drive from one city to another.

But presumably, when you're touring a country, that's not all you want to do.

You probably also want to do some sight seeing, or you need a hotel, so you need to check into that hotel, you need to book a hotel.

there are many different types of actions that you want to do when you tour a country and you can see, this is a real world problem.

We already have the first problem, deciding what the possible actions are.

The same applies for the goal.

When you're going on holiday somewhere, to tour a country or a region, what is your actual goal?

Well, presumably, you want to end up in the same state that you started off, namely, at home.

So, that can't really be the goal.

You want to have something else that you need to describe as your goal, something that happens along the way.

But it's very hard to describe because this is a real-world problem.

What is the little bit easier is probably the cost that is associated with each action and that will mostly be time and money.

As a side remark, the touring in Romania problem is taken from a famous AI textbook, by Russell and Norvig.

And if you want to learn more about search, I recommend that you have a look at this book.

The reference to this book will be given on the course website.

So, what you have just seen is that problem formulation is itself a complex problem.

And its the problem of defining the four components of a search problem.

In problem formulation, we have to decide what actions we want to consider and what states we want to consider in the world.

Probably the most difficult decision there is, at what level of abstraction are we looking at the world?

What detail do we want to take into account and what detail do we want to omit?

So, looking at the touring Romania examples, we could define actions that describe how we drive a car that, say, we have to turn a steering wheel by one degree left or right and we have to move our foot from one pedal to the other.

But this would probably give us too much irrelevant detail and there's a lot of uncertainty involved in that, too.

So, we probably don't want to go to that lower level of detail.

Also, if we try to solve our problem at that level of detail, we would come up with a solution plan that has many, many steps.

If we define the problem at a higher level of abstraction, say, we consider actions that drive us immediately to another big city.

Then, we have the problem that we need to decide how to execute such an action when we come to plan execution.

Also, if this is the level of abstraction we consider driving to another city, we can't really talk about things we do in between, between two cities.

So, deciding at what level of abstraction we model our actions and states is probably the most important decision in problem formulation.

But to help us with problem formulation, there are number of assumptions that search engines and planners often make and if we take those in to account during problem formulation, we may have a much easier task.

The first assumption is that we have a finite number of world states.

This implies that we cannot have continuous variables in those states, as this would automatically give us an infinite number of states.

Then, we will assume that the world is fully observable.

Which means everything that is relevant to us in the state of the world can be seen and will be known to the algorithm to our planner that we're using.

The next assumption is that the actions that were using are deterministic, which means each action has one well-defined outcome.

There's no uncertainty which state we'll be in after we apply an action.

The final assumption is that the world is static.

Which means, there are no events so nothing happens that we don't do.

Only the actions that we do modify the state of the world.

So, these are assumptions about the environment, but we will also make some other assumptions that are useful for planning.

The first one is that we have restricted goals.

And that means that our goals are either given to us as a single state that we want to be in or a set of states that are all gold states.

The second assumption is that the solution we are looking for is a sequential plan, so a solution is a linear list of actions.

There's no parallel activity in our plan.

We shall also not consider time explicitly but only implicit, which means activities will not have a duration for the time being.

And the final assumption is that we're doing offline planning, that is, the state transition system which underlies our planning process is not changing while we're doing the planning.

Okay, time for another quick quiz to give you time to think about this.

###b Planning and Search

So, now we have seen what search problems look like, we can turn towards search algorithms.

But before we describe the algorithm, I need to describe the data structure that we manipulate during the search.

And this is called a search node.

A search node is really a bookkeeping structure that encapsulates a state.

Note that I say search tree here because we will be using the search graph as a search tree.

This is simply to make the algorithms slightly easier to explain.

but there is a simple extension that you can come up with that turns this back into a graph search algorithm.

So, here is the data structure we're looking at.

Each search node consists of five things.

The first thing is the state, the state that is encapsulated in the search node.

So a state corresponds to a configuration of our world, and a node is a place in our search tree.

Note that two search nodes may contain the same state.

The next component of a search node, is the parent node, that we store in each node.

That is the immediate predecessor in the search tree, the parent in the tree.

The only node that doesn't have a parent is, of course, the root node in a tree.

Then we have to store an action in a search node.

That is the action that gets us from the parent node to this node.

Or, more precisely.

From the parents nodes state to this state.

Then we also want to store the path cost, in a search node.

That is the total cost of the path leading to this node in the tree.

And finally, we can store the depth in the tree as part of the node, which allows us to do for example, a simple cutoff, if we go too deep in the search tree.

So these are the five components, that make up a search node.

The state.

The parent node.

The action.

The path cost, and the depth.

So now, we get to the first algorithm that you will see in this course.

This is the general tree search algorithm.

The algorithm is implemented as a function tree search, that takes a search problem as input, and a search strategy.

I will get to the search strategy in a minute.

The search problem is simply what we've seen before, the thing that consists of four components.

The initial state, the successor function, the goal, and the path cost function.

The algorithm starts by creating a new search node.

So that's the structure we've seen in the previous slide, from the initial state of our search problem.

And this is stored in a set of nodes.

These are set braces.

and the set is called the fringe.

It is often also called the set of open nodes, as opposed to the set of closed nodes.

What this means is, these are all the nodes in our search tree that we have not yet explored.

And initially, we have not explored the initial state, and that's the only state we know about initially.

So what we do then is we go through a potentially infinite loop.

And the first thing we do in this loop, is, we test whether there are still nodes in our fringe that we can examine.

If there are no more nodes on the fringe, that means we've explored the whole graph, and we haven't come across a node that is a goal node.

So, in that case, we can return failure.

There is no solution to the search problem if we have explored all the nodes in the graph, and have not come across a goal node.

But initially, there will, of course, be at least one node, namely the node we created from the initial state.

So what we do in the next step, then, is we, we take our set of fringe nodes, and we select one node from the fringe, and we use the strategy to decide which node we will select first.

So this node is the next one we will explore in our search.

And by explore, I mean two things.

The first thing is, we apply the goal test to the state that corresponds to this node.

If this node is a goal node, that means we found a solution to our planning problem, to our search problem.

And we can stop the search here.

Note that this catches the case where out initial state was actually a goal state, because the first thing we do is we apply the goal test before we do anything else to this node.

If the gold task test has succeeded, we can simply return the path to this node, as this must be a solution to our search problem.

If our current node we are looking at is not a goal node, then what we have to do is we generate all the successors and we use the function expand here to do this.

So what we do is, we take the problem, and we take the node, and we apply the successor function that is defined as part of the problem to this node which gives us a set of actions and new states.

And each of these states can be turned into a new node.

Together with the action that led to that state.

And we expand the fringe so we add these new nodes to the fringe.

And this gives us our new fringe.

And, at that point, we go back to the beginning of the loop, and start again.

We look at the fringe, whether it's empty.

If it's not empty, we select a node from the fringe.

So this will be now a, a node at depth one.

Then we apply the goal test to that node.

If it's not a solution, then we have to do the same for that node, expand it, generate its successors, and so on, until we finally come to the point where either we've explored the whole graph, which means the fringe is empty, or we come to the point where a node is a goal node and passes the goal test and in that case our search is done.

Now there are a few subtleties, with this algorithm that I briefly want to go into.

The first one is that a tree corresponding to a finite search graph may be infinite.

Look at this very simple tree.

We have two nodes here.

And we can go from one to the other.

And then we can go back.

If neither of those two nodes is a goal node, that would give us an infinite loop.

We can always start here.

Then we add its successor to the fringe, which is this node.

Then we have only one node on the fringe.

But then we go back to this node, add this to the fringe.

We go here and here.

And our search tree will be infinite.

So, that means that this loop here.

May never terminate even if the search graph is finite.

And the second subtlety has to do with the strategy and that's what we will look at next.

The search control strategy is an effective method for scheduling the application of the successor function.

We have seen this in the algorithm.

The strategy tells us which node from the fringe we will select next to apply the successive function to, and generates its successors.

Now, the first thing to note here is that it's got to be an effective method.

So what we mean by that here is that the strategy must not take up too much time to decide which node from the fringe to expand next.

Ideally, it would take constant time.

What the strategy then does is determine the order in which we expand nodes, or the order in which we explore nodes in our tree, the order in which we go through our tree to look for a gold state.

Our aim, of course, is to produce a gold state as quickly as possible, so we want to descend down our tree to a gold state that is relatively close, and we come across that quite quickly.

So a perfect strategy would know where that gold state is an r tree.

But then, if we knew where the gold state is, we wouldn't have to search for it in the first place.

In general, a strategy that produces a gold state quicker than another is considered a better strategy.

And the way we have described the previous algorithm in the previous slide, with the strategy as an argument to the node selection.

This makes it a deterministic algorithm, as long as the strategy is deterministic.

Otherwise, without the strategy, you can read this as a non-deterministic algorithm.

The way a strategy can be implemented is by keeping the fringe nodes in a specific data structure.

A queue could be either a last in first out queue, or a first in first out queue, and that results in different search algorithms.

For example, the LIFO queue, is effectively a stack, which means our search proceeds by always exploring the node that has, that has been last been added to the queue, which means it does a depth first search.

It goes deeper into the tree before it examines the neighbors of a given node.

Feefo would give you a breadth first search by going to one level of the tree before it goes to the next.

But there are many, many more ways you can think of how to search and we will look at something called a heuristic in the next week.

just another example, you could also take an alphabetical ordering of the nodes, if you can't find some ordering relation between the nodes.

Another thing that is quite important is that in many search problems the complete tree is far too large to fit into a computer's memory, even with the kind of memory we have today.

So the search strategy determines which part of the search tree will be explored and will be in memory, and therefore it may determine whether we are successful at all.

Because if the search tree is too large for memory, we may not come across a goal node at all.

Now for those of you who are not familiar with search I recommend that you either go to the Russel and Norvik textbook that is mentioned on the website and have a look at the corresponding chapters or you should go through the programming exercise and implement the missionaries and cannibals problem as a search problem.

You can use either of those two queueing strategies mentioned there.

Lost and first out, or first and first out, and just see what happens when you apply these two strategies.

Also, a hint regarding repeated states.

To turn the tree search into a graph search, you have to remember all the nodes you've seen so far.

And the most effective way to do this is to add them to a hash table.

And before you add a note to the fringe again you check whether it is already in that hash table.

You should be able to do this in the programming language of your choice.

the missionaries and cannibals search space, as you have seen is very small, so the search should be instantaneous whatever you use.

So, have fun with that.

##Example Problems

We have now seen how planning problems can be formulated as search problems.

And we have also seen a basic algorithm that can be used to solve search problems.

We have also already seen some examples of search problems such as the missionaries and cannibals problem.

Now we shall look at some more example problems.

The first problem we will look at is a toy problem.

And it's a problem that has been around for a long time.

The sliding tile puzzle or sliding block puzzle looks essentially like this.

What we have is a grid.

Here, it's a three by three grid.

And in this grid, we have eight tiles.

That is, one square remains empty.

We can then shift the tiles in the grid.

But, of course, we can only shift them into the empty slot.

Leaving another slot empty as a result.

The tiles themselves are numbered one through eight, and our task is to bring them into the right order, and the right order is showing here on the right, and this could be some initial state from which we start.

So to formulate this as a search problem, we have to ask, what is the initial state?

And the initial state for this problem could be any configuration of the tiles.

So not just this one here shown on the left, but really any configuration you can think of.

The goal state is usually fixed depending on where you look there are different varieties of what the goal state should be like.

This is just one possible state that you see on the right where the tiles are in the correct order.

Sometimes, you'll see the middle square should be empty, sometimes it's another square that should be empty.

But there's usually one fixed goal state that much be reached in this puzzle.

The actions that are possible, and I've already told you this.

Are to move, a tile that is next to the empty space into the empty space.

So I could either move the eight here or the four up.

Or the six into the empty space.

In this case, for this puzzle there would be three possible actions in this state.

For the path cost, we simply assume that moving a tile has a unit cost, so all actions have equal cost.

So, that's it.

That's the definition of the eight puzzle.

There are also larger versions of this puzzle on a four by four grid, with fifteen tiles, known as the fifteen puzzle.

Or on a five by five grid with 24 tiles, known as the 24 puzzle, and so on.

The next problem is also a toy problem.

And it's known as the N queens problem in general, but we will look at the eight queens problem here.

And the problem, is to find a configuration for eight queens on an eight by eight chess board, such that none of the queens attacks any of the others.

For those of you who don't know chess, a queen is simply a chess piece that can be somewhere on the board.

And it can move along any diagonal, any column, or any row.

So, if we look, for example, at this queen here.

It can move anywhere along this diagonal, or this diagonal.

Or it can move anywhere in its row, or anywhere in this column.

So you can see, this queen attacks none of the other queens, and is attacked by none of the other queens.

If we look at this queen, however, we can see that if we move all the way along the diagonal.

This queen is attacked by this queen.

So this configuration shown here is not a solution to the problem.

Now you may wonder how this can be seen as a planning problem because what we're looking for is a configuration, so a static state.

And the answer is that we can look at this as a problem of reasoning about action.

When we look at this here, it says we have to place the queens on the chess board.

And we place the queens, one after another.

So the initial state is the empty chess board.

The actions we have available are placing queens onto the board.

And the goal state is any state where the queens don't attack each other.

Again we can assume a unit cost for the actions, so all actions are equally expensive.

And I'll give you a hint how this can be formulated a little more efficiently because, if we allow a queen to be placed on any board position, we have 64 different positions.

And this would create a pretty big search tree.

So what we can do is we say that the first action can only place a queen in the first column.

So we can only place a queen here.

The second action in our plan must place a queen in the second column.

And so on.

So our plan will definitely be eight steps long and we only have eight possibilities for each step in the plan.

So this defines the N queen's problem as a general search problem, or as a planning problem.

The next problem is more like a real world problem, and it is the dockworker robots problem that is also used in the book this, course is based on, so you will see this problem occur again and again.

The aim with this problem is to have one example that we can use to illustrate many of the techniques and algorithms that will be introduced in this course.

The advantage of this problem is that it is not trivial to solve, but it also can be described fairly quickly.

the informal description of this problem is this, we have a harbor in which we have several locations which may be docks or other places.

at these docks there may be ships, and we have storage areas for containers that need to be moved around at the docks or loaded onto ships.

We also have parking areas for trucks that can carry containers.

And they will be robots, in our cases, or trains that arrive with the containers.

There will also be cranes to move the different containers around.

They can load containers on to ships or robots or they can unload them from ships and robots.

And what we will be interested in is generating plans for the cranes and the robots that achieve certain configuration of containers that we give as goal states.

Now there are some people who say that block stacking or container stacking, which is essentially the same thing, is not a real-world problem.

So this is why I've included this picture here, so this proves that it is a real-world problem.

Now let's look at the state of the world in the dock worker robot example.

What we have in each state are locations and in this example there are two locations L1 and L2 and these locations are connected by paths along which the robot for example can move.

So these two locations are adjacent because they are connected by a path.

At each location we have cranes which can be used to move the containers around.

They can move the containers between the different piles that are available.

And here at this location L2, we see there are two piles, P2 and Q2.

In this case they are both empty.

There's no containers on these piles.

But at the bottom of every pile we will always have a pallet onto which we can stack the container.

So the pallet tells us where the bottom of the pile is.

A pile itself is an abstract object.

It doesn't necessarily need to have anything in it.

An empty pile isn't really something we can touch.

But it can have containers in it.

And containers are, of course, what we have the most of in most states.

So here in this example, we have six different containers.

They're called CA, CB, CC, CD, CE, and CF.

And they can be stacked on top of each other in a pile, as I've already explained.

There's no limit to how high a pile can be, despite the fact that it looks in this picture like the crane can only reach so high, but we will consider arbitrarily high stacks of containers.

Finally, what we have in these states are robots.

This is one robot here, though so there's only one robot.

That is an object that can be loaded with a container by the crane.

And then once it has a container on it, it can move to a different location.

It, of course, can also move, when it's empty, to a different location.

But there can only be one container on a robot at one time.

Also, at each location, we can only have one robot at a time.

We consider a location occupied when a robot is there.

So, I think that's pretty much all you need to know about the world states and the dock worker robot example.

But just to clarify this, not all states will have two locations, and six containers, and one robot.

We will consider a different world with many different numbers of robots, containers, or locations and cranes and they will give us more complexity to the problem.

You can probably guess what the different action types in this domain are, but here they are just to make sure that we are all on the same page and understand what they are.

The first action.

Is the move action and the move action moves a robot, that's the first argument in this action, from one location, the second argument, to some adjacent and unoccupied other location, L prime.

So as result of this action, the robot will be now in a different location.

That other location must be unoccupied and it must be adjacent, which means there must be a path that connects the two locations, L and L prime.

That was the only action that is to be done by the robot.

The other actions are all done by the crane.

So, the first one of those actions is to take action and we can take a container C, with an empty crane K, from the top of a pile.

And they must all be located at the same location L.

So this action as a result will have the crane holding the container and all these objects must have been at the same location obviously.

The river's action is to put down the container C that is held by the crane on top of a pile.

And they must again all be located at the same location L.

Two more actions, again, both to be done by the crane.

The one is loading a container.

But this will involve a robot, too.

So what we do is we load a container C, that is held by the crane K onto an unloaded robot R.

So the robot must not already have a container on it.

And, again, this must all be at the same location.

And the reverse action is to unload the container from.

The robot.

That's the robot.

And, again, the crane must be empty so that we can do this, and all of the objects must be at the same location.

So, these are the five action types that are available in the doc worker robot domain.

Notice that I said action types, because there can be many instances of these actions, depending on how I instantiate these parameters.

So if there's only one robot, then this robot may be the only one that can move, but it can move between different locations, and these are all different actions.

We will specify all this more formally when we have introduced an action description language, and that will be next week.

And here is a very simple example of what the state transition system that corresponds to the dark work or robot problem could look like.

You can see this is a very simple problem, because a large state space wouldn't fit onto one slide, so I had to reduce everything to one robot, one crane, one container, and two locations, which gives us six possible world states, and they're named here S0, S1, S2, S3, and so on, S4, S5.

The objects you see in these states are exactly the things I have just talked about.

So we have a robot here.

We have a crane.

And we have a container, a palette, and the two locations I just talked about.

And the actions are in principals.

Also, those I've just mentioned.

And they give us the state transitions.

And we've labeled them here, the state transitions.

So the take action gets us from state s zero to state s1.

And so on, put action would get us back to that state, we have move one action which moves the robot to location one and we have a move two action which moves the robot to location two.

As you can see from the state transitions, the system is static.

Which means there are no events happening.

All the actions are under our control.

Also, the system is deterministic.

Which means, every action leads to exactly one other state.

So there are no two transitions from one state that lead to different states given the same action.

If we do move one in this state, we always get to this state.

There's no other option.

This is a deterministic state transition system.

Like I said before you will see a lot more about the dock worker robot problem so I will stop here.

So this concludes this first segment of the course.

And here is a quick summary of what we've learned so far.

The first question we've looked at is'What is planning?'.

And more specifically'what is planning in AI?'.

And the answer we have come up with is that planning is an explicit deliberation process that chooses and organizes actions by anticipating their outcomes.

Planning in AI is the computational study of this deliberation process.

Next, we have looked at a conceptual model for planning, which helps us define the semantics of planning problems.

To this end, we have defined the state transition system, which consists of four things.

A set of states, a set of actions, and a set of events were the first three components.

And the most complex one is the state transition function, which maps a given state and an action or event, to another set of states.

Then we have looked at the connection between planning and search.

We have seen that planning problems can be phrased as search problems.

Search problems can be defined by an initial state, a set of actions that are possible in different states, a set of goal states, and a path cost functions that tells us how much the individual actions cost.

And finally, we've seen a number of example problems that can all be phrased as search problems.

##Context - Practical Systems

This section of the course will look at the very wide range of research and development areas that have been explored in AI Planning.

It'll put AI Planning techniques in the context of domain modeling and plan representation.

Consider engagement with users, task assigners, and decision makers, who must select between plan options.

And would include the important aspect of plan execution for practical systems.

The course logo shows a number of applications of practical AI planners over the years.

The Nonlin planner was used in the mid 1970s to generate project plans for electricity to [INAUDIBLE] overhaul.

The project plans in this case were prepared well in advance, because it's costly to have these machines out for any length of time.

But while the repair's taking place typically problems are found, extra maintenance operations need to be injected.

So the plans had to be adjusted as you went along.

And Nonlin was able to do that.

In the late 1970's, Steve Vere at NASA's jet propulsion laboratory created the divisor planner, which was used to generate action sequences for the Voyager spacecraft.

Though it wasn't used for uploading live action sequences to Voyager.

Deviser was based on the Nonlin design.

But added the ability to represent time windows on the activities.

Which is important for this kind of robotic application.

Nonli'ns hierarchical task network approach was also used in some of the earlier research on the robots in Japan.

Assembly, integration and tests of the payload bay for the European Ariane rockets was carried out using a planner based on the O-Plan design.

O-Plan has also been used in a wide range of applications in emergency response and search and rescue.

Edinburg  AI planners have also been used in applications as diverse as help desks, tanker delivery, and many others.

Even the earliest AI planners were used in realistic and practical applications.

The STRIPS planner from the early 1970s was used to control the spShakey robot.

And, as you can see from this table, there's a range of planners over the decades which have been applied to a wide range of application areas.

And we're going to cover some of those in a later module of this course.

As part of this cause, we're going to encorage you to look at a number of readings.

In particular, there's two reviews of AI planning, which we hope you'll find useful.

One is from the 1990's, so it's quite dated, but it does include a lot of the work on those early planners and some of the formative techniques as used in them and that you'll learn about in this course.

And that's the Hendler, Tate in Drummond paper in AI magazine, on AI planning, systems and techniques.

Is also a review, a decade later, by Wilkins and desJardins, on a call for knowledge-based planning, where techniques of knowledge-based systems, and planning were being brought together.

Full details of these readings and the locations where you can find the PDF copies of them will be available in the cource wiki.

Then there's a number of readings, some of them very short readings.

on specific planners and their applications.

And we hope you might dip into these during the course.

If you've got time to read just a couple of these, these are the ones we'd suggest.

The initial review of AI Planners to 1990.

And a very short paper on O-Plan one of the Edinburgh planners, which looks at a wide range of techniques and applications that were in use in that, in that system.

There's a diagram in the 1990 review paper which tries to trace some of the historical connections between the different planners.

The techniques they drew on and how they build on one another.

It's worth looking at that in detail.

Perhaps you could pause the video at this point and take a good look at some of the techniques and areas in there.

There's an online version of this diagram also available at high resolution if you can't see it in the video that you're using.

##Context - Tasking, Execution, Agents and Plans

Planning usually occurs in a human context where tasks or objectives are assigned, where plan execution must take place.

We introduced earlier a model for planning and showed some simplifications to it.

We explained that a richer model is needed for realistic planning in dynamic situations.

In such a context, we've got to observe the performance of activity in the world, and try to observe and interpret the effects of our executed activities there.

This can allow for possible plan revision and replanning where that's necessary.

O-Plan for example, could be configured with three separate planning agents.

Which represented reasons about plans at different levels.

The first was concerned with the human level of task description, setting objectives, giving guidance, or overall constraints.

A second level is a more technical planning level which could propose options to meet the state's objectives, and allow for choices between those options.

And a third level supported plan execution and monitoring.

And could facilitate local planned repair where that was feasible.

However, note that in each case here, each agent has a similar structure which is supported by the open planning architecture, in which the O-Plan planner was built.

For example, the multi-agent planning architecture platform is the basis for the SRI International Side Planner.

The Open Planning Architecture is the basis for O-Plan designed to handle multiple plan roles and levels as you just seen.

And the I-X platform is intended to port multiple types of command, sense-making, analysis in planning for instances to build the I-plan  planner, as well as decision making, execution, and communication between agents even in mixed agent frameworks.

Several practical AI planning systems have been used in emergency response applications.

Often working in the context of a wide range of other agents and services.

They're often in an environment where communication and collaboration is taking place with some sort of infrastructural layer.

And then the core might concer, be concerned with command and control-type issues in search-and-rescue or emergency response situations, but also dealing with people in the field.

And so they are also manipulated the plans that they're given and adjusting them to a local situation.

We may also be including the people who are in trouble or are isolated or being rescued as part of the framework of collaboration and communication that's taken place.

And again, they have to deal with elements of the plan that are coming from these different levels and different centers.

And we perhaps have to deal upwards with decision making authorities by giving briefings of the options that we're generating, explaining those, and looking for decisions on, on which ones are going to be executed and handled.

We'll show you a couple of examples of I-X multiagent applications.

This is an I-X process panel which sports a search and rescue coordinator and the communications with various other agencies and services in a coalition peace keeping operations.

The second application called I-Globe handles the tasking and coordination of mobile ground and air vehicles in a disaster response scenario.

Often, planning is just a small part of a wider collaboration and communication situation involved in a realistic application.

The I-Plan planner here sits in the lower right, and it's quite a simple tool.

But it operates in a much broader context of planned communication, messaging, map views, sense making, and so on.

A paper written in 2001, was part of a special issue in Mayhem Magazine on Robocop, explored emergency response as a grand challenge for multiagent and intelligent systems.

It explored the very many areas involved in effective emergency response, which could be supported by intelligence systems.

These included dealing with the environment.

Supporting the people and organizations involved, and of course, using intelligence systems to assist in the emergency itself.

The key element of this support in both planning and activity management and the sharing of plan related information between agents.

Plenty used for communication and collaboration between agents and systems.

So upon representation itself, is an important area of research.

Plenty used in many areas beyond activity planning.

Such as in, situation understanding summarization, natural language interpretation, and generation and so on.

Plents provide nontological informal representation call for a wide range of practical applications and uses.

A good representation of plans can be used as a target for knowledge acquisition, for user communication, for system manipulation, and as a basis for formal analysis.

AI planning work has influenced standard related process and plan representations used by many industries and fields.

For example, in the MIT process handbook, its process interchange format or PIF.

In the NIST, the National Institute of Standards and Technology in America,

the Process Specification Language, PSL.

And the DARPA, Shared Planning and Activity Representation SPAR.

And has even become the basis for an ISO standard.

##Context - Example Planners

We're now going to look at a couple practical planners in operation.

You'll find some excellent online resources for some of the well-known planners listed in our table.

The course readings have also been designed to introduce some of these and we provide screen captures of a number of our own Edinburgh planners in operation.

Nonlin was created in the mid-1970s, and as mentioned earlier was applied in a project using mixture of A.I.

and Operation Research, O.R.

methods, to create project plans for electricity turbine overhaul.

It allowed for user-driven modifications to the plans as maintenance inspection took place and plans up to be updated.

Nonlin brought together range of A.I.

planning techniques, which were novel at the time.

In this slide, I'm using the terminology now commonly applied to these techniques.

Nonlin was a hierarchical task network planner.

It could generate plans in which the activities were not fully ordered.

It had been common up until that time for A.I.

planners to generate plans which were fully linear in order.

Nonlin searches in the space of planned descriptions rather than in the application state base, a subject which we're going to cover in the later module.

Nonlin uses the goal structure of the plan in considering the different alternatives.

So it always looks at different approaches based on that plan rationale.

Nonlin as a mechanism for answering the question of whether a proposition has a certain truth value at a certain point in the plan.

We call that the QA module, question answering module, is more often, nowadays, called the Modal Truth Criterion that use condition types to limit search.

It allowed for multiple contributors to condition achievement in order to achieve plan robustness.

It included compute conditions for links to external databases and systems because it was built as a practical planning system.

These are similar to attach procedures in some other planners.

And as mentioned earlier, is a mixture of A.I.

and operational research methods, so OR algorithms were used for time and resource constrained reasoning.

Nonlin has been used as a basis for textbook descriptions of hierarchical task network planning.

O-Plan followed on from Nonlin and used some of the same techniques.

It included domain knowledge elicitation and modeling tools and supported a rich plan representation.

It also was a hierarchical task network planner and had detailed constraint management.

It allowed for plugging constraint managers, so that that that aspect could be extended.

Gold structure-based planned monitoring during execution was supported and it supported dynamic domains in which new objectives or tasks could arise while planning an execution was taking place.

Plan repair when time was available or when the situation demanded rapid response was supported.

Interfaces for users with different roles were provided in order that several different users could be refining the plan simultaneously and sharing information about the plan between them.

And explicit management of the planning and execution workflow was supported, so the planner itself could be used in, in these aspects.

There is a simple example of O-Plan acting as a Unix system administrator's aid.

The aim is to create a shell script that reliably removes a volume group.

This is an application which a lot of people have found difficult and can lead to loss of files.

Once the current mapping between logical volumes and physical volumes are, are given by the drop-down numeric boxes, you can just hit the plan button and the planner goes off, and produces the plan, and it prints it out as the script ready to cut, and paste, and use.

Here's a more comprehensive example of O-Plan running on a military operations in urban terrain and where task description planning and execution workflow are all able to be supported with multiple users acting over the internet and all sharing a common plan.

so typically, there will be a task assigner who will set up the problem.

It would go in and define this with simple web form with drop-down lists of the kind of tasks that can be carried out.

And then, a planner can be called to generate specific operations and, and make suggestions of how to respond to the particular emergency, the particular disaster that's occurring.

And the idea is, that between the planner and the task assigner, they can talk and look at these options, refine them, throw away some replan when things are being added, and so on.

And all this is occurring through these user interfaces talking to the planner behind the scenes.

O-Plan was used as the basis for Optimum-AIV, a planner for assembly, integration, and verification of the payload bay of Europe's Ariane rocket launcher.

It was originally designed as plan ERS-1 for assembly integration test of the European Space Agency's ERS-1 spacecraft shown here, on the left.

Optimum-AIV also used the rich plan representation, hierarchical test network planning, detailed constraint management, plan and user rationale recorded.

It could handle issues arising during planned execution and deal with it, these through plan repair using test failure recovery plans.

It was integrated with European Space Agency's Artemis Project Management System.

Let me draw your attention to a number of features which are typical of practical A.I.

planners.

Often, these are based on hierarchical task network planning at an outer layer.

They often also allow for partial order of the resulting plans rather than insisting the plans are fully ordered.

They often include a rich domain model that can manage many types of constraints and include links to simulations, analyses to refine then restrict the plans.

These planners often allow for integration with other systems, user interfaces, databases, spreadsheets, project management, corporate systems of various kinds.

And this is very typical of practical planners and often, much of the effort that goes into a practical planning application can go into this kind of integration.

##Context - Planning++

We'll take a look at the very wide range of planning research areas and techniques.

There's been quite a lot of work in domain modeling, domain description, and domain analysis, prior to using these descriptions, these domain descriptions in planners themselves.

Then of course the core work in AI planning is being in the search methods, and algorithms for creating plans, searching for plans.

People have looked up plan analysis because if you can constrain the types of plan you're interested in through analysis, that can limit the search spaces involved.

It could be useful to pause here and to read through this list of areas and techniques.

How many have you already heard of?

Check out or ask about any acronym you don't know.

Continue on when you're ready.

Then people concerned with realistic and practical planning problems have often had to deal with the hue of repairing plans when things go wrong or when the circumstances change.

A number of people have looked at planned generalization, the reuse of generalized plan fragments.

The sport of the user involved in planning via a suitable user interface is, advice interfaces and the ability to generate plans in a mixed issued fashion between the automated system and the human are important.

In recent years, planners have been made available as web services in order that they can be used as a component of larger scale systems.

And plans can be used in a range of other areas, such as natural language generation, dialogue management, the sharing of plans between humans and so on.

Pause again, to look at the research areas and techniques listed.

So as you can see here, we're dealing with the whole life-cycle plans, right through from, creating domain models, to making use of those plans in productive situations.

The problem is that we need to make sense of all of this.

We need to find a way of fitting in the very many techniques that are available now and that will become available through further research and development.

What's needed is more collaborative planning framework.

At the alta level, we've got to be able to relate to the humans involved in the planning process, where they can present their objectives, their issues, they can make sense of the situation.

They can give multiple options and advice.

They can argue about those options, discuss them, outline plans, and so on.

Then we want detailed planners, search engines, constraints, all that analyzers and simulators, that can act within that outdoor frame, and work in an understandable way, and use it to provide feasibility checks, detailed constraints, and guidance.

Want to be able to share processes and information about process products between humans and systems, and want to look at the current status, the context we're in, the environment we're in, you know, to be sensitive to that.

We need a link between informal and unstructured planning and more structured planning and methods for optimization.

And that's what we've been trying to do with our I-X I plan work, just as an example of how some of this integration is going on in practical A I planning systems.

First of all, we base it on an underlying, intelligible, easily communicated, easily extended, conceptual model for objectives, processes, procedures and plans.

Just based on four components, a set of issues to be addressed, a set of nodes, which are activities to be included in the plan, a set of constraints which have got to be respected and a set of annotations on the entities that are involved there.

We call that model I-N-C-A

Then we want to communicate the dynamic status and presence of the agents involved.

The collaborative processes that process products, and what they, what they're able to do in terms of their capabilities.

Want to be sensitive to the current context so the presentation of options for action are those which are suitable for the context we're in.

And we want to do intelligent activity planning, execution and monitoring repair, and plan repair, and we do that via the iPlan planner and what we call the I-X process panels, which are the user interface element in the I-X technology.

So I-Xs aim is a planning workflow and task messaging catch all.

We designed it in order that it can deal with the wide range of problems that can be addressed in planning and activity management.

It's meant to be able to take any requirement to handle an issue, perform an activity, respect a constraint, or note an annotation.

So that's the underlying computational model of the I-X platform.

They can deal with these by a manual activity, so you can basically, just take items off, for instance, on checklist.

That's a perfectly good way of representing the fact that you've performed an activity in some systems.

But also, by inbuilt internal capabilities in the system, by external capabilities you know about, or by rerouting or delegating to other panels and agents, which is where communication and collaboration and the models of those, those other agents comes in.

And all we can plan next year to compensate to these capabilities and we can use the planner inside the system itself, to plan that kind of work flow.

And then the system receives reports and interprets them to understand the current state it's in, and to be able to start to handle that situation and help the user control the situation.

And the idea is it can cope with just partial knowledge of the process and organization involved, and be able to fit in to an environment where it isn't the only agent that's working on plans and processes within an organization.

I-x, as we've said before, has been applied to emerge in serious bonds, and we've used this kind of process panel or user interface that's what the task.

But it's in a context where there is a lot of verbal tools.

The main editors, links to mapping tools, the planner itself can have a pop up window, its like you look at the options its generating and guide them and give them advice and so on and you can link to messaging and communication tools to the systems.

So that concludes this series of presentations on AI planning and context.

We've been looking at the context of practical systems, and we've shown you some practical planners in use.

We've looked at the context of task assignment, and execution, the fact that we're often working with multiple agents, multiple systems and multiple services, and we've looked at the context of planned representation itself, where rich plan representations can be used for many kinds of communication and collaboration in activity and, and planning situations.

And finally, we looked at Planning++, to try to make sense of the wide range of techniques being developed in this field, and looked at a model for bringing some of these techniques together in a productive way.

#Week 2

**Introduction**

Welcome to week 2 of the AI planning course.

We've already learned a lot of things in week 1.

For example I've introduced to you the basic planning problem which is the problem we are addressing in this course.

I've also told you about a technique that is used in many places in AI but specifically it's very important to planning and that technique is search.

Then we've met some of our friends the duck worker robots an example we will be using throughout the course.

This was followed by my colleague Austin Tate telling you about practical planners and applications where these are used.

But planning is not just about finding plans there is also a context to planning and this is for example what happens before a planning mainly the assignment of tasks to Planners.

And after planning, the plan execution which is very important.

Then, we've also seen that there is a range of techniques that are used in planners today.

That was pretty much what we've learned in Week 1.

And now I want to talk a little bit about the website.

I've already seen a lot of you have used the social platform that comes with this course, which you can see here.

And I would like to encourage you to use the discussion forums.

To bring up any questions, any issues, that you have with the course material and hopefully some of the the community that uses this forum will answer those questions for you or we, the instructors, can help as well.

In this week's first segment we'll be looking at informed search or more specifically the A\* search algorithm.

A\* is a search algorithm just like the ones we've seen last week.

It takes an implicit graph and searches it in its basic form as a tree.

Shown here is the search tree generated by the A\* algorithm for the touring Romania problem where the task is to get from Arad to Bucharest.

What is new here, is that the algorithm uses a number to guide its search, and this number expresses how far from the search node the algorithm thinks the current node is.

This is called a heuristic.

And this heuristic is used to compute some evaluation function that tells the algorithm which node to expand next.

In this graph, we see the numbers here which are the value of this evaluation function.

So, what this algorithm does is use an informed search strategy, as opposed to the uninformed search strategies we've seen so far.

And probably the best known informed search strategy in the A\*t algorithm which is what we will see in the first segment this week.

In this week's second segment we will be seeing our first planning algorithm which is the forward state space.

Search/g algorithm.

This uses the search technology we've seen in the previous segment.

As you will see in detail, this algorithm is actually very simple.

It takes a planning problem as input, which is these three components you see here.

And then starts a loop where it starts from the initial state and builds up a plan starting from an empty plan.

That will satisfy the goal.

The first thing it does is the goal test, which is just what we've seen in our search algorithm.

So this is the goal test here.

And then generates all the applicable actions in the current state.

If there are none, then of course we have failed.

Otherwise it just chooses one of the applicable actions.

That is our new action that we apply in our state.

Then we go to a new state by going forward from our current state.

And extend our plan with this current action.

And we go through this loop, until we have reached a goal state.

And therefore we have found a plan that achieves our goal.

But before we get to this algorithm, we will see a formal definition of what constitutes a planning problem.

And most importantly we will see the scripts representation for operators, which is the set O here, which describes an operator as something consisting of preconditions and effects.

That's what we will look at later this week.

So now it's time to get into the material for week 2.

Week 1 was fairly lightweight, and you've seen an informal introduction to planning.

In Week 2, we will see the material a lot more technical.

We will introduce algorithms, and you will have something to implement, if you want.

##Heuristic Search Strategies

Previously we have seen how the general tree search algorithm uses a strategy to determine which node on the fringe to explore next.

An example of such a strategy was a first in first out queue.

We call this an uninformed source strategy because it does not use any information about the state itself, it only uses information about when the state was queued but nothing that is internal to the state.

We will now look at heuristic search strategies which use information about the state on the fringe to determine which note to explore next.

Information about a state that can be used in a search strategy can be encoded using a heuristic function.

In general, a heuristic function H, maps a node in the search space to a real number, R.

Or sometimes, you also find it maps it to a natural number.

What a heuristic function encodes is the estimated cost of the cheapest path from the given node to a goal node.

So the heuristic function tells us how close is the nearest goal node.

Obviously if the node N given to the heuristic function is a goal node, then the value must be zero.

That is, we are already had a goal node so that nearest goal node is in distance zero.

As you can see, a heuristic function encodes problem specific knowledge in a problem independent way.

For each problem we are looking at, we can define a different heuristic function.

Which is why the heuristic function is problem specific.

But whatever search space we're looking at, whatever problem we're looking at, the heuristic function will always give us a numeric value for each state.

And the fact that it is a number is problem independent.

A perfect heuristic function would always give us the correct distance to the goal node.

But if we had such a function the search wouldn't be very hard in fact it would be trivial.

Unfortunately perfect heuristic functions are very hard to find for most of the problems we'll be looking at.

Best-First search is an instance of the general tree search algorithm, where we use the knowledge provided by the heuristic function to decide which of the nodes on the fringe looks best for expansion.

In fact, Best-First search is a whole group of algorithms, as we can use the heuristic in various ways to decide which node looks best.

And Best-first search can be used as a tree search or graph search or algorithm, depending on whether we use the test for repeated nodes, or not.

In Best-First search, we use the strategy, that uses an evaluation function F, to decide which node in the state space to explore next.

and again, the evaluation function maps a node in the states space, to real number R.

In general we will choose that node from the fringe which has the lowest value F.

So the evaluation function determines the search strategy in Best-First search.

Again, if we had a perfect evaluation function we could use the search to lead us straight to the goal node.

Note that the evaluate function is not problem specific.

It is specific to the algorithm.

But the evaluation function may use the heuristic function, which is problem specific.

What we mean by best in Best-First search is simply defined by the evaluation function.

The node that is best has the lowest F value.

Now, a quick word about the implementation.

There are really two operations that we need to support, when we look at our fringe.

When we generate the successors of a node, we need to add those to the fringe.

And when we select a node from the fringe for expansion, we need to select the node with the lowest F value.

Since we will do both of these operations quite often during the search it is important that these are cheap operations.

The good way to implement this is by means of a priority queue.

A priority queue maintains all the nodes in the fringe, in ascending order of their F values.

A priority queue can be implemented as a binary tree, which means O adding a node and retreading the node with a lowest F value has a algorithmic time complexity.

The simplest Best-First search algorithm is probably Greedy Best-First Search.

This algorithm simply uses the heuristic function defined for the problem as the evaluation function used by the algorithm.

Remember that the heuristic function is problem specific and encodes the distance to the nearest goal node.

Whereas the evaluation function is not problem specific and is used by the algorithm to determine which node to expand next.

So the meaning of these two functions is really completely different.

But Greedy Best-First search simply equates the two and uses the heuristic function to give us a search strategy immediately.

The result is an algorithm that always expands the node that is closest to the goal node next.

The algorithm is called greedy because it always tries to take the largest chunk out of the remaining distance to the nearest goal node.

It tries to get to the goal node in as few steps as possible, but since the number of steps isn't necessarily the cost of a path, this is not necessarily the optimal strategy.

In fact, Greedy Best-First search often gives us solutions that are far longer than the optimal path, and also far more costly.

So let's look at our touring Romania problem to see how Greedy Best-First search works.

To remind you, the initial state was that we are in Arad.

Now suppose our goal state is to be in Bucharest, the capital.

The actions we have available are to drive along the arcs shown in this graph.

And each arc has an associated cost, and that is shown as a number next to the arc.

So from Arad, I could drive to these three towns and the costs would be respectively this, this.

And this number.

What we also need for greedy best first search is a heuristic.

And that's what we've got here.

The heuristic needs to estimate the distance to the nearest goal node.

And our goal node is to be in Bucharest.

So we only have one goal node.

And on a map, we can use the straight line distance to estimate the distance to the, to a different point on the map.

So we will use the Euclidean distance between two points on a two dimensional map.

The table you are looking at simply gives us the values of our heuristic function for different nodes N.

So, if the node N is Arad, the distance would be 366 rounded.

and so on.

For each city in our map, we have a straight line distance in this table.

And this is the value we will use as our heuristic value.

As is to be expected, the heuristic value for the goal note is zero.

Another feature of this heuristic is that it always underestimates the distance to the goal.

Let's look at a simple example here.

For example, we have figure S.

Which, according to the heuristic, is 176 from the nearest goal node, Bucharest.

But, going back to the map.

You can see that from Arad, it really 211 from the goal node.

That is because roads don't tend to be straight lines.

In reality, it is probably something like this.

And that's a longer distance than what the heuristic gives us.

So the real distance is 211.

But, going to the next slide again.

The estimated distance according to the, the heuristic, is 176, which is lower.

Another important observation here is that the heuristic presents us with additional information to what we had in the original problem description given by the map.

There is no way you can compute the values in this table from the information given in the map.

And of course this table presents problem-specific information.

Now, let us have a look at greedy first search in action.

What we see here is the initial state of the algorithm.

All the nodes you see are the fringe nodes, and there's only one node, of course initially.

Fringe nodes are shown in blue here.

That's the legend.

And the node we select to expand next is shown in red.

So currently, there is no node selected.

Within each node, you see the name of the city, plus the heuristic value for that node.

Also, on the right hand side, you see information about the depth of the different nodes in the search tree.

So the first thing the algorithm does is select a node from the fringe.

And since there is only one node, this is the one that's going to be selected.

Then the algorithm performs the goal test on this node.

Which will fail in this case, because this is not goal node.The next step then is to generate the successors of this node.

So now, our initial state, Arad is no longer on the fringe.

But its three successors are now the new fringe.

This means we have to go through another iteration of our loop.

And the first step is to select a node from the fringe.

We do this according to the strategy.

Which tells us we've got to select the node with the lowest f value.

So here, we have three nodes.

And the lowest f value is this one.

So this is the one we will select next.

Again, there's no, it doesn't not pass the goal test, so we have to continue.

We generate the successors, and add these to the fringe.

You can see what we are doing here is tree search.

Because we've generated Arad again.

So this node is the original node.

And we could go back there immediately.

For most search problems, applying an action, and then the reverse action immediately afterwards is not a good idea.

But let's continue with the algorithm.

So the next thing we have to do is select another node.

And we select the node with the lowest f value.

And here, that is Fagaras.

Again this is not a goal node so we have to expand it and add its two successors to the fringe.

Now we select a node from the fringe and the node with the lowest F value is Bucharest and this time the goal test will toss so we finished with our search.

We can now extract the path to the goal node simply by going up from our goal node, the one that we found through the tree, to the initial state and this is our solution path.

That's it, that's how a greedy best first search works.

##A* Tree Search

The problem with Greedy Best-First search is that it often finds sub optimal solutions, often very badly sub optimal solutions.

But the idea of using a juristic function, to determine the search strategy is a good one.

We will see this next when we define the A\* algorithm which always gives us optimal solutions.

A\* is probably, the best known algorithm in all of artificial intelligence and as far as I know it is described in every single AI textbook.

A\* is simply refinement of the best first search algorithm we have seen earlier.

The only difference to Greedy Best-First search is that it uses a different evaluation function.

So remember f, the evaluation function, tells us in which order we explore nodes on the fringe.

The heuristic h tells us the distance to the nearest goal node.

So far it is nothing new, what is new is this component g that we add to the uristic to get our evaluation function and this function g simply gives us the cost to reach the note end so that's the cost we already had to get from our initial state to this note end and to this we add a uristic which is the estimate to the nearest goal note from this note end.

So if we have an initial state I and we somehow to get our note N.

Then the distance between those two.

Is g of n.

Whereas from here, we somehow get to a goal node, and this is our nearest goal node g.

Then this distance is estimated by the heuristic function H, of N.

One way to look at this is that greedy best first search behaves a little bit like depth first search.

It tries to go deep into the search base, as quickly as it can to a goal node.

It always tries to eat at much as possible out of the distance to the goal.

By adding g to h, and using that as our evaluation function.

We sort of add a breadth first component to this depth first search.

In fact, the evaluation function we're using gives us the estimated cost of the cheapest solution through the node N.

Why is that?

Well, very simple.

The cheapest solution though N surely must consist of the path that goes from the initial state to N.

And the cost of that is given by G of N.

And it consists of the cost of getting from N to the goal Node.

And we can estimate that, using the function H, the uristic function.

So when we use this evaluation function to select the next node from the French we are selecting that node N.

Which looks to be on the cheapest path to A goal node.

And we can show that A\* search is optimal if our heuristic function is admissible.

And that means that it always underestimates the distance to the goal.

But I will get back to properties of A\* later.

So let's look at the same touring Romania example we've had before.

Our initial state is that we are in Irat, and we want to get to Bucharest.

Note that the number in brackets in each node is the F value, not the H value.

So this includes the G component.

The amount of path we've already covered.

For the initial node, G is zero, because we haven't gone anywhere yet.

So the initial, for the initial node, the, H value is equal to the F value.

So again, the first thing we do is select the node from the fringe.

And since there is only one node, we select that node.

And then we expand that node and add the successors to the fringe.

Whereas, a rod disappears from the fringe.

So if you go back you will see that the numbers and the different nodes are different from what we have seen previously.

Which is what I've just explained.

They contain the G component as well as the H component.

So again the algorithm proceeds by selecting the node from the fringe that has the lowest F value which is 393 in this example.

We continue by testing whether this is a goal state, which it is not so we have to expand it and generate its successors.

There are four successors again as before.

Arot is one of the successors so we're doing tree search.

But now, one big differences is that the number that we see with Arot is very different.

It's a much higher number because it includes the path that we've already gone through.

So this is not the same as for the initial state, because we've already gone through the loop through that other city, before we returned to Arot.

So we continue with our loop, and we have to select another note from the fringe.

Which will be the note with the lowest F value, in this case 413.

This is not a goal note, so we have to expand it.

And there are three more successors we add to the fringe.

Now something interesting has happened.

Previously this was our lowest value so we estimated that a path through this node could cost as little as 413.

We have expanded this node and seen that its best successor has a value of 417.

This is because the heuristic underestimates.

The distance to the goal, now we are closer to the goal the heuristic has become more accurate and we know the power is in fact a little more expensive.

What that also means is that there is a note higher up in the tree this one here that now has the lowest f value on the fringe so this is the one we will select next.

We will perform the goal test as before.

And since this is not a goal node, we have to expand this node.

Generating two more successors.

Including as you will see, one that is the goal node.

But, having generated to go on node does not mean that we are finished.

We will finish when we select to go on node, and try to perform to goal test on this node.

So let's select the next node from the fringe.

And the node with the lowest F value is now over here, with a value of 417.

That's the successor we previously ignored.

And since this is not a goal note we will proceed by expanding this note.

Generating three more successors and once again of, of those is the goal.

So our goal note appears twice on the fringe now but these have two different paths to the goal note.

Notice that the one further up is the one that Greedy Best-First search found earlier.

Now lets proceed with A\*.

A\* goes through the loop again and selects the note with the lowest f value, which in this case is the bookarest note at depth four.

It performs the goal test and finds indeed Bucharest is the goal node and hence we have found a path to the goal node and it is the optimal path.

We can go again back through the tree, tracing the way we came, to get the optimal path to this goal node.

So a star gave us an optimal path to the goal node, better than what Greedy Best-First search found.

However you can also see that this tree contains quite a few more nodes than the tree that was generated by Greedy Best-First search.

And that means A\* search is generally a little bit slower than Greedy Best-First search and unfortunately this is often the case.

The touring Romania example is not very interesting, because it is a relatively small search space.

So here's something that has a slightly bigger search space.

The 8 puzzle.

Again, to remind you, the 8 puzzle's character is by an initial state, there is one state that is given here, and one goal state, exactly, that is given here.

The actions for this puzzle were that we can move the tiles around the grid, and a good way to think about this is that we are moving the empty tile rather than the tiles themselves, which means there are, at most, four possible actions we can move the empty tile.

In the four different directions, which reduces the branching factor of the tree we are generating.

Also, it might be a good idea to test for reverse action, because undoing what we've just done immediately never leads to anything good in this search space.

Finally, we need to define the cost of the different actions, and we use a unit cost here.

Moving a tile is, same cost for every tile.

What is missing to apply best first search or Greedy Best-First search or a star search here is a heuristic function, and we will look at that next.

In fact I will give you two symbol heuristics for the eight puzzle.

The first one simply counts the number of misplaced tiles.

So we go through all the eight tiles in the puzzle, and check whether it is already at the right position.

If it is not, then we add one to our heuristic.

Because we know if it is not at the right position, we have to move the style at some point.

And since every action moves just one tile, that's a good heuristic to start with.

So let's look at this in this example.

This is our heuristic H1 number of misplaced tiles, well this is wrong, this is wrong, they're all wrong.

So we see that in this example the value of our heuristic is eight that means all eight tiles are in the wrong position.

The second heuristic H2, uses the manhattan block distance as an estimate to how far we need to go.

So again, we go through all the eight tiles in the puzzle, and compute the manhattan block distance, and add those distances together.

What I mean by Manhattan block distance is simply that all the moves we are allowed, are to go somewhere along the grid.

So there are only four possible ways in which one can move a tile.

So lets start with the first time, that's the number seven.

And the way, where we want the number seven to be is here so the Manhattan block distance is one, two, three.

And this is the first value we'll choose.

Then we have the two here.

And where should the two be?

The two should be here.

So the distance is one and so on.

If we continue like this for all eight tiles, we will see that the Manhattan block distance heuristic for this state is eighteen.

It is easy enough to see that both of these heuristics never overestimate the distance to the nearest goal.

It should also be easy to see that the second heuristic, H2, always gives us a much more accurate estimate of how far the goal node is away, but it is not a perfect heuristic.

The actual distance to the goal node from the state shown here is 26.

So if you feel like a little programming now why don't you go ahead and implement the eight puzzle using the a star algorithm and solve a few puzzles.

Use different heuristics.

Play around with it.

See what happens.

##Properties of A*

So now that you know how the A\* algorithm works, I want to go a bit in to the theory, and give you some properties of the algorithm.

These all rely on the fact that the heuristic use is admissible.

And we will talk about that first.

A heuristic is called admissible if it never overestimates the distance from a node to the nearest goal node.

So what this means is that the value of the function h(n) must always be less or equal than the actual distance to the nearest goal node.

The equality is actually quite important.

For example, the heuristic value of a goal node is equal to zero and therefore, it's equal to the actual distance to the goal node.

So any heuristic must have that property.

In other words, there's usually at least one node for which the equality holds namely, the goal node.

Unfortunately, this is often the only node for which this holds.

Otherwise, we would have a perfect heuristic, and that is too good to be true.

Admissible heuristics usually think the nearest goal node is closer than it actually is.

They tend to underestimate the distance to the goal.

An example of an admissible heuristics is one we have seen for the touring Romania problem, namely the straight line distance.

The shortest distance between two points on a map is usually the straight line.

Hence the straight line distance heuristic must underestimate the actual distance along a road.

Thus, it is an admissible heuristic.

For A\* search, here's what that means.

We set that the evaluation function f(n) represents the cost over the shortest path through node n to a goal node.

So if the uristic never overestimates the distance from n to the nearest goal node, then the evaluation function used by A\* never overestimates the true cost of a lu, solution through the node n.

This brings me to the most important property of A\* namely that it is optimal.

We can prove the following theory.

A\* using tree search is optimal if the heuristic h is admissible.

Just as a reminder, optimal here means that the algorithm is guaranteed to find the shortest path from the initial state to a goal node.

And this theorem tells us that if our heuristic is admissible, then A\* will return with an optimal path.

That is of course a very useful property for a search algorithm to have.

By the way a similar theorem can be shown for A\* using graph search but I won't go into this here now.

Another property of A\* is that it is complete which means that if there is a solution, A\* can find the solution.

This can be shown using something called contours.

And since this is quite an interesting concept, I want to introduce this here.

So contours are sets of states that can be reached within a certain cost.

This is a bit like a topographic map that you've all seen, I hope.

In a topographic map, you see lines indicating points of equal altitude.

Here the contour is a line indicating nodes of equal F value, and all the nodes within the contour belong to the set.

A prerequisite for being able to draw contours is that the f values along a path are non-decreasing.

So what this means is that as we move away from the initial state the f values are usually increasing.

Now the way A\* works fundamentally is that it starts from the initial state and then adds nodes according to these contours.

It always starts with a smaller set and then increases the F value as it goes along.

But it will always explore nodes with lower F values before it moves to a higher plateau, so to speak.

If our heuristic function always had the value zero, what A\* would do is essentially draw circles around the initial state.

If the heuristic always had the value zero, this would, of course, be uninformed search.

And, in fact, this algorithm has a name.

It's called uniform cost search, or Dijkstra's Algorithm.

But with the heuristic that provides some information what happens is that the contours become ellipses that are drawn towards a goal state, so they try to get closer to a goal state.

And the more accurate our heuristic is the more these ellipses stretch towards a goal.

A completeness of A\* simply follows from this, because as it explores more and more nodes the contours are growing and growing.

And eventually this must include a goal node.

This is true, because each contour can only contain a finite number of states.

And once these are all explored, A\* will go to the next higher altitude for bigger contour.

And here is what this looks like visually for our touring in Romania example.

We start off in Irat.

And the F value there was 366, so that's the smallest control from which we start that contains A node at all.

Then, A\* will continue to grow this contour.

Initially, as a small set, but then we have a slightly bigger set, that for example, has the value 400.

And as you can see this node here is almost on the edge.

And if you go back to the tree you will see it's F value is above 400, so it must be outside the set.

And so on.

So, A\* continues to grow this contour until it includes the Bucharest node.

In this example, you can see nicely how the ellipsis it draws stretch toward the goal node.

But you can also see it the other way around.

What this means is that A\* must explore all those nodes that are within a contour that has a value just less than the cost of the optimal path.

A\* needs to include all those nodes that are within this contour that almost leads to the goal node.

And often, that is still quite a large number.

So in this example, you see that most of the contours contain different nodes.

But think about the eight puzzle for a second, and what that looks like there.

In the eight puzzle the maximum goal distance you have is 31 steps.

Yet, you have just over 180,000 nodes.

So there must be many nodes that have the same F value.

Here is another property of A\* namely A\* is optimally efficient.

What this means is, that no other, optimal algorithm is guaranteed to expand fewer nodes than A\*.

This is of course true for a given specific heuristic function.

What this gives us is that, any other algorithm that guarantees us an optimal solution must expand at least as many nodes as A\* for a given heuristic function.

Of course, there can still be a more efficient way of finding a solution to a given problem namely if we have a different, more efficient heuristic.

But for a fixed juristic, A\* is optimally efficient.

Notice that efficiency here is counted in the number of nodes that are expanded, but of course that's not the only thing that makes an algorithm efficient.

For example, computing the juristic function has a computational cost but it does not count towards the number of nodes that we expand.

So A\* is only optimally efficient with respect to number of nodes it expands.

To be more specific, it's not only the number of nodes that any other algorithm must expand.

It is exactly those nodes.

Suppose there is another algorithm that is optimal, which means it returns an optimal path.

But it does not expand one of the nodes that is expanded by A\*.

What does this mean?

A\* expands all those nodes with an F value that is less than C* where C* is the value of the optimal power.

So, if another algorithm did not expand one of these nodes.

That means, in the search space of the algorithm, there's an unexplored node that has an F value of less than C*.

So that means the most promising path through that node that the algorithm ignores, looks better than the optimal path.

So if we don't explore that path, we can never find out that it actually turns out to be something worse in the end.

So in the end, A\* expands a minimal number of nodes that still guarantee an optimal solution.

##A* Graph Search

###a A* Graph Search

You have now seen the A\* algorithm for heuristic search and you should understand how it works.

A\* is a pretty good algorithm.

It always finds optimal solutions and it is optimally efficient.

So you may wonder, why would we ever need another search algorithm?

Well, there are a few more problems, and I will go into those next.

Some of these problems are related to the fact that we're usually searching a graph, not a tree, so we will look at a star as a graph search algorithm next.

And here is the pseudocode for the A\* tree and graph search algorithms.

The basic version of this algorithm is the tree search algorithm, and there is an extension that I've also included on this slide which consists of three additional lines of code, that performs the graph search.

And the three additional lines of code are, these.

Three lines here shown in gray.

But let's have a look at the tree search algorithm first.

So we define a function A\* tree search that takes two parameters.

The first is our search problem, and the second is the heuristic that we're using This may or may not be an admissible heuristic.

The first thing we do then, is initialize our set of fringe nodes.

And we initialize the fringe, from our initial state that was given in the problem.

And what we do is, we put this initial state.

Into a queue, into a priority queue that sorts the node by their f value.

That means we always have quick access to the node in that queue that has the lowest f value and that is how we initialize our fringe.

Then we go through the loop that does the note expansion and generates the search tree.

The first thing we do is we test whether the fringe is empty.

That would mean that there would be no more nodes to explore and that means we can return failure because it means we have generated the whole search tree and haven't come across a solution node, so there is no solution node.

So, we can return with failure.

[UNKNOWN].

But if there are nodes on the fringe, then we have to select one of these nodes, for expansion.

Note, that the select function here, does not take a second argument, as it did earlier, because our strategy here is fixed.

We select the node with the lowest f value.

Having said that I should point out that this selection is not quite deterministic as there may be many nodes with the same f value that are currently on the fringe.

So our node is only 1 of the nodes with the lowest f value.

And what we do next, is we perform our goal test.

The goal test on the state contained in that node.

And if this goal test succeeds, that means we have found a solution, and then we can simply return, the path, to this node in the search tree.

If our chosen node is not a goal node, then we have to add it's successors to the fringe and I've shown this here as a loop because we add each successor in turn, and we use the function expand to generate all the successors.

Of the selected node and our loop variable successor here is the next successor in that loop and then for the tree search version of this algorithm, we simply have to add that new successor to the fringe.

So, we extend our current fringe with the new successor and the place where we queue this new successor in our priority queue is of course the f value of that node.

And for those of you who have been wondering where we use the parameter h, that is the second parameter of this function, remember that f is defines as g + h, so that's where we use our heuristic function, and that is all the pseudocode we need for the tree search version of this algorithm.

Algorithm.

But the graph search version isn't much more complicated.

What we need to do in addition is first, we need to initialize the set of all nodes that we have seen so far.

And one way to do this is to initialize this as a hash table.

of nodes.

And intially this will contain only the inital state, that we can take from the fringe or from the problem.

Then the algorithm proceeds as before, going through the loop, trying to take a node off the fringe if there is one.

Performing the goal test on that node.

If it's a goal node then we've found the solution.

And if not we go to the loop where we expand the current node and generate it's successors, and that's where the algorithm for graph search differs.

Namely for each successor that we're generating we have to test whether this is a node that we've already seen and if we have already seen that node then we don't need to add it to our fringe, if we haven't seen the node then we continue as before we add it to the fringe as our new successor at its current f value and we also have to add it to the set of all the nodes we've seen before.

So that we don't explore this note again.

So this is a working version of the graph search algorithm.

However, there is one problem with it.

If the given heuristic is admissable, the graph search version, as shown here, no longer guarantees an optimal solution.

And the reason for this occurs here.

Namely if we generate the successors of a node, we may generate a new node that we have seen before.

But the second time we generate the node, we actually discover a shorter path than the first time we generated it.

So what we really want to do here is keep the node that has the shorter path and doesn't matter whether we discovered that first or later.

So what we could do is simply add a piece of code here that tests whether our new path is shorter than the old path.

But for many problems the heuristic that we'll be using is not admissible, and then A\* doesn't guarantee as an optimal path anyway.

So, we don't have to worry about this shortcut question necessarily.

And here is the first problem with A\* as it stands, and that is it still uses exponential space unless the euristics are perfect.

In fact, in the worst case, the time and space complexity of A\*, are both an O(b^l).

Where B is the branching factor, so that's the number of successes we have per node on average.

An L is the length of the path we're looking for.

b^l is exponential, which means we have exponential time and space complexity, And in reality, we will see that this is true most of the time the fringe grows exponentially.

Now let's look at this.

What does this really mean?

So exponential time complexity.

That means when the problem grows by one step, the size of the problem, the time it takes to solve this problem, doubles.

Or in general, it grows by a factor of B.

This may or may not be acceptable, depending on how much time you actually have.

When it grows into the region of years and centuries, this will no no, longer be acceptable, of course.

But quite often we have another hour we can give our.

But what about exponential space comlexity?

Well, what this means is that a star will eventually exhaust the computer's memory.

So first, it will eat up all the RAM, then it'll eat up all the virtual memory.

And eventually, it'll just have no more memory left.

And what happens then?

Well A\* simply fails.

The algorithm terminates because it's out of memory, and we don't even know whether there is a solution or not.

As an example, the 8 puzzle will still fit into memory, but the bigger version, on the 4 by 4 grid, the 15 puzzle, usually does not fit into memory.

###b A* Graph Search

The fact that the 8 puzzle, the search space usually fits into memory means we can do some interesting analysis on this.

So here's the example we've seen earlier.

This is the same initial state we've seen before.

What we have for this initial state is that we can move in all 4 directions and so we have 4 successors for this state and they are shown here as the 4 successors of this node.

Note that all the arrows here are bi-directional because, in this search problem, as in many others, actions are reversible, which means I can always go back and forth between 1 state and 1 of its successors.

So there's already a little optimization potential for our search algorithm.

We can avoid reversible actions.

So, looking at the successes we have 3 possible actions here.

But of course, 1 of them would us get back to the initial state.

So, in fact, there's only 2 successes that we would generate here.

For this, again, we have 3 possible actions here.

3 possible actions.

And there, 3 possible actions.

Which means, for each of those.

Successors we generate 2 more successors as shown here.

So we look at 1 example here where we have moved the empty tile up and this gives us those 2 successors and in this case there are 2 actions here and 2 actions there which means for each of those as only 1.

Possible, real successor, but again note, it's a by directional arrow, so, we can go back and forth between these states.

And so on I can follow this path, and generate more successors.

You can see there, I've ignored some, that I didn't generate, or didn't look at in detail, and so on.

Here is where I'm simply running out of slide, otherwise I would have gone a little further.

And then you would see what I'm effectively doing here, is the moves I'm following are simply shifting the empty tile in this circle.

And, what this means is eventually, this path will lead up here again.

We're back at our initial space.

So, this shows that as a graph, and it's a graph that contains quite some long loops.

And as I said before we can actually create the whole search space and then analyze how deep the different solutions are.

And this is what this graph shows us.

We have here the depth of the different solution nodes, so what we did is we started a generation from the solution node and generated all the different nodes that are availible in the search space and these are the depth at which they are from the solutions.

And here we've the number of different states that are at that depth.

Altogether there's about 180,000 different states in the search space.

And we can see that most of the states are actually at around depth, what is this, 24.

So, as I have said before the number of states we generate in both tree and graph search grows exponentially.

So what we can expect is that this will initially more or less follow the states base here that we generate the No's as they are available.

But then, then for tree search this will continue to grow exponentially so this goes up here somewhere, and out of the slide.

But then we see for the worst case here, at depth 31, there's really only two different states.

And if we wanted to use tree search to search for those states, we would have to generate all those states up here where, in fact, there are only 2.

So what this would lead us to expect is that tree search is reasonably efficient up to about this depth here and then becomes incredibly inefficient and we should use graph search for these worst case scenarios.

But then graph search also has some additional costs.

We need to maintain that hash table, as we've seen.

And comparing states in the hash table, well depending on the complexity of these states, this can take up quite some time too.

So there's a basic trade off here.

Another problem is related to permutations of solutions.

By that I mean, if we are given one solution part from the initaial state to a goal node, if we can swap some of the states and some of the actions on that path to obtain a different solution path, then this is a permutations of that solution, and this creates problems for that star.

Often permutations are allowed if we have independent actions in our domain.

So if for all states, if we first apply one action and then another, we get one state and that is the same state as applying the actions in the reverse order, then we say these 2 actions are independent.

We could, in principle, execute those 2 actions in parallel, but isnce we are only looking at action sequences so far, we have to choose an order.

And since the order doesn't matter, we have a permutation in our solution here.

The state that is the result of applying these two actions, in which ever order we choose them is the same.

Now why is this a problem for a star?

And here is the worst case scenario.

This is unrealistic but it is the worst case.

Suppose we are given a the search problem where we have an intiatial state and we found a path to our goal state and they're many intermediate notes here.

And our optimal path has a lenth of n, and the worst case is that All these n actions are independent.

It is easy to show that if all the pairs of actions are pair wise independent, then we can basically permute those actions into any order we like, and it doesn't change the outcome of the plan.

Now remember what I said earlier about contours, namely that a star, if the optimal solution length is n in this example, will need to generate all the nodes within a smaller contour.

So, if the F value is n-1 then this is the contour that a star will generate with the FLU being equal to n-1.

What this means is that it will generate all those permutations that have n-1 nodes.

So what this means is in this part there's n-1 factorial, different combinations of solutions.

And, we'll explore all these paths before it moves on to the first solution.

Of course, then it will find n factorial solutions for this problem which are a lot of solutions.

And, you should now understand that there is no way around this, of course.

So you can now go and experiment with this by programming all this in your language of choice.

Specifically, why don't you try to generate this graph here that tells us how many different states exactly there are at the different depths.

So for example, how many states would you find here at depth 27?

That is a question.

##Good Heuristics

Before we move on to the planning algorithms, I want to tell you a little bit more about heuristics and specifically, what makes a good heuristic and how do we find these.

We have defined a heuristic in a technical sense, as a function that estimates the distance to the nearest goal node.

But a heuristic obviously has colloquial meaning as well and that's what's defined here.

So it's, it's heuristics are criteria, methods, or principles for deciding which among several alternative courses of action promises to be the most effective.

So the alternatives that we're looking at are of course the successor nodes that we want to evaluate.

We want to see which one of those is the most promising.

And what we need to do is we need to decide which one to follow first.

Of course we keep in mind the others and follow those later.

We use heuristics in everyday life.

For example, here you see a heuristic for deciding whether a pineapple is ripe.

If you ever go into a shop and want to buy a ripe pineapple, this may work for you, or it may not.

So, if you can rip out the inner leaves easily and the fruit smells like a pineapple should smell, then you're looking at a ripe pineapple and this is one you can buy, assuming the price is right.

If there are no pineapples where you are, tough luck.

The reason why I've circled the word, deciding, here, is because this gives us a different idea of what heuristic can be.

All we need from a heuristic is just a decision.

Which alternative looks best.

So it doesn't really need to be related to the distance, to the goal, at all.

All we need is a function that decides which one is the best node.

If we have that function, that would constitute a perfect heuristic because it would tell us which successor to follow.

And which path we should explore first.

If that deciding.

Is correct.

Then we have a perfect heuristic.

Another example of a heuristic you've applied probably not too long ago, is in choosing this course.

You looked at the introductory material to this course.

And used this as information to decide which of the courses that were available you want to take.

So you used heuristic information about the course to make this choice.

Okay, assuming you understand what a heuristic is, now the question is when is a heuristic a good heuristic?

And if we have a given search problem and a given heuristic we can evaluate that heuristic by looking at the number of states that are generated for a specific problem with that heuristic.

And if we have two heuristics, we could compare them by using the number of states they generate to see which is better.

The better heuristic would generate fewer states.

But that is only heuristic for deciding which heuristic is better.

because what we are really after is, we want solutions as fast as possible, so we have time constraints to respect.

And computing a good heuristic also takes time.

Unfortunately, this means we are dealing with a trade-off here.

And this is the trade-off.

Some heuristics are simple.

So they provide a simple way of discriminating between the successes we generate.

And since we opt-, want to optimize the time we take to research.

Simplicity here means easy to compute.

We want to have a fast way to compute the heuristic value for a given state.

But heuristics that are simple to compute are often not accurate.

And accurate is the other property in our trade-off here.

A heuristic, unless it is perfect, does not provide a guarantee that it tells us which the best successor is to explore next.

So, there's no guarantee that it identifies the best course of action.

But if it's a good heuristic, it will do this more often than a heuristic that is not as good.

So, a good heuristic does this sufficiently often.

It's accurate.

It tells us which is the best course of action, or in the technical sense, it tells us how far the goal state really is.

So now we know what a good heuristic is the important question than is how can we find good heuristics for a given problem?

And this is somewhat similar to the problem of problem formulation it's a matter level problem.

We have to find good heuristic to do good search.

Just as a good problem formulation will ease search.

That means this is a very important question.

And the answer is, there are methods for doing this and we will look at some general methods next.

But then there is a different question that is just as important.

If we have a method, can we automate this method?

And the answer again is yes, but that's a very complex process and we will get to that later in this course.

In fact, automatically finding good euristics has probably been one of the most hot topics in the AI planning research over the last ten, fifteen years.

So here's a general method for finding good heuristic.

The idea is based on a simplified problem or a relaxed problem.

Usually a problem is defined in terms of states, and actions that are applicable in states, and achieves certain things in, successor states.

So what we have is restrictions on these actions when they are applicable, when we can use them, and which states they are useful.

What we can do is relax those restrictions.

So we can look at the restrictions defined in the original problem, and drop some of them or make them less hard.

And that gives us a new problem, which is the relax problem.

And then the following should be fairly obvious to see.

Namely that the cost of an optimal solution for a relaxed problem is an admissible heuristic for the original problem.

In fact it's admissible and consistent, but since we haven't defined consistent yet, I won't go into that.

So you just, you should see why it is admissible.

It's very simple to see because the optimal solution for our original problem is of course also a solution for our relaxed problem.

We've only relaxed the restrictions on the actions.

So, an optimal solution for the relaxed problem can have, at most, as many steps as the optimal solution for our original problem, because that is a solution for the relaxed problem.

In general, what we have is that in our relaxed problem, we allow shortcuts to be taken with these relaxed actions that are not possible in our original problem, so if we take out these shortcuts, we end up with longer solutions.

And since this method is quite abstract I want to illustrate this with an example and we will use the 8 puzzle that we've seen before and the actions that are defined for this 8 puzzle.

So here is the original condition that we had for the applicability of actions.

Namely a tile can move from square a to square b if a is horizontally or vertically adjacent to b and b is blank.

So the condition we have here is a conjunction of two sub-conditions and that should tell us how we can build a relaxed condition quite easily.

Namely by dropping one of the two parts or both of them.

And this is how this works.

If we drop the second part.

Then B is blank.

We end up with this heuristic here.

And that tells us that a tile can move from square A to B if.

A is adjacent to B.

I've dropped the horizontally or vertically.

And what we get there, of course then, is the Manhatten block distance heuristic.

Because we now allow a tile to be moved, no matter where it is moving to, which gives us exactly the block distance for this tile.

And if we add all those up, that's the Manhatten block distance we've seen.

The second one is, if we drop the first part of this definition, so that the adjacency distance is dropped, then we end up with a heuristic.

That's a tile can move from A to B, if B is blank.

And finally we can have a heuristic if we drop both conditions that says a tile can move from a to b and there are no conditions.

And of course this then gives us the misplaced tiles heuristic.

We simply count those tiles that can move to where they need to be in one step because there's no conditions on how they can move.

So what you see here is we've derived two of the heuristics that we've already used for the 8 puzzle using the method we've just introduced by using relaxed conditions of the actions that are applicable in our problem.

So this concludes this section of the course on AI search technology and the A\* algorithm.

You should understand now that a heuristic function encodes problem-specific knowledge in a problem-independent way by mapping a state to a real number.

This information about search states can be used to make the search more efficient.

In general, this is done by using an evaluation function that tells us how good a search node is.

Greedy best first search simply uses the heuristic function as the evaluation function but the better solution is provided by the A\* algorithm.

The evaluation function used by A\* is simply the sum of the heuristic function for a node plus the cost of getting to that node in the first place.

We've also seen that A\* is optimal.

It will always find an optimal solution.

And it is optimally efficient.

It does not expand more nodes than absolutely necessary.

But I've also shown you that A\* is not the answer to all questions, specifically when it comes to graph surge.

Finally, since good heuristics are so important for A\*, I've also talked a little bit about what good heuristics are and how to find them.

So now a big tick because you understand all of that.

##Structured States

###a Structured States

In previous segments, we have looked at a general technique used in Artificial Intelligence to solve problems, namely search.

We have seen how the efficiency of search can be greatly increased through the use of heuristics.

In this segment, we will look at one of the early systems developed in AI for planning, that is the Strips planner.

One of the advances made by the Strips planner, was the representation used by it.

The Strips representation still under lies much of the research done in AI planning today, and the first three parts of the segment, will be concerned with the representation.

Once we understand the representation, we can see how search algorithms as we've seen them previously, can be used to solve planning problems, and that is the remainder of this segment.

The first thing we will look at now, is how states were represented in the Strips representation.

The way we have looked at search so far is we have treated states as black boxes.

That is we have very limited access to the internal structure of a state or an action.

Of course, there are some things we can do with states.

For example, we can see whether it is a goal state or not, and this is necessary to terminate the search.

We can also find out what the applicable actions are, and this is related to the successor states.

Remember, we have defined the successor function to return an empty set if there are no applicable actions.

So, these two are related to each other, and give us information about what can be achieved from this state.

More simple functions about states would be an equality test and a hash function.

We need those two to turn tree search into graph search.

And finally, we've seen how a heuristic estimate gives us some information about a state that we can use to guide the search algorithm.

But we had no other access functions to look into the states and actions, so they are black boxes.

But our aim in planning is to reason about these states and actions and that is very difficult if we don't have access to the internal structure.

Just think about what we have learned at the end of the last segment, mainly that it is possible to, to automatically derive good heuristics but this is only possible if we know something about the states and actions in our search problem, so we need to know the internal search structure.

The strips representation gives us a standardized ways of looking at the internals of states, and actions.

Now this is not as pretty as it here shown in the picture, but there will be a formal language that we will define, that describes what is going on in the state and what actions do to the state.

And that is the fundamentals of the strips representation we will look at now.

The strips representation is based on first order predicate logic.

This means we have objects in our domain and these objects are somehow related to each other.

Lets look at our dock worker robot example.

In the dock worker robot domain we have several types of objects that we are looking at.

The first type we are looking at are the robots.

We will introduce symbols like these, robot one and robot two to refer to these robots in our logic.

So we can use this symbol to denote the robot with the number one and the robot with the number two and so on.

And we can have an arbitrary number of symbols denoting robots.

They are of the type robot, that means they all represent a robot.

Robots are of course, the objects that carry a container from one location to another adjacent location.

The next type of objects we have are cranes.

And again, we have symbols to denote the different cranes that are in our domain.

Cranes can be used to stack or unstack containers from a pile.

Or load or unload containers onto a robot.

As opposed to robots, cranes have a fixed location, so they cannot move.

And then we have of course containers.

Containers are denoted by these symbols here, these can be arbitrary symbols of course and they are the things we are moving around in our domain.

Containers can be in some pile or they can be loaded onto a robot or they can be held by a crane.

Robots, cranes, and containers are physical objects that means they're tangible and we can see these objects in the real world.

The next two types, do not have that property.

The first one is locations, which denote places that we have in our world, again we introduce symbols, to represent these different places.

These could be a storage area, a dock, a ship, or some parking or passing area in our domain of interest.

And then we have piles of containers again we have symbols for these, pile one, pile two here.

And a pile can be empty in which case there's no mass to it, it's not a tangible object or it contains some containers in which case we can actually see the pile and the containers that belong to it.

Each pile is attached to a single location so we cannot move the piles around.

And at the bottom of each pile we have a pallet.

This is denoted by the symbol pallet and despite the fact that we have several piles and therefore several pallets at the bottom of these piles we only need one symbol to denote all these pallets as we don't need to distinguish the different pallets.

So, in a state we have objects and these objects are grouped into types as we've seen here.

And each individual object is denoted by a unique symbol in our domain.

Once we have identified the objects and types that exist in a domain, we need to specify them in a way that the planner can take as input.

Which means we have to write a text file that defines these objects and types.

In this file we need to adhere to a specific syntax, such that the planet will understand what we're trying to say to it.

The syntax understood by most planners these days is the PDDL syntax.

And PDDL stands for Planning Domain Definition Language.

Here is an example of how we can specify a domain in PDDL that defines the types we have just seen in the previous slide.

The first thing we do is we tell the system that we are trying to define something.

And what we are trying to define is a domain.

And this domain has a name.

This is an arbitrary name, and does not really matter at this stage.

But the keywords define on domain are always there.

Next, PDDL allows us to specify requirements for a specific domain.

In this case we specify that our domain requires strips and typing.

Strips is actually always assumed and typing is a very simple extension, but there are many other extensions that you can use.

The typing extension is, of course, required for the types, we will see next.

The types are introduced by the keyword, types, I've seen here.

And then we simply name the different types, the symbols that identify them.

Location, pile, robot, crane, container.

These are the types we've seen in the previous slide.

Pddl also allows you to insert comments into your input file.

And these are introduced by the semicolon symbol.

Everything from the semicolon to the end of the line is considered a comment, and is therefore not part of the formal language.

It will simply be ignored by the planning system.

Of course, there is a lot more to planning domains than just the types and we will look at that next.

###b Structured States

The strips representation is based on the idea that in a world state, we have objects that are somehow related to each other.

That means we need to define some relations that we can use to relate the objects to each other.

Relations are also known as predicates.

For a given world state, a relation between several objects can either hold or not hold.

That is, it is either true or false.

Here are the relation defined for the dock worker robot domain in the PDDL syntax.

The first predicate mentioned here is the adjacent predicate.

Which defines when two locations are next to each other.

So the way this works is that we name the relation that we want to define.

And then we give the parameters to this relation.

In this case, they are two locations.

They are denoted by variables here.

You can recognize a variable in this syntax because it starts with a question mark.

So this is not a specific location that we've defined, but just a variable.

And this is followed by the minus symbol with the type of those variables.

So what this is defining is that adjacent is a relation that is defined over two locations.

L1 and L2.

And again, there's a comment here that explains this more readable.

Location L1 is adjacent to L2.

Then we have a relation attached, which defines where a pile, P, is.

Namely at some location, L.

And the relation, belong, which defines where the crane is also at some other location.

Note that these variables, despite having the same name, have no relation to each other.

They're just used locally in this definition of this predicate.

Together, these three relations define the topology of the domain.

One thing that is special about these relations is that they are static.

That is, once they are defined for a specific state, they will never change from state to state.

Locations are adjacent, they will always be adjacent.

There's no action that can change that.

The other relations here are known as dynamic or fluid relations, because they can change from state to state.

For example, the at relation tells us where a robot is currently, it tells us at which location this robot is currently located.

Then we have an occupied location, which is a unary relation, it takes only one argument, and tells us whether a given location is occupied by a robot at that location.

Then we have the loaded relation which tells us whether there is a container loaded onto a robot, or which container is loaded onto a robot.

An unloaded is true if robot is not loaded with a container.

Then we have a holding relation, which tells us which container is held by a crane.

And we have an empty relation, which is true, if and only if the crane is not holding a container.

You can see here that these relations are not independent of each other.

For example, holding and empty are closely related.

If the crane is empty, it cannot hold the container.

And vice versa, if it is holding a container, it cannot be empty.

The same goes for loaded and unloaded, and at and unoccupied.

These two relations depend on each other.

Okay.

Remaining relations.

We have an IN relation, which tells us where a container is, in which pile a container is currently located.

Then we have a TOP relation, which tells us which container is at the top of a given pile.

And, finally, we have an ON relation, which tells us which container's on which other container.

The second container here can also be the pallet, which is technically defined as a type of container.

So these are the predicates used in the dock worker robot domain, defined in the PDDL syntax.

Now, here is a formal definition of what a state in the strips representation actually is.

So we start off with a first order language, L.

And in this language, we have finitely many predicate symbols.

These are the symbols we've just seen how to define them in PDDL.

We also have finitely many constant symbols.

These are the symbols representing the individual objects in our domain.

And we have no function symbols.

And we haven't defined any function symbols.

A state in a strips planning domain then is a set of ground atoms over this language L that we've just defined.

Let me go into this.

By an atom we mean a predicate with an appropriate number of objects that we have just defined in the previous slide.

An atom is considered ground when all its arguments all the objects it relates are real objects as opposed to variables so there must not be variables in the atoms we define here.

Then a state is simply a set of these ground atoms over this language L.

Logically, we interpret this as a cun junction that means each and everyone of the atoms in a state holds in that state, it must be true in that state.

So we can say that a ground atom p holds in a state s, if and only if p is an element of this set s.

So a state is a set of ground atoms.

And if p is an element of that set.

Then we say that p holds in that state.

And vice versa, if p is not an element of that set.

We say that p does not hold in that state.

We call that the closed world assumption.

And this is quite an important assumption made by the strips representation, as we define it here.

We can also define what it means for a state to satisfy a set of ground literals, G.

I need to explain this.

A literal is basically an atom that is either a positive or a negative.

So it can be an atom or a negated atom.

G is our set of ground literals, and I've used the letter G because eventually this is how we will represent goals in our planning problems.

And this is simply the symbol we use to denote that S satisfies G.

And we consider this the case if every positive literal in G is also in S.

So every positive literal in G holds in S, and every negative literal in G does not hold in S.

So it's not an element of S.

Let's illustrate this with an example.

Depicted here is a state in the Dock Walker robot domain.

I hope you all intuitively understand what's going on in this picture.

But for our planner we need to translate this state into our formal representation in the strips representation.

This means we need to define the set of ground atoms that hold in this state.

We can do this by going through the relations individually or in some other order.

The first relation we defined earlier is the adjacent relation and that tells us which locations are reachable from which other locations.

So we have two locations here, location one and location two.

And they are connected by this path.

So what we have is that location one is adjacent to location two, and vice versa.

And that's what we've got here.

We need to actually write down both ways of this relation.

The symmetry of this relation is of course not known to the planner automatically.

Then we have two piles, p1 and p2.

And they are both at location one.

And that's what we express with these two ground atoms here.

That p1 is at location one.

And p2 is also at location one.

And then we have one crane, denoted by the symbol, crane one.

And again, we write down that the crane one belongs to location one.

So this defines the topology of the domain.

What we also have in this example is this robot, R1.

And we can see here that the robot is at location two, which is this relation here.

And that means that location two is occupied by this robot.

And we can also see that the robot is unloaded, which means there's no container on the robot.

Also the crane isn't holding a container, so empty crane one must also hold in this state.

Now, what remains to be described is the two stacks of containers we see here.

The first stack consist of this container C1 at the bottom, and C3 at the top.

And we have got here that these two containers, C1 and C3 are both in the pile P1.

Then we have C3 on top of C1, you can see that here, and C1 is on top of the pallet.

The top of the pile is the container C3 which defines this pile at location one and that is pile one.

The second pile only contains one container C2, and that container's in the pile P2.

And it is immediately on the pallet and also constitutes the top of this pile.

These are all the ground atoms that hold in the state depicted here.

And what this means is, according to the closed world assumption, that all the atoms that are not mentioned in the state do not hold.

They are not true.

For example the ground atom at our one location one, does not hold, because at the location one there is not the robot R-1.

So this is false.

It is not part of our state, it is not in that set, and the closed world assumption means it is false.

So this is what a state looks like in the strips representation.

It is subset of ground atoms.

##Structured Operators

###a Structured Operators

Now, we have seen what the strips representation for world states looks like.

The internal structure is represented as a set of ground atoms.

Where each atom expresses a relation that must hold between objects in the domain.

The next step, then, is to look at operators with internal structure.

And actions that are instances of these operators.

Remember that actions are what give us the state transitions in our state transition system.

For states I started with some examples and some informal definitions and followed this with the formal dentition of what a strips representation state is.

For operators I'll do it the other way around.

I will start with the formal definition of what a strips operator is.

A planning operator in the strips representation is simply a triple, so means it consists of three different things.

A name, the name of the operator, the preconditions of the operator, and the effects of the operator.

The name of the operator, the name that we can use to refer to this operator, is simply given by this syntactic expression.

So it consists of a unique name symbol that we can use to refer to this operator.

It must be unique to this operator, meaning no other operator can have the same symbol.

And it consists of some parameters which characterize the objects that are manipulated by this operator.

These objects are defined as variables for an operator as an operator is generic.

They may also be typed if we are using the types extension in PDDL.

The components that define the internal structure of an operator are the preconditions, and the effects of the operator.

And both of these are sets of literals.

Remember that a literal is either a positive or a negative atom.

So, a relation between objects.

Where the objects are named by the variable given in the operator name.

X1 through xk here.

The preconditions are simply those literals that we want to be satisfied in the state before we execute an action.

The effects are the things that are true after we've executed an action.

Again, the effects can be literals, which means they can be positive or negative, and traditionally the positive literals that are effects are often referred to as the ad list, as this is something that is asserted in the new state after an action is applied, and the negative literals are referred to as the delete list as these are removed from the state.

An operator is generic in the sense that it only specifies what is manipulated by the operator, as variables.

An action, on the other hand, is specific because there's a ground instance of a planning operator.

So if we take all the variables and choose values, objects in our domain, for these variables, then our operator becomes an action.

And of course, there can be many actions that are instances of the same operator in our planning domain.

Now, here are some examples of operators defined for the dock worker robot domain.

The first example we will look at is the move operator.

Move is the symbol that identifies this operator, and the name of the operator is given by all of this.

This expression here.

Namely, the symbol that identifies it plus the three variables that are used to identify its parameters.

And these are a robot and two locations.

The location we are moving from and the, the location we are moving to.

Then, the internal structure is defined by preconditions and effects.

There are two positive pre-conditions in this example.

Namely that the two locations when moving between must be adjacent.

The location when moving from must be adjacent to the location we are moving to.

And the robot R, the robot that we are moving must be at the location we are starting from.

And then we have one negative precondition and that is given here.

This is simply the symbol for negation, that's the NOT symbol in logic.

the negative precondition here says the location m we are moving to must not be occupied by a robot when we move there.

So three preconditions.

Two positive one negative.

And then we have the effects.

Again two positive effects, and two negative effects here.

Let's look at the positive effects first.

When we move robot R from location L to location M as a result the robot will be at location M.

And the location M will be occupied as a result of this action.

These are the effects that are added by this operator.

Hence, this is known as the add list.

the delete list, or the negative effects of this operator, are that the location L is no longer occupied.

We've just moved the robot away from this location.

And the robot is no longer at this location, L, that we've just moved away from.

There are a few things that are quite common about this operator.

But they are not part of the definition of a strips operator.

And those are, all the parameters are actually mentioned in the preconditions.

R, l, and m.

All of those are mentioned in these three conditions.

Also very common is that we have the negative effects of the operator, the delete list, as part of the preconditions, only negated.

For the second negative effect, this is explicit here.

And for the first one, it is implicit.

Of course, the location l was occupied before the robot moved away from that.

But because we have this precondition, it was not necessary to mention that.

Quickly, the two other examples.

The second one is that we're loading.

With crane K at location L.

We're loading container C onto robot R.

These are the four arguments to this operator.

And all this is the name of the operator.

Again, we have preconditions and effects.

And they are divided into positive and negative preconditions and effects.

In fact, we have no negative preconditions for this operator.

So the positive preconditions are that the crane must belong to that location.

It must be at that location.

the crane must be holding the container.

So it can load it onto the robot, which must also be at that location.

And must be un-.

Loaded so it cannot have a container on it.

The effects are.

That now the crane will be empty after we've loaded the container onto the robot.

So it will no longer hold the container, but the container will be loaded onto the robot, and the robot will no longer be unloaded.

Similarly there's the put action which has as parameters a crane, a location, a container, another container and a pile into which we're putting the container C.

The crane must be at that location and also the pile must be at that location and of course we want the crane to hold to the container.

And this precondition here tells us the meaning of this second container D that is a parameter that to put action, namely it is the container that is on top of the pile before we execute our action.

And the reason why we need to know this is simply such that we can withdraw this from our state.

So the container D is, after we've executed the action, no longer on top of the pile because we have the container C on top of D and now C is the new top of the pile.

So C will also be in our pile, the crane will be empty, and the crane will no longer hold container C.

there's also an unload and a take action that reverse the load and the put action but I won't go through those in detail here.

In the previous slide, we have seen some examples of operators, of what they look like logically.

Now I will introduce the PDDL syntax, for defining an operator in a planning domain.

This example, again, starts with a comment, introduced by the semicolon, that tells us what this action will actually do.

And here is the first slightly confusing thing about the PDDL definition.

Namely, the operator is called an action in PDDL.

So, in the strips representation, we consider an action to be an instance of an operator.

In PDDL, this is what is called an action.

This is actually not uncommon in the planning literature, that terminology is not agreed.

So you always have to make sure that you understand what authors mean by the different terms like action or action type or operator, et cetera.

So in PDDL, this action has the unique name, move, that can be used to identify this, action, this operator.

And it has several parameters that are defined here.

And there is one parameter which is this one, a variable.

Remember, variables start with the question mark R, which is of type robot.

And then it has two more variables, both of the type location.

This is what we call the name of the operator.

In the definition of a strips planning operator two slides ago.

Next we have the preconditions and effects.

So the preconditions are introduced by the preconditions symbol, and then are just listed.

It is also exclusively stated in PDDL that this is a conjunction.

Of pre-conditions.

So they all must hold.

And that's what we introduce with the an symbol.

PDDL also allows other pre-conditions, dis-disjunctive pre-conditions.

But we will not look at those here.

So what we have is that, the first pre-condition is that two locations must be adjacent.

The robot must be at the first of those two locations we're starting from.

And this location must not be occupied.

Slightly different syntax, but exactly the three pre-conditions we've seen in the previous slide.

Similarly for the effects, they're listed here.

They are a conjunction of effects.

And it's the same four effects we've seem previously.

Namely that the robot must be at the two location, as a result of the action.

And this location will be occupied.

The robot is no longer at the from location, which is not, no longer occupied.

You should now be able to express the other two operators introduced in the previous slide, in the PDDL presentation.

So why don't you do that as a little exercise here.

###b Structured Operators

What we still need to formally define is when an action is applicable in a state, and what the corresponding state transition is.

To do this I will first introduce a little notation.

Suppose we're given a set L of literals.

So, these are positive and negative atoms.

Then I can use this here, this symbol L plus to refer to those atoms that are positive in L and I can use L L minus to refer to those atoms whose negations are in L.

So L plus and L minus are sets of atoms namely those that are positive and negative in L respectively.

Now, let A be an action, and S be a state.

And when I say action here, I mean action, a ground instance of an operator.

Then we can say that the action A is applicable in the state S, if two conditions hold.

Namely, all the positive preconditions of A, preconditions plus, of A, must be a subset of the state, S.

And we also want none of the negative preconditions of A to be in S and this can be expressed by this expression.

The intersection of the negative preconditions and what is in the state S must be empty.

In other words there can not be a negative precondition that is also a true in the state.

A is applicable in S if all the positive preconditions hold in S and none of the negative preconditions hold in S.

And now we can define the state transition function gamma for a given state.

That's the state in which we are initially.

And the action A that we are applying in this state.

And this can be defined as follows.

We start off with the state, and we deduct all the negative effects.

As I told you earlier, this is the delete list.

So we delete this from the state.

And then we add all the positive effects so we add them to the state.

And again, I've already explained.

This is the add list.

So we start off with a set of ground atoms.

We remove some ground atoms and we add some ground atoms.

And this is the state that we get as a result of applying the state transition function, in state as using action A.

One minor point that is of quite a lot of importance and some detail, is that we first delete the negative effects and then add the positive effects.

That means, if there is an atom that is a negative effect and a positive effect.

It will be in the resulting state because it is first removed and then added again.

You may wonder, why would you ever want to have an action that has the same atom as a negative and positive effect.

Well, maybe have look at the move action that we've just introduced.

And see whether this would ever be possible that an atom can occur as a negative and a positive effect depending on what values I choose for the variables.

Finding the actions that are applicable in a given state is actually a non trivial problem.

You can see this quite easily when you think about the number of possible actions that may be applicable in a given state.

Namely, all those operator instances where you replace every parameter by every possible value.

That would give you an exponential number of actions.

In general there are way fewer actions applicable in this state, though.

And here is an algorithm that you can use to find the applicable instances of a given operator in a given state.

This function, add applicables, takes five parameters.

The first is A, the set of actions and this is initially empty.

This is just the set in which we collect the result of this functions for the applicable actions.

Then we give it an operator of which we seek instances that are applicable in our state.

We have a set of remaining preconditions of this operator so these are the preconditions we still need to look at.

Initially these are all the preconditions of the operator.

Then we have a substitution for the variables that are parameters of the operator.

Initially no variables are replaced by values.

And then, of course, we have the state in which we are trying to apply this operator.

So, the algorithm will first deal with all the positive pre-conditions.

So at first, tests whether there are positive pre-conditions left.

And if there are none left, it goes to the first part here.

And if there are positive pre-conditions left, it will go to the second part here, the second part of this if statement.

And the first thing we do is select the next positive pre-condition that we are trying to apply to restrict the actions that we can apply in this state.

So PP is the positive precondition that we look at next, and then we go through all the propositions in our state here as P, and try to match this against PB.

Of course, if the predicate doesn't match it can never be a match and therefore we can ignore these.

We only look, need to look at those propositions that have the same predicate.

And then we extend the current substitution that we were given as our parameter such that the two SP and PP match.

This gives us a new substitution and if this substitution is valid it means we were able to extend the substitution.

Then we can do a recursive call and the recursive call goes as follows: We dont change the set of actions in this case.

We simply use the same operator again.

We remove the precondition from the set of preconditions that we've just dealt with.

So PP is the one we've just exploited, we no longer need to look at that.

We've changed the substitution by adding more, values to variables.

And the state remains the same.

We go through this loop until there are no more positive preconditions, and then we end up in the first case of the if statement.

So now we've dealt with all the positive preconditions.

We now look at all the negative preconditions.

So we loop over NP where NP is a negative precondition and all we have to do is now check whether our state falsifies this negative precondition.

If this is the case then this cannot result in an applicable action.

But if the state does not falsify the negative preconditions then we have found an instance of this operator up that we were given that is an applicable action in this state.

And that, what we have to do is simply replace all the parameters of this operator according to the substitution that we've been building up.

Here is an example that should illustrate this algorithm.

In this example, we are given our state over here.

That's the set of ground atoms.

And we have our PDDL action defined here.

That is the move action we've seen previously.

So these are the two parameters we've seen in our algorithm.

that's the operator is the move action, and s is the state we were just given.

The other parameters are an empty set.

The pre-conditions of the move operator.

And an empty substitution.

The algorithm then starts by testing whether there are positive preconditions left which is the case which means we immediately go to the L parts of the algorithm.

And the first thing there is we choose the next positive precondition we want to look at.

So let's go back to the example.

The first positive preconditions is adjacent from two and this is the first precondition we will pick here.

So that's PP, PP is this adjacency relation.

Looking back at the algorithm we see that the next step is to look for all those state propositions that can match this positive precondition.

So let's look at our state.

This is the adjacency relation and we have two adjacency ground atoms in our state, namely location one to location two and location two to location one.

So these are the two cases here I've listed for SP.

The loop will go through those two and succeed there.

The next step in the algorithm, then, is to extend the substitution, such that SP and PP match.

And we can see that this has happened here.

I'll go through the first case on the left here.

we've extended the substitution, such that the variable from, from our precondition must be location one and two, must be location two.

So that adjacent one or two matches what we have here in our substitution.

The algorithm then continues with the recursive call.

Because the substitution is still valid.

And it simply removes that positive precondition from the preconditions that we still need to do.

So again, we end up at the entry to the algorithm.

See that there are positive preconditions left.

End up in the else branch.

And select the next positive precondition that we need to deal with in the algorithm.

And as we can see here the next positive preconditions at R from so that's the next positive precondition we select and we need to match that against the state, let's look at the algorithm again.

So we're now here that we're going through a loop over all of the propositions in the state that match this positive precondition.

And if we look at the at relations of the state, there is only one option here.

So this loop is only one long.

And we choose this here.

So we try to extend the substitution such that it matches this state proposition.

And what we see here is that we need to match R to R1, and from to location two.

But in our substitution we have already set that from must be location one.

So it can't be location one and location two at the same time.

Which mean our substitution is not valid.

But if we go back to the algorithm.

We had a branch here previously in this loop that we haven't explored yet.

And that is listed here.

Namely, where SP was adjacent, location one to location two.

And again, what we do is extend our substitution such that the state proposition matches the positive pre-condition.

And we can do that here.

So we have that our new substitution maps from to location two, and two to location one.

Again, this is followed by the recursive call.

So we need to check whether there are more positive preconditions, just like in the other case.

And yes, there is another positive precondition.

The same one we dealt with in the other case, the at relation.

And again there is only one possible instance in the state that can match that.

And that's at r1, location two.

So, where are we now in our algorithm?

We are here.

We need to extend the substitution, such that the state proposition matches the positive precondition.

So let's go back to the example.

What we have here is we need to match R to R1.

And again we can do that.

And we match from to location two.

And again, previously we have matched from to location two.

So this is consistent with the way we need to match from now.

Which means we can actually, in this case, extend the substitution.

And have a new substitution in which case, from.

Is location two, two is location one.

And R is R1.

So again this would be followed by the recursive call here, and that means we reach the algorithm with a set of empty.

Positive preconditions.

There are only negative preconditions left.

Let's look at the example.

We've dealt with adjacent and we've dealt with at.

So, there are only negative preconditions we need to deal with.

And the negative preconditions, then, that's this part here of the algorithm.

We go through every negative precondition and check whether this is falsified in the state.

Instantiated with our substitution sigma.

So, what does this look like in the example?

So, there's only one negative precondition in our example.

Namely, that.

We want the location we're moving to not to be occupied.

And that is our MP now, which means it is not occupied location one.

In the example the variable we are instantiating is two and our substitution tells us that two must be location one.

So this is the instantiated negative precondition not occupied location one.

And as we can see in the state, the only location that is occupied is location two.

Which means, our negative precondition is not falsified here.

So we can turn this into an action, that we add to the set A.

That's the final step here.

So we add to A, the instance of this operator, instantiated using our substitution that we've defined here.

That was the last substitution that we had.

And we get the operator, move R1, from location two to location one.

And indeed this is the only applicable action here, and that means we are done with this algorithm.

##Domains and Problems

###a Domains and Problems

Now that we have seen how the internal structure of operators and states can be represented in the strips representation, it is time to turn to the bigger picture.

And by that I mean planning domains and planning problems.

Planning domains implicitly define the graph in which we are doing search.

Planning problems also define the states from which we are going and to which we want to go.

Plans are solutions to planning problems.

And we will now define all this formally.

Here's a quick overview of the concepts we need to define for classical planning.

Most of modern planning, can be achieved by simply extending this classical representation.

So, in classical planning.

The task is to find a solution for a planning problem.

So we need to define, what is a solution?

And what is a planning problem?

A solution, and a planning problem.

The planning problem consists essentially of three things.

And those are given to the planner to come up with solutions.

The first component of a planning problem is the initial state which is a set of atoms which relate the object in our planning problem to each other.

This is exactly the type of state we've just seen in the strip star presentation.

The next component is the planning domain and the planning domain consists, in essence, of operators which are described by names, preconditions, and effects.

And the final component of a planning problem is the goal.

Note that the planning domain is essentially a reusable component.

We will often see many planning problems that refer to the same domain, but have a different initial state and different goal.

But domain is reusable.

Finally when the planner has solved the planning problem, it will return a solution to that planning problem, which is a plan.

Here is how we can define a strips planning domain.

We start with a function free first oral language L, as we've seen before.

A strips planning domain over this language L, is then a restricted state transition system, consisting of these components S, A and gamma, as we've seen before.

You may wonder what happened to the set of events, that can happen in this state transition system and that's exactly what we mean by restricted here.

In the restricted state transition system, we do not have events.

We only have the states S, the actions A, and the state transition function gamma.

The set S of possible world states is then defined as the set of all possible strip's states.

So, all sets of ground atoms that you can possible define are an individual state.

The set of actions that we need is then the set of all ground instances of some strips planning operators.

And these are the operators that are defined in the planning domain.

And finally, we define the state transition function.

Which is, in this case, a deterministic function.

Because it maps to exactly one state.

Again, this is to do with the restricted state transition system.

So the state transition function takes a state s, and an action a.

And maps that to a new state.

Which is defined by taking all the ground atoms that are true in this state.

Removing the negative effects of the actions.

And adding the positive effects of the actions.

That is true for an action that is applicable in the state.

If the action is not applicable.

Then, gamma SA is undefined.

So, there is no resulting state if we try to apply an action that is not applicable.

This definition of a state transition function is, of course, what we've seen before, full of strips, actions, defined earlier.

Finally, the set S is closed under gamma, which means there are no states that cannot be reached through the stationary illusion function that are in our set S.

So, that defines a strip's planning domain, and I hope that none of this comes as a surprise to you, given what we've seen earlier.

Here is an example of a planning domain in the PDDL syntax.

The core of the representation, are of course the planning operators, and that's what we have here on the right.

These are the five actions defined, or the five operators, defined for the doc worker robot domain, and again, they're called actions in pdl, but these are operators.

As you can see, they are perimeterized and have variables whereas actions would be ground.

So, we have the move action, the load action, the unload action, the take action, and the put action.

That is the complete set of operators defined for the doc worker robot domain.

In addition to the operators, the PDDL language allows us to define a few other things.

And that's what we've got over here.

So we can, for example, say that a domain has a name.

So that we can refer to this domain, in planning problems, as we will see later, by name.

Then we can specify requirements, which allows the language to extensible.

We can specify types.

These are the five types we've seen earlier.

Locations, piles, robots, cranes, and containers.

Then, in this domain specification, we also have one constant defined, which is the pallet.

This is defined as part of the domain here, as it will occur in every planning problem.

And then we have the predicates, which where are exactly the predicates we've seen earlier defined for the dock worker robot domain.

You can see the complete set here defined with the types of the arguments.

And that is the definition of this domain.

If you find this a little hard to read here, or want to download it.

You can always go to the book website.

So this is all based on the automated planning book, that you see here.

And there's a website for this book.

And you can see here at the bottom is the PDDL specification of the dock worker robot domain.

And so if you click this link, you will get a PDDL file which is almost exactly what you've seen in this slide.

And since the URL is a little hard to read here.

It isn't bigger.

So it's http://projects.laas.fr/planning/.

If you go to that URL you get to this website, and you will be able to download the PDDL domain.

###b Domains and Problems

Now with a formal definition of a STRIPS planning domain, we can now define a STRIPS planning problem.

A STRIPS planning problem is a triple consisting of three components, and these are given here.

This is, of course, the state transition system that we're looking at.

This is the initial state and that is the goal.

So, what are these formally?

The first is a STRIPS planning domain that is defined over some first order language we've seen earlier.

And again, we have the state space here, the action space and the state transition function as we've seen in the previous slide.

Then there are two more things we need for a planning problem.

The first is an initial state which is simply one of the states in our states base.

Finally, the goal of a planning problem is defined as a set of ground literals.

And once we are given a goal described in this way, we can define the set of possible goal states, as all those states in s that satisfy the goal.

And satisfaction was something we defined earlier, you may remember the symbol.

We said that s satisfies g if all the positive literals in g are true in s and none of the negative literals in g are holding in s.

So that is the definition of a planning problem.

An interesting observation is that goals are defined as exactly the same things that are the preconditions of operators namely sets of literals or for actions sets of ground literals.

That is not a coincidence.

And here's a very simple example of a STRIPS planning problem, a DWR example from the dock worker robot domain.

The planning domain is the one we've defined earlier consisting of the five operators described in PDDL.

Then I need to pick an initial state which is what I've done here, so this is our initial state and I've described that in this expression here as a set of ground atoms.

So each of those are relations that relate different objects in the domain to each other.

It's a very simple example as you can see.

And then I have to pick a goal, which is a set of ground literals.

In this case, I want the robot to be loaded with a container, so it is not unloaded, and I want the robot to be at location too.

These are my two goal conditions, and in this simple example, there is only this one state, that is a goal state.

And here is a definition of a planning problem in the PDDL syntax.

Note that this is a different problem so it is not the problem we have seen in the previous slide.

In PDDL we of course must define the initial state, and the goal.

Just like we need for any planning problem.

But there are a few other things that we need to define.

The first thing is, we can give the planning problem a name, so we can refer to it.

That's what we do here, it's a DWRPB1 problem.

Then we can say which domain we're using.

And we're saying that by referring to the domain name as shown here.

Then we can explicitly define what objects we're using in this domain.

This is actually optional and requires the typing extension.

So we say that there's one robot which is a type robot.

then we have two locations, two cranes and four piles here and all together we have six containers and the palate.

The palate is of course also defined in the domain, but that doesn't matter here.

So this declares what things exist in our states.

Then the initial state contains the ground atoms that relate these objects to each other.

So that's what we got here.

We start off with a static relations.

These are the things that describe the topology, and won't change from state to state.

So there's the adjacency between the two locations.

There is where the different piles are.

Piles one, P1 and Q1 are at location one.

P2, Q2 at location two.

And where the different cranes are.

One crane at each location.

Then we have.

The dynamic relation's listed next.

And they tell us what is in the first pile.

Namely, container A, B, and C.

Here's the order of the containers on the pallet.

Container C is at the top.

The same for the second pile.

This is also at location one, containing three containers, stacked on top of each other.

And container F at the top.

The piles at the other location are both empty as you can see here because the top of the pile is the pallet.

Finally we learn where the robot is and that's unloaded and also that the two cranes are currently not holding anything.

That was the initial state, and now comes the goal.

The goal is described as a conjunction.

So all of these things must be true.

And it's simply saying, in which pile the different containers must be.

Ca must be in P2.

Cc in P2, and so on.

And the other four containers must be in the pile, Q2.

So it doesn't specify the order in which the containers must be in those piles, just in which piles they must be.

Note that there are no negative goals in this example but of course it is possible to define negative goals in general.

And if you find this a little hard to read again here's the URL where you can find the complete description of this problem so you can download it and read it in quiet and peace later.

###c Domains and Problems

What we've seen so far is the complete input that must be given to a planner.

Now we will turn to the output of the planner which is, of course, a plan.

And a plan is a sequence of actions, A1 through AK.

We're using the pi symbol here to denote the plan and the sequence consists first of the action A1 then of the action A2 and so on until we reach the action AK.

And K must be greater or equal to zero so the plan can be empty and contain no actions.

The length of a plan then, is simply the number of actions it contains.

So it is K, where K is zero or more actions.

Given this definition of plans, there's a number of things we can do with those plans.

For example, we can concatenate plans.

That means, if we're given a plan that consists of actions A1 through AK, and another plan consisting of A prime one through A prime J, then we can define the concatenation of those two plans, as simply the sequence of actions consisting, first, all those actions from the first plan.

And then, all those actions from the second plan.

We can also extend our state transition function as follows.

We can define a state transition function for a state and a plan.

By defining that it is the state from which we start, if our plan was empty.

So if there were no actions in the plan, we simply remain in the same state.

But if there were actions in the plan.

So k was greater than zero.

Then what we do is we first execute the first action.

That gives us a new state.

And then what we do.

We apply the state transition function, recursively, to the remainder of our plan.

For this to be well defined, of course our action, A1, the first action in our plan, must be applicable in the state in which we're trying to apply the plan.

In any other case the state transition function applied to the state and the plan is undefined.

So the most important thing to remember here is we define plans simply as sequences of actions.

Of course there are more complex types of plans but this is what we're looking at here now.

The output of a planner is a plan.

But of course, not every plan is a solution to a planning problem.

And here is how we can define what constitutes a solution.

We are given a planning problem consisting of a state transition system or a set of operators.

An initial state, and a goal description then we can say that a plan pi, is a solution for a planning problem.

For this planning problem that we are given here.

If the state transition function applied to the initial state of the planning problem.

And the plan that we're trying to execute in that problem gives us a new state.

And that state satisfies the goal.

So, a plan is a solution, if the execution of the plan leads us to a state in which the goal is satisfied.

Since there may be many plans that are solutions for planning problems, we want to distinguish those.

And we can do that by defining some properties of the solution plans.

So we can say that the solution is redundant if there's a proper subsequence of this plan, that is also a solution for P.

So if we can take our plan pi, remove some of the actions, and still have a solution plan, then we call the original plan redundant.

And we can say that the plan pi is minimal if no other solution for our planning problems contains fewer actions.

So it is minimal if it has the minimal number of actions.

Now all the definitions you've seen so far apply to classical planning.

But in fact, there are variations of what is exactly classical planning.

Most of what we've seen so far is the strips representation.

Here, I want to give you a quick overview of what other flavors of classical representations exist.

The simplest representation is the propositional representation.

In the propositional representation, a world state is described by a set of propositions.

So, a proposition is a simple symbol.

It does not contain any internal structure.

In the strips representation, in the state, we have first order atoms, ground atoms, to be more specific.

And these relate objects to each other.

In the propositional representation, there are only propositions, symbols.

Actions in the propositional representation consist of preconditions, post-conditions to be added and to be removed.

The first thing to note here is there are only actions, there are no operators because we don't have parameters and that's because we have symbols that have no objects they relate to each other.

So there's no variables which means everything is ground, everything is in action.

Then we have preconditioned propositions which means we don't have positive and negative propositions in the preconditions.

Only positive propositions exist in this simple representation.

And then we have negative and positive effects, as before these are symbols not ground atoms.

And I've also mentioned they are often called the add list and the delete list to be removed.

The strips representation then is simply the same, but instead of propositions we're dealing with first auto-literals, in most cases.

Specifically this means the preconditions of an operator can be positive or negative.

The final variant is the state variable representation where a state.

Consists of a set of state variables.

A state variable is simply a variable that must have a value.

It's usually described by a function.

So, for example, we could have a function that describes the location of the robot.

RLook that takes a robot as an argument, and tells us where that robot is at location L.

The advantage of this representation is, that it represents the functional property of this.

The robot can only be at one location at any one time.

So in the strips presentation, at least, in theory.

You can have several assertions of where the robot might be.

In the state variable representation.

And, the functional property, namely that the robot can only be at one location at a time is exploited.

Actions in this representation are simply undescribed by sets of preconditions that look like expressions like this and assignment of new values to these state variables like this one.

So what you need to remember here is that there are different flavors of the classical representation, there's not just the stripser presentation we've looked at.

There's also these other two.

But, and this is the important point, these representations can be translated into each other so essentially they're all equivalent in terms of expressiveness.

There's nothing that you can say in one of these three representations that you can't say in the other.

##Forward Search

###a Forward Search

So far, we have laid a lot of groundwork before we can describe our first planner.

We have seen how a planning domain has described a states space.

We have seen how to define a planning problem, what constitutes a solution to the problem, and previously, we have seen how to search through a search space.

If we put all this together, we end up with a forward state-space search planner and that is what we will look at next.

So here is the basic idea how state space search works, namely, we want to apply standard search algorithms that we've seen before like breadth-first search, depth-first search, or A\*, you may remember, to a planning problem.

And to do this, we need to define the search space which may be different from the state space.

But in this case, our search space is simply a subset of the state-space that we define for the planning problem.

The nodes in our search space correspond simply to world states.

So the states that we have in our planning problem are nodes in our search tree.

The arcs in the search space correspond to state transitions as defined by the operators and the actions that we execute as part of the plan.

And a path in the search space corresponds to a plan, which is the solution we are looking for.

More specifically, here is how we can define a planning problem as a search problem.

So we are given a planning problem as a set of operators which implicitly define a state transition system.

We are given an initial state and a goal description.

These three components make up our planning problem, then we can define the search problem as follows.

For the search problem, we need an initial state and we simply take the initial state from our planning problem.

Then, we need a go for our search problem and we define a goal test here naming the test that s satisfies g, so the state that we're curently searching must satisfy the goal.

Then we can define a path cost function for our search and that is simply the length of the plan we're currently looking at.

Implicitly, this means that all actions have equal cost here and that is why the path cost function as the length of the plan works.

And the final component we need is a successor function and the successor function, denoted gamma of s here, is what we will define next.

The successor function gamma of s for single state is defined here.

It is the set of all states gamma s,a for all actions a that are applicable in the state s.

So this set consist of all those states that can be reached by an applicable action from our state s.

If I wanted to compute this, I would have to, I have to go through all the operators and find all the ground instances of these operators that are applicable in the state, then I could apply those actions in the state and I would get all the successor states here.

This is how gamma of s is defined.

We can extend this definition slightly.

Suppose we are not in one state but we are in a set of states.

We know that we are in one of these states and we, we want to define what states are reachable from any of these states.

Then, this is simply the union over any of these states of gamma sk.

So we compute the successors for each of the individual states and build the set union, which is the result of gamma s, s1 through sn.

This gives us the states that are reachable from any of those states that are the input to the function in one step.

We can make this definition yet more general by naming the number of steps we want to allow.

In the simplest case, we have gamma zero which means we allow zero steps.

So if we're in any of these states, s1 through sn, and want to compute the states that we can be in after zero steps.

Well, that's exactly the states we start in.

If we don't do anything, we can't go to any other states.

But, in general, we want to allow m steps here and we want to say, initially, we are in one of these states s1 through sn.

And then we can apply a recursive definition of the function gamma by saying we apply this to gamma m minus one, so we take the set of states we start from.

We can go m minus one steps from these states, that this, that is the input to this here, and then, we can go one more step here.

And that is then the set of all states that are reachable from s1 through sn in m steps.

So that is what we've defined here, we've defined the function gamma m, which maps a set of states to another set of states.

Mainly, exactly those states which are reachable in m steps from anyone of the states given in the input.

The transitive closure of this function, then simply defines the set of all reachable states, so this is defined here.

All the reachable states are simply the union of k from zero to infinity of gamma k of s.

So we start in our initial state and we apply k steps from there.

This is this set and we can apply zero, one, two, three, and so on, up to infinity steps.

And if we take the union of all that, that is all the states that are reachable from our initial state s.

And that is the function gamma forward of s.

And here is why I've given you such a complex definition, because, with this definition, I can very simply state when a planning problem has a solution.

So we can state that a STRIPS planning problem defined by a state transition system initial stating goal or a statement defined by the operators and the initial state in the goal, has a solution if and only if the following holds.

Namely, if we take the set of all goal states and we take the set of all reachable states and we build the intersection between these two sets, then, this must not be the empty set or you can see it the other way around, too.

If this set actually contains an element, let's say a state Sg, then this state is in both these sets, which means it is a goal state and it is reachable from the initial state.

And if there's a reachable state from the initial state that is a goal state, that means we have a solution to our planning problem.

Now, this is all great.

But you may wonder, when are we actually going to see a planning algorithm?

###b Forward Search

Here is the four one state space search algorithm.

This algorithm is defined as a function forward search that takes three arguments namely the three components that make up a planning problem.

The first component is the set of operators defined for the planning problem.

Then we have an initial state and a goal description.

The algorithm works by starting from the initial state and searching from here.

And it also builds up a solution plan why we go through this loop.

As for the previous search algorithms, the first thing in the loop we do is test whether we have reached a goal state.

The goal test, now, is whether the state that we're currently looking at, initially the initial state, is a goal state.

We test this by testing whether it satisfies the goal.

If this is the case, then we can return our plan, initially, the empty plan.

So, if our initial state was the goal state, then we return the empty plan, and we are done.

If not, then we have to continue.

And what we have to do next is compute the state transition function.

We do this as described earlier by computing all the ground instances from all the operators defined in our planning problem that are applicable in our state.

So this gives us the set of applicable actions in our current state.

Now if this set was empty, if there are no applicable actions in the current state, then we can return failure.

That means we, we have exhausted our search space and haven't come across a solution.

The next step is simply choose one of the applicable actions that we have just computed.

What I've done here is simply made my life a little simpler by describing the algorithm as a non-deterministic algorithm.

This is a non-deterministic choice point.

What this means, in the actual implementation, would have to do search here.

It would have to back track to this point, to try out the different action.

If the one we've chosen previously fails.

So this would build up a search tree, branching at exactly this point.

In a non-deterministic algorithm we can, of course, assume that we have chosen the right action here.

Then what we do is we simply update our current state by applying the state transition function of the previous state, this is the previous state, and the action that we apply in this state.

And of course we have to add this action to the plan, so we concatenate new plan consisting of just one action to our old plan, and get the new plan as a result and that's it.

We simply go through our loop again until we either come to this point where we can return a plan to a solution state.

Or we come to this point where we return failure meaning we have exhausted the search space and didn't find a solution.

And here is a very simple example to illustrate this algorithm.

We start off in a initial state, which is the trivial problem we've seen earlier.

And we have defined the goal also from the example we've seen earlier to give us just one state as a goal state.

But the algorithm doesn't know that there's only one goal state, or where it is.

So we will remove this, here.

So, the first thing the algorithm does is test whether this is a goal state.

And I can assure you it is not.

So the algorithm will continue by computing the applicable actions.

And then selecting one of these applicable actions.

And in this case, what the algorithm does is select this action, here.

We're taking, with the crane, there's only one, at location one, the container, which is on this pile here.

from the pallet in the pile.

That is the action that the algorithm chooses.

Then what happens is it applies the state transition function to get a new state, which is the state we see here.

And it also updates its plan, which is what we have here.

And it continues like this through the loops so it checks whether this is a goal state.

It isn't a goal state, it computes the applicable actions, picks one of those, in this case that's the move action, and accordingly has to compute a new state, that's the new state we generate with this move action.

And so on we continue through the loop and see this is not a goal state, so we compute the applicable actions again, now we try to load the container with the crane at the location.

And, we get a new state.

As a result, now you can see the container is on the robot.

And, this is not a goal state so we go through the loop.

And we find there's a final action that we need to execute.

We need to move the robot to the other location.

And then we get a new state.

And this is now our goal state so at the beginning of the loop, the algorithm will terminate.

And it will return at this stage this plan here consisting of those four actions that, gave us the path through this state space.

So you have seen that the algorithm was only a very small step given all the definitions we had before.

But now we want to say a little bit more about the algorithm and what we want to say is that the algorithm possesses two properties that are very important.

Forward search is sound and forward search is complete.

Soundness means that if the function returns a plan as a solution, then this plan is indeed a solution.

This is, of course, a very useful property of such an algorithm.

If the algorithm was not sound, that means it could return a plan that isn't a solution.

So we would still not know what the solution is but the algorithm is sound.

And the proof of this is very simple.

We can show this by induction.

And we show that, at the beginning of the loop, this statement here always holds.

So we have the two loop variables, state and plan.

And we show that the state is always equals to gamma of si, and the plan we're currently looking at.

This is true, initially, of course because the initial value of state is the initial state.

And the initial plan is empty.

So gamma applied to SI with the empty plan means we still are in the initial state.

And those two are equal.

And then we can show that this condition is maintained through the loop.

Each iteration of the loop keeps this condition true.

Which means that it is also true for the final iteration before we return the plan.

And that means the state is the result of applying the state transition function, NSI, with the plan.

And we return from the function when state satisfies the goal.

So, therefore, this plan must reach the state.

And our algorithm is sound.

The second property that the algorithm is complete means that if there is a solution to our problem, the algorithm can find the solution.

And since this is a non-deterministic algorithm, we talk about an execution trace.

So there is a set of choices that we can make at the non-deterministic choice points, such that the algorithm will return the solution plan.

And again, the proof can be done by induction.

And this time, we show that our plan is always a prefix of the plan we're looking for.

What you need to remember is only that our algorithm is sound and complete.

##Backward Search

We have now, almost reached the end of week two, and we have just seen our first planning algorithm.

Some of you may be a little disappointed that it took so long to get to the first planning algorithm, and for those people here comes the next planning algorithm.

In the algorithm we've just seen, search states are exactly those states that are world states in the planning problem.

States are sets of ground atoms.

The algorithm then searched forward from the initial state through all the reachable states, until it comes across a goal state.

The algorithm we will look at next is Backwards State-Space Search.

In this algorithm we'll start form the goal, and search backwards through the state space, until we reach the initial state.

This is quite straight forward, and very similar to forward search, as you will see.

We will start by defining two concepts, namely, relevance and regression sets.

Relevance is really the equivalent concept to applicability, as it tells us which actions we can use to move through our search base.

Again, we start with a planning problem consisting of the usual things.

Namely, a state transition system that tells us how the world can evolve, a initial state from which we're moving away, and a goal description which tells us which states are goal states.

Then we can say an action, A, of our action set, is relevant for goal, G, if the following two conditions hold.

Firstly the goal in intersected with the affect of the action must not be empty.

This means they must be an element that is in both sets, so there must be element there is on the one hand go, and on the other hand an infect of the action.

This means the action must contribute to the goal in some way.

Secondly the positive goals of the goal description and the negative effect of the action must not intersect and the negative goals and the positive effects must also not intersect.

This means the goal must not conflict with the effects of the action.

Looking at the first case, if we had a negative effect of the action, that was also a positive goal, that means this action would delete this goal from our state.

So it would no longer hold.

The second case is just the other way around.

We have a positive effect that adds a negative goal to the state, which we don't want.

So, an action is relevant for a goal, if it contributes to the goal, that's the first condition.

And if it does not interfere with the goal in a negative way.

That's the second condition.

Now we can define the regression set of a goal G for a relevant action A.

And as you can see this is meant to be the inverse of the state transition function gamma and it is computed as follows.

We start off with the original goal which is a set of ground literals and we remove all the effects of the action from that goal and then we add all the preconditions of the action to the goal.

Effectively, this computes from a given goal G.

We remove all the effects, meaning we remove all those things that have been achieved by the action that we have selected.

So we no longer need to achieve these if we execute this action as the last step before the goal.

But then, we need to have all the preconditions true, so that we can actually execute this action.

So what this gives us, is a new sub-goal.

And if we can somehow achieve this sub-goal, then we know that through the action A, we can achieve our original goal.

Relevance and regression sets tell us how we can move through our state base backward.

They tell us how we can, given a goal and a relevant action for this goal, compute a new sub goal that constitutes a new search state for our backward search.

So here is how we can define the successor function for the backwards search, which is equivalent to the reachability analysis we did for forward search.

We start with regression through a single step from a given goal.

This is defined as the set of all those sub-goals, gamma minus one GA, so the regression sets, for a relevant action for G, our original goal.

To compute this we start with our original goal, then compute all the relevant actions for this goal, and regress through these actions two new sub goals.

So this is a set of sub goals that we get as a result.

The next step is that we extend this function to take multiple goals as input, so the input is now a set of goals rather than a single goal.

And if we regress through zero states that means simply the set of goals stays the same.

There's no change.

If we can go one step backwards in our search from a given set of goals, this is simply the union over all the individual goals and we regress those through our regression function defined earlier.

And then we can apply this for M steps backwards from a given set of goals by simply doing a recursive definition as we did for reachability.

So we apply it for one step after we've applied it to M minus one steps for the same set of goals.

What this means is that, from any of the sub-goals that we've computed in this way, we can reach the original goals in exactly M steps.

M actions are necessary to go from the sub-goals to one of the original goals.

And we can define the transitive closure for this function, which is the set of all regression sets that are possibly.

So these are all the possible sub goals, that we can compute from our original goal.

This is, pronounced gamma backwards, is simply the union over all lengths of plans that we can implement here, where K is the length of the plan, and we compute gamma minus K, of our original goal.

So for any K from zero to infinity, this gives us all the sub goals that are possibly reachable in our search base from our original goal.

Now, given these definitions we can define a search space for backwards search planning.

The input to the algorithm is again a statement of planning problem consisting of a set of operators in a initial state, and the goal to description as before.

Then the search problem can be defined by the following four components.

We start with the initial state for a search.

That is not the initial state not a state base but the goal.

So we're searching backwards from the goal.

And in our search space the goal is the initial state.

And if the goal is our initial state that means we need a new goal test for the search space.

And this goal test is that the intitial state in our problem specification satisfies our sub-goal S.

Remember, we move through the search space, or that is the idea, from the goal backwards by computing sub-goals and S is meant to be one of these sub-goals.

Now if we come across a sub-goal that is satisfied the, in the initial state, that means we can reach the goal state from the sub-goal according to our regression function just defined.

So, if our initial state satisfies the sub-goal, we have reached a goal state in a our search space.

The path cost function remains unchanged, it is simply the length of the plan.

And the successor function, will be using is simply the regression function we've defined in the previous slide.

In general this function takes a sub-goal that we've come across, and computes its successors in the search space.

So, this concludes the definition of the search space for backward search planning.

Next I could show you the backward search planning algorithm and pseudo code, but I won't.

The pseudo code would look almost identical to the code defined for forward search and I'll leave that to you as an exercise to modify that algorithm so that it performs backward search.

Now that you understand how two planning algorithms work, I'll even given you the idea for a third one.

And to introduce this, I'll give you an example.

Suppose our goal, our original goal we start from is that we want the robot to be at location one.

There is one operator in the dock loc robot domain, that can achieve this goal, and that is the move operator for moving a robot r from location l to location M.

And we can see that this operator is relevant because it has an effect at r m which we can use to achieve our goal at robot location one.

So all the actions that can be relevant for this goal must be of this form that we want to move the robot from some location L to location one.

But, l remains a variable here.

So we don't know what this value of l should be.

In fact, if you choose the wrong value for ,, it may even interfere with the goal.

Because we also have a negative effect, not at rl.

In the backwards search we've considered so far, we've only looked at actions for regressing goals to sub-goals.

So what we could do in our algorithm is simply replace this value L through all possible constants that are of the right type.

But if there are many places from which we can move to location one that means there are many options and that increases the branching factor in our search unnecessarily.

So what we can do is simply keep this variable as a variable and that is what is called lifted backwards search, which can also deal with partially instantiated operators where not all the parameters of the operators are replaced by actual values.

This does reduce the branching factor but unfortunately it also makes the algorithm a lot more complicated.

Keeping variables in a plan is an example of what is sometimes called least commitment planning where we try to make as few commitments as possible during the planning process unless we have a good reason for making a specific commitment.

We will see a lot more of this type of planning next week.

So this concludes the segment on states-based search planning.

In this segment we've learned a lot about the STRIPS representation for planning.

In the STRIPS representation we have seen a standardized way of representing the internal structure of states, namely a sets of ground atoms.

So we have objects that are related by some relations, and sets of these atoms describe what the world state looks like.

And then we have defined what the internal structure of operators looks like.

Namely, an operator consists of a name with parameters, a set of preconditions, and a set of effects.

The effects are often divided into positive and negative effects or the add list and the delete list.

Based on this we can define strips planning domains which are simply sets of operators, and we can define strips planning problems and consisting of a domain, an initial state, and a goal description.

And all this we've learned together with a new syntax, the PDDL syntax, for describing planning domains and problems.

PDDL is probably the most commonly understood language by planners today.

And next we have seen how forward states space search can be used to solve planning problems.

And there is a variant of that we have also seen how we can search this space backwards from the goal to the initial state.

Unfortunately this planning algorithms as of described them here are very inefficient.

But as we will see later on the course it doesn't take all that much to turn them in to the state of our planning algorithms.


#Week 3

#Week 4

#Week 5

#Features

##Feature - AI Planning for Robots

AI Planners, using a generation of action sequences in robots such as the Freddy 2, Hand/Eye Assembly Robot displayed here in the National Museum of Scotland in Edinburgh.

This version of Freddy from four decades ago had a large, fixed mounted arm with sophisticated touch sensitive grippers, and a side mounted camera to look at its world.

Instead of the robot moving to reach objects, the platform underneath the robot was shifted instead.

Freddy was demonstrated on flexible assembly tasks involving simple toy, cars, and boats.

The video playing is taken from a 16 millimeter film of a Freddy project demonstration made in 1973.

Freddy had to identify jumbled up parts via vision, unscrambled the parts, and then assembled them to create the required product.

Some parts in the area didn't belong to the target assembly.

In one project, the Edinburgh, normally an AI Planner, was used to generate object construction and arm movement sequences for Freddy.

In its time, Freddy was one of the most sophisticated robots in the world.

The Stanford Research Institute Problem Solver Strips is one of the best known a most influential AI Planners, and you'll learn about it on this course.

Despite of being created over 40 years ago, it was already news control the activity of a robot called Shakey at the Stanford Research Institute, which is now call SRI International.

Strips had may interesting features including use in theorem proving techniques to reason about the state of points in the plan.

Strips gave us a representation of actions with preconditions and effects.

It's got a mechanism called macrobes, for generalization of its operators.

It included execution support facilities to deal with partial failures.

Shakey is now on display at the Computer History Museum in Mountain View, California.

As we saw, Freddy included an early robot arm, but as you can see here, technology has come a long way.

For example, this modern research robotic arm has many more capabilities.

Obviously, there's a lot more to robotics than just activity planning and there are significant challenges in terms of vision, mechanical issues, and spatial reasoning including locating the robot, and tracking other objects.

But planning is a key aspect of intelligent behavior.

Not every robot has the kind of activity plan that we are describing on this course.

That is, one which can plan from first principles.

But robots typically do have plans, perhaps precompiled ones, that they use to control their behavior.

AI Planners have been used in much more recent robots also, of course, such as in some humanoid robots and the robots used in the Annual RoboCup Soccer League Teams.

Here, planning may have to cover the coordination of different robots with different roles, and the robots themselves can have planners on board that adjust their behaviors dynamically.

The planners have been used in different kind of robots, too.

For example, an on board planner and reactive execution agent was used to autonomously control the NASA Deep Space 1 spacecraft prepared in its flight rounding over the comet.

AI Planners have been used to provide adaptability and flexibility robots in the past and in the present, in the factory, in the home, and in the outer space.

And it's nearing which to put good use tomorrow.

We hope you'll be part in creating this future.

##Feature - Story of A* and STRIPS by Nils Nilsson

Hello, my name is Nils Nilsson.

I helped in the development of A Star and STRIPS, and I'd like to explain a little bit about the history of those programs.

In the mid 1960's at SRI, then called the Stanford Research Institute, I was working on a robot called Shakey.

Shakey had several programs, some for dealing with perception.

We had machine vision programs, for example, and some controlled Shakey's actions as it moved around in it's environment.

There were two problems in developing these programs for controlling Shakey's actions.

One problem was how Shakey should navigate throughout a, a room strewn with obstacles without bumping in to any of them.

Another problem concerned how Shakey should put together its high level actions in order to achieve high level goals.

With regard to the navigation problem, we set up waypoints that were adjacent to and somewhat standing off of various obstacles in the room.

These waypoints could be considered nodes in a graph.

The edges of the graph would be the straight line distances between way points that Shakey would be able to travel.

So the problem of navigating from one point in the room to another is the same as the problem of finding the shortest path in a graph.

Edgar Dykstra had an algorithm for doing just that, but the problem with the Dykstra algorithm was that it searched outward from the start node toward the goal in all directions.

What we wanted was an algorithm that focused its search more in the direction of the goal.

Now I was aware of a program developed at Edinburgh University, a graph traverser program, by Jim Dorin and Donald Rickey that did focus toward the goal.

Their algorithm assigned numbers to nodes in the graph that were purported to be the difficulty of achieving the goal from that particular node.

I suggested that, that number ought to be an estimate of the distance from the node to the goal, ignoring any obstacles that might be in the way.

A colleague at SRI, Burt Brofel, suggested that the number ought to involve, also.

The distance from the start node to the node in question and that would prevent Shakey from being led down promising but ultimately futile paths.

Another colleague, Peter Hart, suggested that if the estimate from the node to the goal was a lower bound on the true distance Shakey would have to travel from that node to the goal.

Then the algorithm, which we named A Star, would in fact achieve the shortest path that was possible.

Then Peter Hart and Burt Rayfield /g and I together set about to prove Peter's conjecture and that was the development of the A\* algoritm which involved this number associated with each node that in, that was a sum of the distance from the start node to the node in question plus the estimate of the distance from that node to the goal.

With regard to the second problem, the one I'm stringing together, Shakey's high level actions to achieve higher level goals, Richard Fikes and I, another colleague at SRI, developed a system we called STRIPS for Stanford Research Institute Problem Solver.

STRIPS used high-level models of Shakey's world, that is instead of just the coordinates, the positions in Shakey's room, we used a database of facts that were true of particular situations that Shakey could get itself into.

So we still wanted to be able to solve the problem of achieving these high-level goals by some sort of graph searching program.

And so the starting node of the graph would be a list of all the facts that were true in Shakey's present situation.

The goal then was also described by some statements of facts that we wanted Shakey to make true by achieving, by applying, actually, these high-level actions.

So what we needed in order to convert this into a search problem in the graph would be a computational way of producing states of the world, that is, other databases describing what a particular state of the world would be when Shakey applied one of it's high-level actions.

To perform that computation, we invented something called STRIPS rules.

Now, a STRIPS rule would consist of a pre-condition, that is to say all of the facts that had to be in a particular world, in order for Shakey to apply one of it's high-level actions.

A delete list, that is to say a list of facts which could no longer be guaranteed to be true if Shakey did apply one of it's high-level actions, and an add list, which would be those facts which the high-level action would make true.

And so what we did is use these strips rules to go from one state, that is one database describing Shakey's current situation, to successor states in the graph.

So we could even use the A Star algorithm if we had a good way of, estimating what the distance would be from one of these databases, to one which had Shakey's goal achieved.

So we could use A Star, if we had that particular system.

Actually, in the system that we employed at program, the STRIPS system, we worked backwards from the goal by applying these STRIPS operators in a somewhat backward direction.

Those of us who were involved in the development, A\* and STRIPS, are gratified to know that these systems are used in many present day AI applications.

#Additional Stuff

##O-Plan Unix Systems Admin Demo

This is a demonstration of O plan.

Running on a simple single shot planning example.

We call it Unix volume groups.

What we're doing here is selecting a number of logical volumes to map against a number of physical volumes in the UNI-X systems administration environment.

And the planner which we are just going to run, that's it already run, is producing a script which can safely remove the volume mappings.

This a good example of generative AI planner being used in an appropriate application area, previously a complex deeply nested script was used and its maintenance was difficult and error prone.

##I-X CoSAR-TS Demo

This is description of the Coalition Search and Rescue Task Support Project.

Which is funded by the DARPA DAML Program And it's conducted by AIAI [UNKNOWN] and IHMC at University of West Florida.

So we're initially showing you the CoABS grid manager.

And you can see that there's a number of services which are running.

in particular, there'll be a JINI lookup service.

And a number of other services running directly on the CoABS grid.

Using KAoS for my IHMC and this provides a directer service.

And we're running a domain manager named dm1.

We then have two I-X process panels running.

CoSAR TS for the coalition search and rescue panel.

And US-SAR for the US search and rescue panel.

And we have two services running for information lookup, called hospitals and resources.

Four agent guides are running, which help us to maintain policy across the use of these agents.

Now we have, KAoS policy administration tool, which is helping us set up and maintain these policies.

So we have one domain which is running, mini coalition, within this is the four agency or previously, CoSAR, US-SAR and then the two services, hospitals and resources.

We're interested in limiting the kinds of search and rescue resources which can be chosen, depending on the flights that these resources would have to make.

The countries that they will overfly.

So we're going to establish a new KPAT policy, which we're going to call gao-flight-restriction.

This is going to be a negative authorization policy.

We'll choose from one of the available actions available to us, and select that.

And within this slighted action we have an available roll of has on country.

In this case, we're interested in placing a restriction on a country, which is Gao And then we can add that as part of the policy specification.

That we have, we have home country Gao.

Then there's going to be a second part of this policy.

Which is the country that we won't permit overflights of Gaoen search and rescue resources too.

So we'll say has rescue destination in this case a limitation will be on the contrary so basically Gaoen search and rescue resources can not overfly arbelo so you can see that we've got two parts policy specification there.

And then we commit that policy and then we want to distribute it.

So, we'll just show you now the four agents once more within the KPAT tool.

And we can show you that for the resources, agent we have that policy in place.

So we're now going to move over to show you the two process panels which are available.

The coalition search and rescue coordinator is going to be where most of the action in the demonstration takes place.

And then we're going to use a US-SAR officer panel to initiate the action, because they would have taken the initial report about the downed airman because he's a US national.

So in this case the US-SAR officer is told to send any reports of downed airman up to the coalition search and rescue coordinator for their activity.

So, we'll take a look at the actions available to us at the process panel can support.

And we see there that's there's a way to expand the description of this activity is a particular built in standard operating procedure.

And we get the expansion, which is five substeps here.

We now look, take a look at the first part.

And we see a way to break that down even further.

You can see the indented sub process.

And we're going to look at the first part of that, which is to establish the medical capability.

Now, in this case, we know that we've got a burned Ammon So we're going to use burn care as the medical facility capabilities we're looking up.

And then we're able to indicate that we've done that manually.

Then we're going to use a service available to us.

This is one of the services which is registered.

So we can invoke the hospital service to establish medical care, establish the hospitals we have that can deal with burn capabilities.

We're just showing you here that we know about hospitals and indeed resources as a service.

This is in the ice base two which described services and in relationships in that particular panel.

So we're going to invoke the hospital service agent and it's going to now be acting.

You can see it there it's already going off and looking for, downloading information from the, SONAT database about Arabello and Binni in this case.

And it's checked, finding hospitals, and that it's filtering those by the ones that can provide burdened care.

We can take a look at that.

We can look at the details, look at the kinds of hospitals available, where they are and so on.

Find out that some can deal with a burn carriage can see.

GahwadEl looks like a likely candidate.

We're going to, be interested in exactly where the hospital is in relationship to the downed hammond.

So we can perhaps best do that by looking at a map of the facility.

This would be loaded from the information known to the I-X process panel.

You can see the pilot down in the sea there.

And his condition report is in burns.

And you can also see the GahwardEl Hospital.

which, as we noted before is, is, has burned care facilities available.

So we're going to go ahead and slight that hospital by binding the variable appropriately.

And we can just tick that off as done.

And then we're going to look at the next entry, which has established the country that, that hospital is in.

You can imagine this being done more automatically in future.

In this case we're going to indicate that it's Arabello because that's what was indicated in the information that came back in the look up.

So now, that we've got our hospital available, we're interested in selecting the search and rescue results appropriate to the task.

And we have a built in standard operating procedure to remind us how to do this.

And in particular the first one is to, look up the SAR resources available which is going to invoke a resources agent.

This will use a soap connection over to a CMU matchmaker, which finds these appropriate search and rescue resources.

And we're finding some resources coming back, and in fact only resources that are allowed for all flights of arabello are going to come back so chaos has already been filtering the number of resources found.

This case there are two that are permitted and seem well to do the job and we're going to choose the U.S.

Marine helicopter that's convenient.

It's in the right sort of location and it's also helps us deal with the fact this is a U.S.

Airman down.

So we're going to indicate that we selected that resource.

By.

Oh.

We, we'll look at the map first just to show you where that U.S.

Marine helicopter is.

So we're now going to show you we can.

Bind the variable for the SAR results selected.

The US Marine helicopter.

And again we'll indicate we've done that.

Again you can imagine in the future much of this could be done in a more automated way.

And then we're going to notify the SAR resource itself, this would best be done by talking with the U.S.

SAR officer who would control that resource so that it goes back onto the process panel and when they themselves have notified the helicopter pilot they would indicate it's done and you would see that the, the, there's a cascade of done information done to excess information flying back up turning lue if we have not find the hospital and we just going to check what the outstanding issue was that we're ask to just check it, as we went along, and yes the country and SAR resource chose nar allowed to get the, and that was in fact constrained by the KPAT policy.

So that concludes are demonstration.

Further information is available at some of the URL's now being shown.

Thank you for watching the demonstration of the course RTX project.

##I-X I-Globe Demo

The SenseMaker receives a report about fire in the area.

He inserts a new record of the incident into his object view table.

Since he has no more information at the moment, he doesn't take any other action.

The incident is automatically added to the current state descriptor.

The state descriptor also contains current states of all field units and is shared in real time by both the Sensemaker and Commander.

25 minutes later, the Sensemaker receives reports about people injured in Town1 and Town2.

Again, he inserts corresponding records into his object view table and the state descriptor is updated.

Next, the Sensemaker creates two activities "treat injuries in Town1 and 2" and passes them to the Commander.

The Commander receives these activities immediately.

Before executing them, he orders to take aerial snapshots of the affected areas.

He passes the corresponding commands to subordinate field units.

Using HTN planning (I-Plan), CNP negotiation and the commitment mechanism, field units generate and execute a distributed plan.

They report completion of the task back to the Commander.

Based on the received snapshots, the Commander decides to build a mobile hospital nearby Town1 and 2.

He issues a corresponding command for the field units.

Once again, the field units come up with a distributed plan.

They transport construction materials and a builder to the site, and a mobile hospital is built there.

Upon completion of the task, the Commander executes the two treat injuries commands.

Medical material and medics are transported to the sites.

As usual, task completion is reported to the Commander.

Since these two tasks were originally created by the SenseMaker, he is also automatically notified about the tasks completion.

Approximately an hour later, the Sensemaker receives a report about collapsed houses in Town3.

He inserts a new record into his object view table.

He then passes a new task "build houses in Town3" to the Commander.

The Commander receives this task and executes it.

Field units take care of the task.

Successful completion of the task is reported to the Commander and is also propagated to the Sensemaker.
