<div id="content">
				<a id="top"></a>
	        		        	<h1 id="firstHeading" class="firstHeading">PGM:Inference: Sampling Methods</h1>
				<div id="bodyContent">
		            <h3 id="siteSub">From Coursera</h3>
		            <div id="contentSub"></div>
		            		            		            					<!-- start content -->
					<div lang="en" dir="ltr" class="mw-content-ltr"><table id="toc" class="toc"><tr><td><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1"><a href="#Inference:_Sampling_Methods"><span class="tocnumber">1</span> <span class="toctext">Inference: Sampling Methods</span></a>
<ul>
<li class="toclevel-2"><a href="#Lecture_Video_Table_of_Contents"><span class="tocnumber">1.1</span> <span class="toctext">Lecture Video Table of Contents</span></a>
<ul>
<li class="toclevel-3"><a href="#Simple_Sampling"><span class="tocnumber">1.1.1</span> <span class="toctext">Simple Sampling</span></a></li>
<li class="toclevel-3"><a href="#Using_a_Markov_Chain"><span class="tocnumber">1.1.2</span> <span class="toctext">Using a Markov Chain</span></a></li>
<li class="toclevel-3"><a href="#Gibbs_Sampling"><span class="tocnumber">1.1.3</span> <span class="toctext">Gibbs Sampling</span></a></li>
<li class="toclevel-3"><a href="#Metropolis-Hastings_Algorithm"><span class="tocnumber">1.1.4</span> <span class="toctext">Metropolis-Hastings Algorithm</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</td></tr></table>
<h1> <span class="mw-headline" id="Inference:_Sampling_Methods">Inference: Sampling Methods</span></h1>
<h2> <span class="mw-headline" id="Lecture_Video_Table_of_Contents">Lecture Video Table of Contents</span></h2>
<h3> <span class="mw-headline" id="Simple_Sampling">Simple Sampling</span></h3>
<p><pre>
0:34    Sampling-based estimation
3:58    Sampling from discrete distribution
14:02   Forward sampling from a Bayes network
17:02   Forward sampling for query
18:16   Queries with evidence (rejection sampling)
22:06   Summary
</pre></p>
<h3> <span class="mw-headline" id="Using_a_Markov_Chain">Using a Markov Chain</span></h3>
<p><pre>
0:17    Goal
1:40    The &quot;sticky&quot; issue
2:25    Mixing
5:08    Example run graphs
9:09    Using the samples
11:49   MCMC algorithm Summary I
13:29   Summarizing the implications
</pre></p>
<h3> <span class="mw-headline" id="Gibbs_Sampling">Gibbs Sampling</span></h3>
<p><pre>
0:42    Gibbs chain
4:35    Example
7:25    Computational cost
9:12    Another example
12:02   Computational cost revisited
13:03   Gibbs chain and regularity
17:59   Summary
</pre></p>
<h3> <span class="mw-headline" id="Metropolis-Hastings_Algorithm">Metropolis-Hastings Algorithm</span></h3>
<p><pre>
1:17    Reversible chains
6:16    Metropolis-Hastings chain
9:59    Acceptance probability
17:19   MCMC for matching
21:39   MH for matching: augmenting path
25:54   Summary
</pre></p>

<!-- 
NewPP limit report
Preprocessor node count: 38/1000000
Post-expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Expensive parser function count: 0/100
-->


</div><div class="printfooter">
Retrieved from "<a href="https://share.coursera.org/wiki/index.php?title=PGM:Inference:_Sampling_Methods&amp;oldid=4171">https://share.coursera.org/wiki/index.php?title=PGM:Inference:_Sampling_Methods&amp;oldid=4171</a>"</div>
					<div id='catlinks' class='catlinks catlinks-allhidden'></div>					<!-- end content -->
									</div>
			</div>