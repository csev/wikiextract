<div id="content">
				<a id="top"></a>
	        		        	<h1 id="firstHeading" class="firstHeading">PGM:Temporal Models - HMMs</h1>
				<div id="bodyContent">
		            <h3 id="siteSub">From Coursera</h3>
		            <div id="contentSub"></div>
		            		            		            					<!-- start content -->
					<div lang="en" dir="ltr" class="mw-content-ltr"><table id="toc" class="toc"><tr><td><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1"><a href="#Hidden_Markov_Models"><span class="tocnumber">1</span> <span class="toctext">Hidden Markov Models</span></a>
<ul>
<li class="toclevel-2"><a href="#Unrolled_network"><span class="tocnumber">1.1</span> <span class="toctext">Unrolled network</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Applications_of_HMMs"><span class="tocnumber">2</span> <span class="toctext">Applications of HMMs</span></a></li>
<li class="toclevel-1"><a href="#Summary"><span class="tocnumber">3</span> <span class="toctext">Summary</span></a></li>
<li class="toclevel-1"><a href="#Lecture_Video_Table_of_Contents"><span class="tocnumber">4</span> <span class="toctext">Lecture Video Table of Contents</span></a></li>
</ul>
</td></tr></table>
<h1> <span class="mw-headline" id="Hidden_Markov_Models">Hidden Markov Models</span></h1>
<p>Hidden Markov models (HMM) is simple yet extraordinarily useful class of probabilistic temporal models. It can be viewed as a subclass of Dynamic Bayesian networks (DBN).</p>
<p>Its simplest form can be viewed as a probabilistic model that has a single state variable S and a single observation variable O. There is the transition model that tells us the transition from one state to the next over time (S to S'). And there is the observation model that tells us in a given state how likely we are to see different observation (S'-&gt;O')</p>
<p><strong>Simple 2TBMs of HMMs</strong><br />
<pre>
   S -&gt; S'
         `&gt;O'
</pre></p>
<h2> <span class="mw-headline" id="Unrolled_network">Unrolled network</span></h2>
<p>From our 2TBM and started state at time zero, we can produce an unrolled network which has the same repeated structure:
<pre>
    S0 -&gt; S1  -&gt; S2   -&gt; S3    ...
           &#96;&gt;O1   &#96;&gt;O2    &#96;&gt;O3
</pre>          </p>
<p>Markov models have a lot of internal structure that manifests most notably in the transition model (sometimes also in the observation model). The following image show an example of transition model. Each of these nodes in the transition model (S1, S2, S3, S4) is not a random variable rather it is particular assignment to the S variable (values of S). 
<a href="/wiki/images/d/d9/Transition_Model_-_HMM.png" class="image"><img alt="Transition Model - HMM.png" src="/wiki/images/d/d9/Transition_Model_-_HMM.png" width="931" height="496" /></a></p>
<p>So, we can recognize that if we're currently at state S1, the model is likely to transition to S2 with probability 0.7 or stay in S1 with probability 0.3. Notice that all outgoing probabilities have to sum to one, because it's a probability distribution over the next state given that in current time point the model is in state S1.</p>
<h1> <span class="mw-headline" id="Applications_of_HMMs">Applications of HMMs</span></h1>
<ul>
<li>Robot localization</li>
<li>Speech recognition

<ul>
<li>HMMs are really the method of choice for all current speech recognition systems</li>
</ul></li>
<li>Biological sequence analysis</li>
<li>Text annotation</li>
</ul>
<h1> <span class="mw-headline" id="Summary">Summary</span></h1>
<ul>
<li>HMMs can be viewed as a subclass of the framework from Bayesian networks</li>
<li>HMMs seem unstructured at the level of random variables. </li>
<li>HMMs structure typically manifests in sparsity and repeated elements within the transition model that
really doesn't manifest in any way at the level of random variables</li>
<li>HMMs are used in a wide variety of applications for sequence modeling and they're probably one of the most used
forms of probabilistic graphical models out there. </li>
</ul>
<h1> <span class="mw-headline" id="Lecture_Video_Table_of_Contents">Lecture Video Table of Contents</span></h1>
<p><pre>
0:00    Hidden Markov Models
1:02    Unrolled network
2:41    Applications of HMMs
3:18    Robot Localization
5:02    Speech Recognition
5:55    Segmentation of Acoustic Signal
6:54    Phonetic alphabet
7:37    Word HMM
8:26    Phone HMM
9:10    Recognition HMM
11:01   Summary
</pre></p>

<!-- 
NewPP limit report
Preprocessor node count: 30/1000000
Post-expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Expensive parser function count: 0/100
-->


</div><div class="printfooter">
Retrieved from "<a href="https://share.coursera.org/wiki/index.php?title=PGM:Temporal_Models_-_HMMs&amp;oldid=2971">https://share.coursera.org/wiki/index.php?title=PGM:Temporal_Models_-_HMMs&amp;oldid=2971</a>"</div>
					<div id='catlinks' class='catlinks catlinks-allhidden'></div>					<!-- end content -->
									</div>
			</div>