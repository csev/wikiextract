<div id="content">
				<a id="top"></a>
	        		        	<h1 id="firstHeading" class="firstHeading">Algo1:DesignPrinciples</h1>
				<div id="bodyContent">
		            <h3 id="siteSub">From Coursera</h3>
		            <div id="contentSub"></div>
		            		            		            					<!-- start content -->
					<div lang="en" dir="ltr" class="mw-content-ltr"><p>When approaching algorithms, consider these Algorithm Design Principles:</p>
<table id="toc" class="toc"><tr><td><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1"><a href="#Brute-Force"><span class="tocnumber">1</span> <span class="toctext">Brute-Force</span></a></li>
<li class="toclevel-1"><a href="#Free_Operations"><span class="tocnumber">2</span> <span class="toctext">Free Operations</span></a></li>
<li class="toclevel-1"><a href="#Divide_and_Conquer"><span class="tocnumber">3</span> <span class="toctext">Divide and Conquer</span></a></li>
<li class="toclevel-1"><a href="#Randomize"><span class="tocnumber">4</span> <span class="toctext">Randomize</span></a></li>
<li class="toclevel-1"><a href="#Greedy_Algorithm"><span class="tocnumber">5</span> <span class="toctext">Greedy Algorithm</span></a></li>
<li class="toclevel-1"><a href="#Dynamic_Programming"><span class="tocnumber">6</span> <span class="toctext">Dynamic Programming</span></a></li>
<li class="toclevel-1"><a href="#Reductions"><span class="tocnumber">7</span> <span class="toctext">Reductions</span></a></li>
<li class="toclevel-1"><a href="#Intractables"><span class="tocnumber">8</span> <span class="toctext">Intractables</span></a></li>
<li class="toclevel-1"><a href="#Tool_Box"><span class="tocnumber">9</span> <span class="toctext">Tool Box</span></a></li>
<li class="toclevel-1"><a href="#Reference_Algorithm_Repositories"><span class="tocnumber">10</span> <span class="toctext">Reference Algorithm Repositories</span></a></li>
</ul>
</td></tr></table>
<h1> <span class="mw-headline" id="Brute-Force">Brute-Force</span></h1>
<p>These direct route algorithms work, but they may not be the most elegant or efficient. However, they can be useful. When a problem is a bit vague, brute-force thinking may help clarify the vagueness. Use brute-force to confirm that you understand the problem being solved and provide an initial working solution. Once the problem is understood through brute-force, we should always if we can do better, but within context. Knuth wrote, "Premature optimization is the root of all evil." Make sure that there is sufficient return on investment to justify the additional algorithm analysis. Strassen’s matrix multiplication is an example. Matrix multiplication seems almost certainly $$O(n^3)$$, but Strassen showed that it’s &lt; $$O(n^3)$$.</p>
<h1> <span class="mw-headline" id="Free_Operations">Free Operations</span></h1>
<p>There are many operations that are free or nearly free. Do these on your data sets, or at least perform a mind exercise with them. Organizing your data this way may lead to an idea of how to solve the problem without adding additional processing cost. Free operations include sort, heap/priority queue, binary tree, hash, depth first search, breadth first search, minimum spanning tree, etc. Basically any operation at or under $$O(n\ log\ n)$$.</p>
<h1> <span class="mw-headline" id="Divide_and_Conquer">Divide and Conquer</span></h1>
<p>Define the solution in terms of itself and solve recursively. Use the Master Method to evaluate complexity. If the solution doesn't fit the Master Method constraint, which happens in QuickSort analysis for example, then select the desired complexity and use induction to see if it can be confirmed.</p>
<h1> <span class="mw-headline" id="Randomize">Randomize</span></h1>
<p>Use randomization to obtain better performance on average. Adding randomness to what is usually a deterministic environment is counterintuitive, so it may not seem obvious. However, when data can take on a pathological nature, such as QuickSorting a sorted set of data, then randomization may be an efficient way to address the pathology.</p>
<h1> <span class="mw-headline" id="Greedy_Algorithm">Greedy Algorithm</span></h1>
<p>"Greed, for lack of a better word, is good." Gordon Gecko - Wall Street (movie). It can be good for algorithms too. Organize a greedy algorithm by processing the max (or min) values of a set and never reconsidering the consequences as you process the elements in order. Greedy algorithms can be effective, since they tend to be linear processing, other than sorting the choices, which is log-linear, but they can be tricky to get correct. Don't forget to consider a priority queue in the implementation.</p>
<h1> <span class="mw-headline" id="Dynamic_Programming">Dynamic Programming</span></h1>
<p>Build a solution from the ground up. This is similar to Divide and Conquer, but without much of the Divide portion. Consider the problem as if you already had solved parts. Then see if you have base cases as starting points. Build those parts iteratively upon one another, like a bricks in a wall, until you reach the solution at the top. Dynamic programming problems will occur with recurrence relations (i.e., recursive programs) that have a lot of subproblem overlap, such as the recursive Fibonacci: F(n) = F(n-1) + F(n-2). Note that the solution to F(n-1) will require F(n-2) as well, which is overlap, since it’s used in F(n) too. Don’t recompute F(n-2) more than once if you can avoid it. DP is also useful when you want only one option of the recurrence, i.e., the optimal choice. You don’t know the optional choice in the recurrence, but if you knew it, you would be able to solve the problem. DP recurrences tend to use information retrieved from optimal subproblems rather than pushing work down to smaller portions of the problem. For example most tree algorithms push functionality down to lower parts of the tree and assemble the results. Each subtree needs to execute its own portion of the subproblem. It just doesn’t return values back up. Additionally, trees don’t tend to have overlap, based upon their very structure, so DP techniques usually don’t apply to them.</p>
<p>Use DP to build the optimal sub problems so when you need to make a choice the answer is already there. This will be obvious if your recurrence relation uses the max or min value of subproblems. DP tends to return a numeric result, rather than a structural result. However, a structural result can be back computed by working back through the DP results. Some bread crumbs may be needed to reconstruct the path back.</p>
<p>DP programs tend to use arrays (or array concepts, such as associative arrays) to store results of sub problems. If reconstruction is not required, then some space efficiency may be possible. Many recurrence relations don’t access sub optimal problems way back. They tend to access sub optimal problems only one layer down. Therefore, the DP may only need to maintain layers for sub optimal problems that are actually needed. All others can be discarded once no longer needed. This may cause problems with reconstruction, but it depends upon the problem. Sometimes reconstruction that appears to require all layers may not really require them if one is clever. Each case is different.</p>
<h1> <span class="mw-headline" id="Reductions">Reductions</span></h1>
<p>Don’t reinvent the wheel. A new unknown problem may really be a known problem in disguise. Before diving head first into creating a new algorithm, make sure you understand the problem. Consider it from different perspectives. It just may be a problem, maybe with some slight adjustments, for which you already have a known efficient algorithm. Reduce your new problem to a known solvable problem.</p>
<h1> <span class="mw-headline" id="Intractables">Intractables</span></h1>
<p>Sometimes you’re stuck between a rock and a hard place. You’ve exhausted your tool box. No matter what data structure, algorithm or technique you try, you just can’t crack this nut. Maybe there’s a reason. Maybe the problem is intractable; that is, there’s no known efficient solution, so don’t waste your time trying to find one. If an NP-Complete problem reduces to your new problem, then your problem is intractable, but all hope is not lost. Improvements will take creativity and possibly not a lot of performance gains.  NP-Complete problems have no known efficient solutions. Any improvements may be meager and difficult to obtain. 
<ul><li> Focus on tractable sub problems. While the general solution may be intractable, your operational scenarios may be special specific cases for which there is a tractable solution. Look for specifics or invariants in your scenarios which you may be able to leverage.
</li><li> Use heuristics. That is fast algorithms that may not return the optimal solution, but they may return an answer that’s good enough. Consider the traveling salesman problem. The optimal solution is not computable from a practical point a view, but a heuristic that returns 90% of the optimal may be more than acceptable. Consider a heuristic for the Knapsack problem. Chose items based upon their value/size ratio, but add a final step to choose the largest value item if greater than the value/size ratio answer. There’s a reason thieves prefer jewelry over linens. A greedy algorithm based upon a value/size ratio won’t produce the optimal set, but it will produce a reasonable set in much less time. Use local optimization search, which optimizes locally, not  globally. Two cases:
</li><li> Start with a heuristic, and use LOS to improve it.
</li><li> No heuristic, then run many randomized potential solutions and use LOS to find the best.
</li><li> LOS is not a trivial task. Lots of craftsmanship.
</li><li> Suffer exponential performance, but there may be algorithms, while still exponential, such as $$O(2^n)$$, which are still better than brute-force $$O(n!)$$ performance (i.e., $$O(n^n)$$).</p>
</li></ul>

<h1> <span class="mw-headline" id="Tool_Box">Tool Box</span></h1>
<p>Abraham Maslow wrote, "If your tool box only contains a hammer, every problem tends to look like a nail." Learn new algorithms to add to your tool box. Books are a good source as well as the internet. MOOCs have been fantastic. Algorithm Coursera courses are available from Stanford, Princeton, Rice University, UC San Diego, et al. One does not need to master all algorithms. One needs to know of their existence and where to learn more about them.</p>
<h1> <span class="mw-headline" id="Reference_Algorithm_Repositories">Reference Algorithm Repositories</span></h1>
<p>If you have a problem for which you don't have an algorithm at hand, but you feel like an algorithm surely exists, consult a <a rel="nofollow" class="external text" href="http://en.wikipedia.org/wiki/List_of_algorithms">list of algorithms</a> or a <a rel="nofollow" class="external text" href="http://en.wikipedia.org/wiki/List_of_algorithm_general_topics">list of algorithm topics</a> or just Google "algorithm list". What you're seeking may very well be on one of these lists.</p>

<!-- 
NewPP limit report
Preprocessor node count: 2/1000000
Post-expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Expensive parser function count: 0/100
-->


</div><div class="printfooter">
Retrieved from "<a href="https://share.coursera.org/wiki/index.php?title=Algo1:DesignPrinciples&amp;oldid=29293">https://share.coursera.org/wiki/index.php?title=Algo1:DesignPrinciples&amp;oldid=29293</a>"</div>
					<div id='catlinks' class='catlinks catlinks-allhidden'></div>					<!-- end content -->
									</div>
			</div>