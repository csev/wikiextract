<div id="content">
				<a id="top"></a>
	        		        	<h1 id="firstHeading" class="firstHeading">ML:Errata: Week 3</h1>
				<div id="bodyContent">
		            <h3 id="siteSub">From Coursera</h3>
		            <div id="contentSub"></div>
		            		            		            					<!-- start content -->
					<div lang="en" dir="ltr" class="mw-content-ltr"><table id="toc" class="toc"><tr><td><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1"><a href="#VI._Logistic_Regression"><span class="tocnumber">1</span> <span class="toctext">VI. Logistic Regression</span></a>
<ul>
<li class="toclevel-2"><a href="#Decision_Boundary"><span class="tocnumber">1.1</span> <span class="toctext">Decision Boundary</span></a></li>
<li class="toclevel-2"><a href="#Cost_Function"><span class="tocnumber">1.2</span> <span class="toctext">Cost Function</span></a></li>
<li class="toclevel-2"><a href="#Simplified_Cost_Function_and_Gradient_Descent"><span class="tocnumber">1.3</span> <span class="toctext">Simplified Cost Function and Gradient Descent</span></a></li>
<li class="toclevel-2"><a href="#Advanced_Optimization"><span class="tocnumber">1.4</span> <span class="toctext">Advanced Optimization</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#VII._Regularization"><span class="tocnumber">2</span> <span class="toctext">VII. Regularization</span></a>
<ul>
<li class="toclevel-2"><a href="#The_Problem_of_Overfitting"><span class="tocnumber">2.1</span> <span class="toctext">The Problem of Overfitting</span></a></li>
<li class="toclevel-2"><a href="#Cost_Function_2"><span class="tocnumber">2.2</span> <span class="toctext">Cost Function</span></a></li>
<li class="toclevel-2"><a href="#Regularized_linear_regression"><span class="tocnumber">2.3</span> <span class="toctext">Regularized linear regression</span></a></li>
<li class="toclevel-2"><a href="#Regularized_logistic_regression"><span class="tocnumber">2.4</span> <span class="toctext">Regularized logistic regression</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Quizzes"><span class="tocnumber">3</span> <span class="toctext">Quizzes</span></a></li>
<li class="toclevel-1"><a href="#Programming_Exercise_Errata"><span class="tocnumber">4</span> <span class="toctext">Programming Exercise Errata</span></a></li>
</ul>
</td></tr></table>
<h1> <span class="mw-headline" id="VI._Logistic_Regression">VI. Logistic Regression</span></h1>
<p>PDF: <a rel="nofollow" class="external text" href="https://d396qusza40orc.cloudfront.net/ml/docs/slides/Lecture6.pdf">Lecture6.pdf</a></p>
<h2> <span class="mw-headline" id="Decision_Boundary">Decision Boundary</span></h2>
<p>At 1:56 in the transcript, it should read 'sigmoid function' instead of 'sec y function'.</p>
<h2> <span class="mw-headline" id="Cost_Function">Cost Function</span></h2>
<p>The section between 8:30 and 9:20 is then repeated from 9:20 to the quiz. The case for $$y=0$$ is explained twice. 
At 10:28 in the transcript, it should read 'nailed it' instead of 'melted'. 
At 11:21 in the transcript, it should read 'gradient' instead of 'grading'.</p>
<h2> <span class="mw-headline" id="Simplified_Cost_Function_and_Gradient_Descent">Simplified Cost Function and Gradient Descent</span></h2>
<p>These following mistakes also exist in the video:</p>
<ul>
<li>6.5: On page 19 in the PDF, the leftmost square bracket seems to be slightly misplaced.</li>
<li>6.5: It seems that the factor 1/m is accidentally omitted between pages 20 and 21 when the handwritten expression is converted to a typeset one (starting at 6:53 of the video)
At 9:07, 9:13, and 9:25 in the transcript, it should read 'vectorized' instead of 'vector rise'.</li>
</ul>
<h2> <span class="mw-headline" id="Advanced_Optimization">Advanced Optimization</span></h2>
<p>In the video at 7:30, the notation for specifying MaxIter is incorrect. The value provided should be an integer, not a character string. So (...'MaxIter', '100') is incorrect. It should be (...'MaxIter', 100). This error only exists in the video - the exercise script files are correct.</p>
<h1> <span class="mw-headline" id="VII._Regularization">VII. Regularization</span></h1>
<p>PDF: <a rel="nofollow" class="external text" href="https://d396qusza40orc.cloudfront.net/ml/docs/slides/Lecture7.pdf">Lecture7.pdf</a></p>
<h2> <span class="mw-headline" id="The_Problem_of_Overfitting">The Problem of Overfitting</span></h2>
<p>At 2:07, a curve is drawn using predicting function $$\theta&#95;0+\theta&#95;1 x+\theta&#95;2 x^2$$, which is said as "just right". But when size of house is large enough, the prediction of this function will increase much faster than linear if $$\theta&#95;2 &gt; 0$$, or will decrease to $$-\infty$$ if $$\theta&#95;2 &lt; 0$$, which neither could correspond to reality. Instead, $$\theta&#95;0+\theta&#95;1 x+\theta&#95;2 \sqrt{x}$$ may be "just right".</p>
<p>At 2:28, a curve is drawn using a quartic (degree 4) polynomial predicting function $$\theta&#95;0+\theta&#95;1 x+\theta&#95;2 x^2 +\theta&#95;3 x^3 +\theta&#95;4 x^4$$; however, the curve drawn is at least quintic (degree 5).</p>
<h2> <span class="mw-headline" id="Cost_Function_2">Cost Function</span></h2>
<p>In the video at 5:17, the sum of the regularization term should use 'j' instead of 'i', giving $$\sum&#95;{j=1}^{n} \theta &#95;j ^2 $$ instead of $$\sum&#95;{i=1}^{n} \theta &#95;j ^2 $$.</p>
<h2> <span class="mw-headline" id="Regularized_linear_regression">Regularized linear regression</span></h2>
<p>In the video starting at 8:04, Prof Ng discusses the Normal Equation and invertibility. He states that X is non-invertible if m &lt;= n. The correct statement should be that X is non-invertible if m &lt; n, and may be non-invertible if m = n.</p>
<h2> <span class="mw-headline" id="Regularized_logistic_regression">Regularized logistic regression</span></h2>
<p>In the video at 3:52, the lecturer mistakenly said "gradient descent for regularized linear regression". Indeed, it should be "gradient descent for regularized logistic regression".</p>
<p>In the video at 5:21, the cost function is missing a pair of parentheses around the second log argument. It should be
$$J(\theta) = [-\frac{1}{m}\sum&#95;{i=1}^{m}y^{(i)}log(h &#95;\theta (x^{(i)}) + (1-y^{(i)})log(1-h &#95;\theta (x^{(i)}))] + \frac{\lambda}{2m} \sum&#95;{j=1}^{n} \theta ^2 &#95;j$$</p>
<p>In the original videos for the course (ML-001 through ML-008), there were typos in the equation for regularized logistic regression in both the video lecture and the PDF lecture notes. In the slides for "Gradient descent" and "advanced optimization", there should be positive signs for the regularization term of the gradient. The formula on page 10 of 'ex2.pdf' is correct. These issues in the video were corrected for the 'on-demand' format of the course.</p>
<h1> <span class="mw-headline" id="Quizzes">Quizzes</span></h1>
<ul>
<li>Typo "it's" in question «Because logistic regression outputs values 0≤hθ(x)≤1, it's range [...]»</li>
<li>$$\frac{1}{m}$$ factor missing in the definition of the gradient in question «For logistic regression, the gradient is given by $$\frac{\partial}{\partial \theta&#95;j} J(\theta) = \sum&#95;{i=1}^m$$ [...]» . It should be $$\frac{\partial}{\partial \theta&#95;j} J(\theta) = \frac{1}{m} \sum&#95;{i=1}^m$$ [...]. See also <a href="#Simplified_Cost_Function_and_Gradient_Descent">#Simplified Cost Function and Gradient Descent</a> above.</li>
</ul>
<h1> <span class="mw-headline" id="Programming_Exercise_Errata">Programming Exercise Errata</span></h1>
<ul>
<li><p>In ex2.pdf on page 5, Section 1.2.3, "gradent descent" should be "gradient descent".</p></li>
<li><p>If you are using a linux-derived operating system, you may need to remove the attribute "MarkerFaceColor" from the plot() function call in plotData.m.</p></li>
<li><p>In ex2.m at lines 10 through 13, the list of files the student needs to complete should include plotData.m</p></li>
<li><p>In costFunction.m on line 12, the gradient vector is incorrectly initialized to an $$(n+1) \times (n+1)$$ zero matrix via <code>grad = zeros(size(theta))</code>.  The gradient vector should instead be initialized to an $$(n+1) \times 1$$ zero vector via <code>grad = zeros(size(theta),1)</code>.</p></li>
<li><p>In costFunctionReg.m on line 12, the gradient vector is incorrectly initialized to an $$(n+1) \times (n+1)$$ zero matrix via <code>grad = zeros(size(theta))</code>.  The gradient vector should instead be initialized to an $$(n+1) \times 1$$ zero vector via <code>grad = zeros(size(theta),1)</code>.</p></li>
</ul>
<p></p>

<!-- 
NewPP limit report
Preprocessor node count: 2/1000000
Post-expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Expensive parser function count: 0/100
-->


</div><div class="printfooter">
Retrieved from "<a href="https://share.coursera.org/wiki/index.php?title=ML:Errata:_Week_3&amp;oldid=34425">https://share.coursera.org/wiki/index.php?title=ML:Errata:_Week_3&amp;oldid=34425</a>"</div>
					<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks"><a href="/wiki/index.php/Special:Categories" title="Special:Categories">Category</a>: <ul><li><a href="/wiki/index.php/Category:ML:Errata" title="Category:ML:Errata">ML:Errata</a></li></ul></div></div>					<!-- end content -->
									</div>
			</div>